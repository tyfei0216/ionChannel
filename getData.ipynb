{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pytorch_lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "import trainUtils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.binary_cross_entropy(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data2/tyfei/trainresults/ionChannels/ESMC/domain885/\" \n",
    "with open(os.path.join(path, \"config.json\"), \"r\") as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = trainUtils.loadPretrainModel(configs)\n",
    "\n",
    "model = trainUtils.buildModel(configs, pretrain_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['esm_model.embed.weight', 'esm_model.transformer.blocks.0.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.0.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.0.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.0.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.0.attn.q_ln.weight', 'esm_model.transformer.blocks.0.attn.k_ln.weight', 'esm_model.transformer.blocks.0.ffn.0.weight', 'esm_model.transformer.blocks.0.ffn.0.bias', 'esm_model.transformer.blocks.0.ffn.1.linear.weight', 'esm_model.transformer.blocks.0.ffn.3.linear.weight', 'esm_model.transformer.blocks.1.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.1.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.1.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.1.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.1.attn.q_ln.weight', 'esm_model.transformer.blocks.1.attn.k_ln.weight', 'esm_model.transformer.blocks.1.ffn.0.weight', 'esm_model.transformer.blocks.1.ffn.0.bias', 'esm_model.transformer.blocks.1.ffn.1.linear.weight', 'esm_model.transformer.blocks.1.ffn.3.linear.weight', 'esm_model.transformer.blocks.2.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.2.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.2.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.2.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.2.attn.q_ln.weight', 'esm_model.transformer.blocks.2.attn.k_ln.weight', 'esm_model.transformer.blocks.2.ffn.0.weight', 'esm_model.transformer.blocks.2.ffn.0.bias', 'esm_model.transformer.blocks.2.ffn.1.linear.weight', 'esm_model.transformer.blocks.2.ffn.3.linear.weight', 'esm_model.transformer.blocks.3.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.3.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.3.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.3.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.3.attn.q_ln.weight', 'esm_model.transformer.blocks.3.attn.k_ln.weight', 'esm_model.transformer.blocks.3.ffn.0.weight', 'esm_model.transformer.blocks.3.ffn.0.bias', 'esm_model.transformer.blocks.3.ffn.1.linear.weight', 'esm_model.transformer.blocks.3.ffn.3.linear.weight', 'esm_model.transformer.blocks.4.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.4.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.4.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.4.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.4.attn.q_ln.weight', 'esm_model.transformer.blocks.4.attn.k_ln.weight', 'esm_model.transformer.blocks.4.ffn.0.weight', 'esm_model.transformer.blocks.4.ffn.0.bias', 'esm_model.transformer.blocks.4.ffn.1.linear.weight', 'esm_model.transformer.blocks.4.ffn.3.linear.weight', 'esm_model.transformer.blocks.5.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.5.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.5.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.5.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.5.attn.q_ln.weight', 'esm_model.transformer.blocks.5.attn.k_ln.weight', 'esm_model.transformer.blocks.5.ffn.0.weight', 'esm_model.transformer.blocks.5.ffn.0.bias', 'esm_model.transformer.blocks.5.ffn.1.linear.weight', 'esm_model.transformer.blocks.5.ffn.3.linear.weight', 'esm_model.transformer.blocks.6.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.6.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.6.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.6.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.6.attn.q_ln.weight', 'esm_model.transformer.blocks.6.attn.k_ln.weight', 'esm_model.transformer.blocks.6.ffn.0.weight', 'esm_model.transformer.blocks.6.ffn.0.bias', 'esm_model.transformer.blocks.6.ffn.1.linear.weight', 'esm_model.transformer.blocks.6.ffn.3.linear.weight', 'esm_model.transformer.blocks.7.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.7.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.7.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.7.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.7.attn.q_ln.weight', 'esm_model.transformer.blocks.7.attn.k_ln.weight', 'esm_model.transformer.blocks.7.ffn.0.weight', 'esm_model.transformer.blocks.7.ffn.0.bias', 'esm_model.transformer.blocks.7.ffn.1.linear.weight', 'esm_model.transformer.blocks.7.ffn.3.linear.weight', 'esm_model.transformer.blocks.8.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.8.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.8.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.8.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.8.attn.q_ln.weight', 'esm_model.transformer.blocks.8.attn.k_ln.weight', 'esm_model.transformer.blocks.8.ffn.0.weight', 'esm_model.transformer.blocks.8.ffn.0.bias', 'esm_model.transformer.blocks.8.ffn.1.linear.weight', 'esm_model.transformer.blocks.8.ffn.3.linear.weight', 'esm_model.transformer.blocks.9.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.9.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.9.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.9.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.9.attn.q_ln.weight', 'esm_model.transformer.blocks.9.attn.k_ln.weight', 'esm_model.transformer.blocks.9.ffn.0.weight', 'esm_model.transformer.blocks.9.ffn.0.bias', 'esm_model.transformer.blocks.9.ffn.1.linear.weight', 'esm_model.transformer.blocks.9.ffn.3.linear.weight', 'esm_model.transformer.blocks.10.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.10.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.10.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.10.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.10.attn.q_ln.weight', 'esm_model.transformer.blocks.10.attn.k_ln.weight', 'esm_model.transformer.blocks.10.ffn.0.weight', 'esm_model.transformer.blocks.10.ffn.0.bias', 'esm_model.transformer.blocks.10.ffn.1.linear.weight', 'esm_model.transformer.blocks.10.ffn.3.linear.weight', 'esm_model.transformer.blocks.11.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.11.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.11.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.11.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.11.attn.q_ln.weight', 'esm_model.transformer.blocks.11.attn.k_ln.weight', 'esm_model.transformer.blocks.11.ffn.0.weight', 'esm_model.transformer.blocks.11.ffn.0.bias', 'esm_model.transformer.blocks.11.ffn.1.linear.weight', 'esm_model.transformer.blocks.11.ffn.3.linear.weight', 'esm_model.transformer.blocks.12.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.12.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.12.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.12.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.12.attn.q_ln.weight', 'esm_model.transformer.blocks.12.attn.k_ln.weight', 'esm_model.transformer.blocks.12.ffn.0.weight', 'esm_model.transformer.blocks.12.ffn.0.bias', 'esm_model.transformer.blocks.12.ffn.1.linear.weight', 'esm_model.transformer.blocks.12.ffn.3.linear.weight', 'esm_model.transformer.blocks.13.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.13.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.13.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.13.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.13.attn.q_ln.weight', 'esm_model.transformer.blocks.13.attn.k_ln.weight', 'esm_model.transformer.blocks.13.ffn.0.weight', 'esm_model.transformer.blocks.13.ffn.0.bias', 'esm_model.transformer.blocks.13.ffn.1.linear.weight', 'esm_model.transformer.blocks.13.ffn.3.linear.weight', 'esm_model.transformer.blocks.14.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.14.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.14.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.14.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.14.attn.q_ln.weight', 'esm_model.transformer.blocks.14.attn.k_ln.weight', 'esm_model.transformer.blocks.14.ffn.0.weight', 'esm_model.transformer.blocks.14.ffn.0.bias', 'esm_model.transformer.blocks.14.ffn.1.linear.weight', 'esm_model.transformer.blocks.14.ffn.3.linear.weight', 'esm_model.transformer.blocks.15.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.15.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.15.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.15.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.15.attn.q_ln.weight', 'esm_model.transformer.blocks.15.attn.k_ln.weight', 'esm_model.transformer.blocks.15.ffn.0.weight', 'esm_model.transformer.blocks.15.ffn.0.bias', 'esm_model.transformer.blocks.15.ffn.1.linear.weight', 'esm_model.transformer.blocks.15.ffn.3.linear.weight', 'esm_model.transformer.blocks.16.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.16.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.16.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.16.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.16.attn.q_ln.weight', 'esm_model.transformer.blocks.16.attn.k_ln.weight', 'esm_model.transformer.blocks.16.ffn.0.weight', 'esm_model.transformer.blocks.16.ffn.0.bias', 'esm_model.transformer.blocks.16.ffn.1.linear.weight', 'esm_model.transformer.blocks.16.ffn.3.linear.weight', 'esm_model.transformer.blocks.17.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.17.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.17.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.17.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.17.attn.q_ln.weight', 'esm_model.transformer.blocks.17.attn.k_ln.weight', 'esm_model.transformer.blocks.17.ffn.0.weight', 'esm_model.transformer.blocks.17.ffn.0.bias', 'esm_model.transformer.blocks.17.ffn.1.linear.weight', 'esm_model.transformer.blocks.17.ffn.3.linear.weight', 'esm_model.transformer.blocks.18.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.18.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.18.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.18.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.18.attn.q_ln.weight', 'esm_model.transformer.blocks.18.attn.k_ln.weight', 'esm_model.transformer.blocks.18.ffn.0.weight', 'esm_model.transformer.blocks.18.ffn.0.bias', 'esm_model.transformer.blocks.18.ffn.1.linear.weight', 'esm_model.transformer.blocks.18.ffn.3.linear.weight', 'esm_model.transformer.blocks.19.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.19.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.19.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.19.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.19.attn.q_ln.weight', 'esm_model.transformer.blocks.19.attn.k_ln.weight', 'esm_model.transformer.blocks.19.ffn.0.weight', 'esm_model.transformer.blocks.19.ffn.0.bias', 'esm_model.transformer.blocks.19.ffn.1.linear.weight', 'esm_model.transformer.blocks.19.ffn.3.linear.weight', 'esm_model.transformer.blocks.20.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.20.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.20.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.20.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.20.attn.q_ln.weight', 'esm_model.transformer.blocks.20.attn.k_ln.weight', 'esm_model.transformer.blocks.20.ffn.0.weight', 'esm_model.transformer.blocks.20.ffn.0.bias', 'esm_model.transformer.blocks.20.ffn.1.linear.weight', 'esm_model.transformer.blocks.20.ffn.3.linear.weight', 'esm_model.transformer.blocks.21.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.21.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.21.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.21.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.21.attn.q_ln.weight', 'esm_model.transformer.blocks.21.attn.k_ln.weight', 'esm_model.transformer.blocks.21.ffn.0.weight', 'esm_model.transformer.blocks.21.ffn.0.bias', 'esm_model.transformer.blocks.21.ffn.1.linear.weight', 'esm_model.transformer.blocks.21.ffn.3.linear.weight', 'esm_model.transformer.blocks.22.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.22.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.22.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.22.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.22.attn.q_ln.weight', 'esm_model.transformer.blocks.22.attn.k_ln.weight', 'esm_model.transformer.blocks.22.ffn.0.weight', 'esm_model.transformer.blocks.22.ffn.0.bias', 'esm_model.transformer.blocks.22.ffn.1.linear.weight', 'esm_model.transformer.blocks.22.ffn.3.linear.weight', 'esm_model.transformer.blocks.23.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.23.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.23.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.23.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.23.attn.q_ln.weight', 'esm_model.transformer.blocks.23.attn.k_ln.weight', 'esm_model.transformer.blocks.23.ffn.0.weight', 'esm_model.transformer.blocks.23.ffn.0.bias', 'esm_model.transformer.blocks.23.ffn.1.linear.weight', 'esm_model.transformer.blocks.23.ffn.3.linear.weight', 'esm_model.transformer.blocks.24.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.24.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.24.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.24.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.24.attn.q_ln.weight', 'esm_model.transformer.blocks.24.attn.k_ln.weight', 'esm_model.transformer.blocks.24.ffn.0.weight', 'esm_model.transformer.blocks.24.ffn.0.bias', 'esm_model.transformer.blocks.24.ffn.1.linear.weight', 'esm_model.transformer.blocks.24.ffn.3.linear.weight', 'esm_model.transformer.blocks.25.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.25.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.25.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.25.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.25.attn.q_ln.weight', 'esm_model.transformer.blocks.25.attn.k_ln.weight', 'esm_model.transformer.blocks.25.ffn.0.weight', 'esm_model.transformer.blocks.25.ffn.0.bias', 'esm_model.transformer.blocks.25.ffn.1.linear.weight', 'esm_model.transformer.blocks.25.ffn.3.linear.weight', 'esm_model.transformer.blocks.26.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.26.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.26.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.26.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.26.attn.q_ln.weight', 'esm_model.transformer.blocks.26.attn.k_ln.weight', 'esm_model.transformer.blocks.26.ffn.0.weight', 'esm_model.transformer.blocks.26.ffn.0.bias', 'esm_model.transformer.blocks.26.ffn.1.linear.weight', 'esm_model.transformer.blocks.26.ffn.3.linear.weight', 'esm_model.transformer.blocks.27.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.27.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.27.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.27.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.27.attn.q_ln.weight', 'esm_model.transformer.blocks.27.attn.k_ln.weight', 'esm_model.transformer.blocks.27.ffn.0.weight', 'esm_model.transformer.blocks.27.ffn.0.bias', 'esm_model.transformer.blocks.27.ffn.1.linear.weight', 'esm_model.transformer.blocks.27.ffn.3.linear.weight', 'esm_model.transformer.blocks.28.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.28.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.28.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.28.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.28.attn.q_ln.weight', 'esm_model.transformer.blocks.28.attn.k_ln.weight', 'esm_model.transformer.blocks.28.ffn.0.weight', 'esm_model.transformer.blocks.28.ffn.0.bias', 'esm_model.transformer.blocks.28.ffn.1.linear.weight', 'esm_model.transformer.blocks.28.ffn.3.linear.weight', 'esm_model.transformer.blocks.29.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.29.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.29.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.29.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.29.attn.q_ln.weight', 'esm_model.transformer.blocks.29.attn.k_ln.weight', 'esm_model.transformer.blocks.29.ffn.0.weight', 'esm_model.transformer.blocks.29.ffn.0.bias', 'esm_model.transformer.blocks.29.ffn.1.linear.weight', 'esm_model.transformer.blocks.29.ffn.3.linear.weight', 'esm_model.transformer.blocks.30.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.30.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.30.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.30.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.30.attn.q_ln.weight', 'esm_model.transformer.blocks.30.attn.k_ln.weight', 'esm_model.transformer.blocks.30.ffn.0.weight', 'esm_model.transformer.blocks.30.ffn.0.bias', 'esm_model.transformer.blocks.30.ffn.1.linear.weight', 'esm_model.transformer.blocks.30.ffn.3.linear.weight', 'esm_model.transformer.blocks.31.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.31.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.31.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.31.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.31.attn.q_ln.weight', 'esm_model.transformer.blocks.31.attn.k_ln.weight', 'esm_model.transformer.blocks.31.ffn.0.weight', 'esm_model.transformer.blocks.31.ffn.0.bias', 'esm_model.transformer.blocks.31.ffn.1.linear.weight', 'esm_model.transformer.blocks.31.ffn.3.linear.weight', 'esm_model.transformer.blocks.32.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.32.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.32.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.32.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.32.attn.q_ln.weight', 'esm_model.transformer.blocks.32.attn.k_ln.weight', 'esm_model.transformer.blocks.32.ffn.0.weight', 'esm_model.transformer.blocks.32.ffn.0.bias', 'esm_model.transformer.blocks.32.ffn.1.linear.weight', 'esm_model.transformer.blocks.32.ffn.3.linear.weight', 'esm_model.transformer.blocks.33.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.33.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.33.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.33.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.33.attn.q_ln.weight', 'esm_model.transformer.blocks.33.attn.k_ln.weight', 'esm_model.transformer.blocks.33.ffn.0.weight', 'esm_model.transformer.blocks.33.ffn.0.bias', 'esm_model.transformer.blocks.33.ffn.1.linear.weight', 'esm_model.transformer.blocks.33.ffn.3.linear.weight', 'esm_model.transformer.blocks.34.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.34.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.34.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.34.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.34.attn.q_ln.weight', 'esm_model.transformer.blocks.34.attn.k_ln.weight', 'esm_model.transformer.blocks.34.ffn.0.weight', 'esm_model.transformer.blocks.34.ffn.0.bias', 'esm_model.transformer.blocks.34.ffn.1.linear.weight', 'esm_model.transformer.blocks.34.ffn.3.linear.weight', 'esm_model.transformer.blocks.35.attn.layernorm_qkv.0.weight', 'esm_model.transformer.blocks.35.attn.layernorm_qkv.0.bias', 'esm_model.transformer.blocks.35.attn.layernorm_qkv.1.linear.weight', 'esm_model.transformer.blocks.35.attn.out_proj.linear.weight', 'esm_model.transformer.blocks.35.attn.q_ln.weight', 'esm_model.transformer.blocks.35.attn.k_ln.weight', 'esm_model.transformer.blocks.35.ffn.0.weight', 'esm_model.transformer.blocks.35.ffn.0.bias', 'esm_model.transformer.blocks.35.ffn.1.linear.weight', 'esm_model.transformer.blocks.35.ffn.3.linear.weight', 'esm_model.transformer.norm.weight', 'esm_model.sequence_head.0.weight', 'esm_model.sequence_head.0.bias', 'esm_model.sequence_head.2.weight', 'esm_model.sequence_head.2.bias', 'esm_model.sequence_head.3.weight', 'esm_model.sequence_head.3.bias'], unexpected_keys=['pos_weights'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.load(\"/data2/tyfei/trainresults/ionChannels/ESMC/domain885/epoch=7-validate_acc=0.99.ckpt\")\n",
    "model.load_state_dict(t['state_dict'], strict=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get val loader\n"
     ]
    }
   ],
   "source": [
    "ds = trainUtils.loadDataset(configs)\n",
    "dl = ds.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called new epoch\n",
      "[{'seq_t': tensor([[ 0, 20, 12, 10,  9,  4,  4,  4, 12,  6,  7,  6,  6, 18, 18,  6,  5, 12,\n",
      "         18, 10, 19,  4, 12,  8,  6, 12, 12, 14,  7, 15, 18,  6, 12, 14, 11,  6,\n",
      "         11,  4,  5,  7, 17,  4, 12,  6,  8, 18, 12,  4,  6, 18,  4, 20, 19,  8,\n",
      "          8,  4,  4, 11,  8, 20, 14,  8,  9, 19, 15,  4, 18, 12,  6, 11,  6, 18,\n",
      "         23,  6,  5,  4, 11, 11, 18,  8, 11, 18,  8, 19,  9, 11, 18,  5,  4,  7,\n",
      "         13,  9,  6,  4,  4, 18, 15,  5,  4,  4, 17, 12,  4, 12, 17,  7,  5,  6,\n",
      "         23,  4, 12, 20,  7, 19, 18,  6, 10, 11,  7,  7,  4,  7, 12, 18, 15,  2]])}, tensor([[1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), {'seq_t': tensor([[ 0, 20, 17, 15, 12, 11, 15,  7, 15, 11, 20,  8, 15, 10,  8, 12,  5,  5,\n",
      "         12, 12,  7, 18,  8, 20, 20, 19,  8,  6,  7,  8,  4,  5,  5,  9, 10, 17,\n",
      "         15,  7,  9, 12,  8, 13, 17,  6, 10,  7, 10,  7, 11, 11, 17,  6, 12, 11,\n",
      "         15,  9,  5,  6, 15, 18, 10, 15,  8,  9, 11, 10, 18,  6,  9, 11, 15, 12,\n",
      "         19, 11, 17, 15, 11, 19,  6, 15, 14,  5,  7, 11,  4, 13, 10, 19,  6, 10,\n",
      "         16,  7,  9, 13,  9, 13, 13,  8, 13,  9,  2]])}]\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(dl):\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds.val_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3950/3950 [04:19<00:00, 15.22it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(devices=[7])\n",
    "res = trainer.predict(model, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0.9904) tensor(0.4937)\n",
      "1\n",
      "tensor(0.7721) tensor(0.4491)\n",
      "2\n",
      "tensor(0.6507) tensor(0.5962)\n",
      "3\n",
      "tensor(0.8374) tensor(0.0823)\n",
      "4\n",
      "tensor(0.9588) tensor(0.2078)\n",
      "5\n",
      "tensor(0.8786) tensor(0.9002)\n",
      "6\n",
      "tensor(0.8169) tensor(0.0947)\n",
      "7\n",
      "tensor(0.7937) tensor(0.0484)\n",
      "8\n",
      "tensor(0.9918) tensor(0.0185)\n",
      "9\n",
      "tensor(0.8153) tensor(0.0710)\n",
      "10\n",
      "tensor(0.8488) tensor(0.0211)\n",
      "11\n",
      "tensor(0.9758) tensor(0.0010)\n",
      "12\n",
      "tensor(0.7801) tensor(0.4045)\n",
      "13\n",
      "tensor(0.8716) tensor(0.2199)\n",
      "14\n",
      "tensor(0.9122) tensor(0.2914)\n",
      "15\n",
      "tensor(0.7906) tensor(0.3482)\n",
      "16\n",
      "tensor(0.8054) tensor(0.1405)\n",
      "17\n",
      "tensor(0.8916) tensor(0.2956)\n",
      "18\n",
      "tensor(0.8722) tensor(0.1831)\n",
      "19\n",
      "tensor(0.8085) tensor(0.3882)\n",
      "20\n",
      "tensor(0.9174) tensor(0.7464)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49367088079452515,\n",
       " 0.44907405972480774,\n",
       " 0.5961934328079224,\n",
       " 0.08230452984571457,\n",
       " 0.2078189253807068,\n",
       " 0.9002057909965515,\n",
       " 0.09465020895004272,\n",
       " 0.04835391044616699,\n",
       " 0.018518518656492233,\n",
       " 0.07098765671253204,\n",
       " 0.021090535447001457,\n",
       " 0.001028806553222239,\n",
       " 0.40452393889427185,\n",
       " 0.21988427639007568,\n",
       " 0.29142555594444275,\n",
       " 0.3482377827167511,\n",
       " 0.14045239984989166,\n",
       " 0.29563388228416443,\n",
       " 0.18306154012680054,\n",
       " 0.3882167339324951,\n",
       " 0.7464492321014404]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "t = 1\n",
    "num = []\n",
    "for t in range(21):\n",
    "    y = [] \n",
    "    pre = [] \n",
    "    for i in res:\n",
    "        if t==0:\n",
    "            pre.append(i[0][0])\n",
    "            y.append(i[1][0][0])\n",
    "        elif (i[1][0][t] > -0.5):\n",
    "            pre.append(i[0][1][t-1])\n",
    "            y.append(i[1][0][t])\n",
    "    print(t)\n",
    "    print(acc(torch.tensor(pre), torch.tensor(y)), torch.sum(torch.stack(y))/len(y))\n",
    "    num.append((torch.sum(torch.stack(y))/len(y)).item())\n",
    "num    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.97008542,   1.14705881,   0.87053568,   6.8023253 ,\n",
       "         2.16666666,   0.55188679,   5.31818171,  10.08620674,\n",
       "        41.78571352,   7.13414601,  32.49999997, 292.49999751,\n",
       "         1.25550664,   2.11111106,   1.69642863,   1.36363633,\n",
       "         3.65384625,   1.69642863,   2.74038468,   1.32558142,\n",
       "         0.67216981])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5/np.array(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5915)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomseed': 1509,\n",
       " 'model': 'esm3_sm_open_v1',\n",
       " 'id': 'A0A151XG23',\n",
       " 'ori_seq': 'MTISYASEVPNGSSFGCFWRILIKWRGSVYKLIWRELLAYLFFYYLINFTYRYVLNEHQRQIFEKIRYYFGNSSESIPMSFVLGFYVSLVVKRWWEQYKLLPWPDNLALFISAAIPGNDERGRLMRRNIVRYAVLAYVITLQRISLRVKRRFPTLQHMVDVGLMMESEKKIFEMMNKKAAMSKYWMPLVWATNIINRARKEALITSDHVVQTLLVELSDIRKRLGALIGYDTVCVPLVYTQVVTLSLYAYFFSALLGRQFVEHSNGTGKYEEPDMYFPFFTALQFCFYIGWLKVAEVLINPFGEDDDDIELNWLIDRHIKAGYMIVDEMHEEHPELLKDQYWDEVVPKDLPYTVASEHGQHRKSHQDDVYADYESVDTPLVERKKNGWFQRQITRVGSVRSSSTTYSSGGGFFTRNRHNSVYSSPETGGLPQTNNPNLKMSLYDRLVGRKSVRSQRMGRQGTMTKLNSVPVSLKNRPRIPTPDVTKEVIDREQRLALSAANAANIGAGVVGVIPANGHYPTDLSVVVLSPIQETESSPASGKSGAAALAQAVLSPTLTSAGLMTPVTLTPVTMGHLTQLGLMTTATTTTPHLNNQSNSVQATLTEVTSSEEEGSGSGSSRSGSITGQEDRSTPLIDNNNSPMGSNGSSPIFDGYNDRSPILMRPEKLGYVVTAAVPGIQTNRAMAIDARGRRSASLPGPPVSPAQLSNDRSMSLPQSPGLQSREIRIASVSSRQDGLAIDRLITGSHNQGIRSRTNSFGHDLSRSNQRGQDASRKISTVSCTNASLSSSTTSPTSSTSISTTTVTSSKRGEVYV',\n",
       " 'seq_t': array([ 0, 20, 11, 12,  8, 19,  5,  8,  9,  7, 14, 17,  6,  8,  8, 18,  6,\n",
       "        23, 18, 22, 10, 12,  4, 12, 15, 22, 10,  6,  8,  7, 19, 15,  4, 12,\n",
       "        22, 10,  9,  4,  4,  5, 19,  4, 18, 18, 19, 19,  4, 12, 17, 18, 11,\n",
       "        19, 10, 19,  7,  4, 17,  9, 21, 16, 10, 16, 12, 18,  9, 15, 12, 10,\n",
       "        19, 19, 18,  6, 17,  8,  8,  9,  8, 12, 14, 20,  8, 18,  7,  4,  6,\n",
       "        18, 19,  7,  8,  4,  7,  7, 15, 10, 22, 22,  9, 16, 19, 15,  4,  4,\n",
       "        14, 22, 14, 13, 17,  4,  5,  4, 18, 12,  8,  5,  5, 12, 14,  6, 17,\n",
       "        13,  9, 10,  6, 10,  4, 20, 10, 10, 17, 12,  7, 10, 19,  5,  7,  4,\n",
       "         5, 19,  7, 12, 11,  4, 16, 10, 12,  8,  4, 10,  7, 15, 10, 10, 18,\n",
       "        14, 11,  4, 16, 21, 20,  7, 13,  7,  6,  4, 20, 20,  9,  8,  9, 15,\n",
       "        15, 12, 18,  9, 20, 20, 17, 15, 15,  5,  5, 20,  8, 15, 19, 22, 20,\n",
       "        14,  4,  7, 22,  5, 11, 17, 12, 12, 17, 10,  5, 10, 15,  9,  5,  4,\n",
       "        12, 11,  8, 13, 21,  7,  7, 16, 11,  4,  4,  7,  9,  4,  8, 13, 12,\n",
       "        10, 15, 10,  4,  6,  5,  4, 12,  6, 19, 13, 11,  7, 23,  7, 14,  4,\n",
       "         7, 19, 11, 16,  7,  7, 11,  4,  8,  4, 19,  5, 19, 18, 18,  8,  5,\n",
       "         4,  4,  6, 10, 16, 18,  7,  9, 21,  8, 17,  6, 11,  6, 15, 19,  9,\n",
       "         9, 14, 13, 20, 19, 18, 14, 18, 18, 11,  5,  4, 16, 18, 23, 18, 19,\n",
       "        12,  6, 22,  4, 15,  7,  5,  9,  7,  4, 12, 17, 14, 18,  6,  9, 13,\n",
       "        13, 13, 13, 12,  9,  4, 17, 22,  4, 12, 13, 10, 21, 12, 15,  5,  6,\n",
       "        19, 20, 12,  7, 13,  9, 20, 21,  9,  9, 21, 14,  9,  4,  4, 15, 13,\n",
       "        16, 19, 22, 13,  9,  7,  7, 14, 15, 13,  4, 14, 19, 11,  7,  5,  8,\n",
       "         9, 21,  6, 16, 21, 10, 15,  8, 21, 16, 13, 13,  7, 19,  5, 13, 19,\n",
       "         9,  8,  7, 13, 11, 14,  4,  7,  9, 10, 15, 15, 17,  6, 22, 18, 16,\n",
       "        10, 16, 12, 11, 10,  7,  6,  8,  7, 10,  8,  8,  8, 11, 11, 19,  8,\n",
       "         8,  6,  6,  6, 18, 18, 11, 10, 17, 10, 21, 17,  8,  7, 19,  8,  8,\n",
       "        14,  9, 11,  6,  6,  4, 14, 16, 11, 17, 17, 14, 17,  4, 15, 20,  8,\n",
       "         4, 19, 13, 10,  4,  7,  6, 10, 15,  8,  7, 10,  8, 16, 10, 20,  6,\n",
       "        10, 16,  6, 11, 20, 11, 15,  4, 17,  8,  7, 14,  7,  8,  4, 15, 17,\n",
       "        10, 14, 10, 12, 14, 11, 14, 13,  7, 11, 15,  9,  7, 12, 13, 10,  9,\n",
       "        16, 10,  4,  5,  4,  8,  5,  5, 17,  5,  5, 17, 12,  6,  5,  6,  7,\n",
       "         7,  6,  7, 12, 14,  5, 17,  6, 21, 19, 14, 11, 13,  4,  8,  7,  7,\n",
       "         7,  4,  8, 14, 12, 16,  9, 11,  9,  8,  8, 14,  5,  8,  6, 15,  8,\n",
       "         6,  5,  5,  5,  4,  5, 16,  5,  7,  4,  8, 14, 11,  4, 11,  8,  5,\n",
       "         6,  4, 20, 11, 14,  7, 11,  4, 11, 14,  7, 11, 20,  6, 21,  4, 11,\n",
       "        16,  4,  6,  4, 20, 11, 11,  5, 11, 11, 11, 11, 14, 21,  4, 17, 17,\n",
       "        16,  8, 17,  8,  7, 16,  5, 11,  4, 11,  9,  7, 11,  8,  8,  9,  9,\n",
       "         9,  6,  8,  6,  8,  6,  8,  8, 10,  8,  6,  8, 12, 11,  6, 16,  9,\n",
       "        13, 10,  8, 11, 14,  4, 12, 13, 17, 17, 17,  8, 14, 20,  6,  8, 17,\n",
       "         6,  8,  8, 14, 12, 18, 13,  6, 19, 17, 13, 10,  8, 14, 12,  4, 20,\n",
       "        10, 14,  9, 15,  4,  6, 19,  7,  7, 11,  5,  5,  7, 14,  6, 12, 16,\n",
       "        11, 17, 10,  5, 20,  5, 12, 13,  5, 10,  6, 10, 10,  8,  5,  8,  4,\n",
       "        14,  6, 14, 14,  7,  8, 14,  5, 16,  4,  8, 17, 13, 10,  8, 20,  8,\n",
       "         4, 14, 16,  8, 14,  6,  4, 16,  8, 10,  9, 12, 10, 12,  5,  8,  7,\n",
       "         8,  8, 10, 16, 13,  6,  4,  5, 12, 13, 10,  4, 12, 11,  6,  8, 21,\n",
       "        17, 16,  6, 12, 10,  8, 10, 11, 17,  8, 18,  6, 21, 13,  4,  8, 10,\n",
       "         8, 17, 16, 10,  6, 16, 13,  5,  8, 10, 15, 12,  8, 11,  7,  8, 23,\n",
       "        11, 17,  5,  8,  4,  8,  8,  8, 11, 11,  8, 14, 11,  8,  8, 11,  8,\n",
       "        12,  8, 11, 11, 11,  7, 11,  8,  8, 15, 10,  6,  9,  7, 19,  7,  2]),\n",
       " 'structure_t': array([4098, 2476, 3118, 1945, 1237,  470, 1658, 3259, 3132,  255, 3562,\n",
       "         699, 1862, 1638, 3827, 3575, 3501,  542,  492, 2414, 3809, 2702,\n",
       "        2103, 1476,  519, 3843, 3909, 3382,  962,  568, 2682, 2477,    1,\n",
       "        1874, 3372, 2983, 2222, 3950, 3310, 2082, 3207,  992, 2842, 2222,\n",
       "        3112,  598, 2056, 1421, 3207, 1035,  264, 1816, 1842, 2078, 1701,\n",
       "        3538, 1412, 3328,  824, 2747, 3900,  992, 2842, 2682, 1259, 3310,\n",
       "        2747, 4048, 3310, 3310, 3728, 2292,  598, 3306, 2106, 2842, 2439,\n",
       "        3966, 4084, 3639, 2693, 2874, 1359, 3242, 2056, 1444,  297,  227,\n",
       "         588, 3837, 2480, 2835, 3954,  845, 3402,  137,  588,  338, 1180,\n",
       "        3109, 1009, 3208, 2561, 3907,  181, 1080,  522, 3055,  183, 3672,\n",
       "        1240, 1077, 1039,  144, 3057, 1232,  516,  341,  240, 2118, 2456,\n",
       "        2206, 4042, 3776,  123, 3546, 4000,   24, 3196, 3055, 1042, 2147,\n",
       "        1052, 3097, 3412, 1756, 2841, 2400, 2426,  923, 3278, 2806, 3119,\n",
       "        2958, 1127,  742,  504, 3142, 3486, 2414, 3735,  959, 3729, 1495,\n",
       "        3111,  131, 3148, 1195, 2716, 1149, 1800, 1478, 2641, 2710,  917,\n",
       "        4055, 3842, 2874,  282, 2926,    9,  598, 1937, 3306, 3607, 2222,\n",
       "        2366, 3607,  598,   49, 3842, 1412, 2008, 2862,  550, 2109, 3584,\n",
       "        3181, 1101, 2439, 2169, 2215, 2416, 1800, 3241, 1077, 2660, 1800,\n",
       "        3444,  517, 3101,  790, 4008, 3444, 1955,  156,  438,  247,  264,\n",
       "          22, 2076, 2605, 1295, 1736, 2424,    9,  773,  199,    9, 1195,\n",
       "        1498, 1830, 3501, 2343, 1711, 3450, 3336, 2929,  682, 1432, 3504,\n",
       "        3702,  824,  754, 1988, 2008, 2780,  588, 3109, 2290,  462, 1450,\n",
       "        3961,  222, 2296,  588, 2751, 1883, 3954, 2842, 2828,  861, 2693,\n",
       "        3879,  192, 2740, 3882, 1512, 2673, 1323, 2254,  747, 1517, 2883,\n",
       "        3158,   65, 2833, 2657, 2563, 2873, 2637,  548, 1605, 2490, 1200,\n",
       "        1419, 2612, 3816, 1924, 1495, 3902, 3411, 1555, 2056, 2660, 2964,\n",
       "         631,  247, 2450, 2176, 2381, 1783, 3049, 3282, 2162,  168, 2693,\n",
       "        1470,  375, 1355, 2988, 3933, 3487,  437, 2287,  246, 2137, 4004,\n",
       "        3168, 3088,  364, 2906,  696, 2605, 3637,  410, 2874, 1894,  390,\n",
       "         353, 3211,  970, 2065, 4035,  137, 3351, 2417, 1428, 2531,  817,\n",
       "        1611, 2029,   67, 3621, 2652, 1347, 1364, 3319,  897, 2866, 2298,\n",
       "         370, 3999, 3108, 1570,  247, 2921, 3915, 1570, 1005,  897, 3697,\n",
       "        1684, 3146, 3575, 3450, 3450, 3158, 1532,  719,  589, 3235, 1680,\n",
       "         318, 4090, 1210,  997, 3144, 1678,  255, 1681, 2171, 2136, 2669,\n",
       "        1990, 3478, 3148,  915, 3570,  663,  505, 1986, 3153, 2886, 3480,\n",
       "        3037, 1404, 2435, 1047,  275, 3325, 3674,  247, 2872, 3319, 2329,\n",
       "        1490, 2612, 2637, 2524, 1843, 4084, 3715,  397, 1364, 1956, 1185,\n",
       "         854, 3765, 1035, 3611, 1953, 3583, 3511, 1047,  690,  257, 1140,\n",
       "        3319, 1523, 3919, 3109, 3247, 1591, 3611,  531, 3562, 2604,  200,\n",
       "         481, 1738, 3393,   93, 3229, 2104,  530, 1745, 2942, 1448, 2321,\n",
       "        3089,  657, 3422, 3685, 1556,   76, 3681, 3426, 1567, 1686, 2701,\n",
       "        3999, 1047, 1901, 2695, 2669, 3625,  525, 1865, 2840, 3707, 1543,\n",
       "         342, 3209, 2760,  495,  868, 1106, 1452, 1350,  407, 2524, 3425,\n",
       "        1589, 3099, 1265, 1140, 2552, 1310, 4073, 3631,  801, 1349,  663,\n",
       "        1432, 1450, 4081, 1352, 2279,  903, 3378, 1432, 2605, 3680, 2279,\n",
       "         867, 3205, 3789,  385, 1432,  137, 1432,    9, 3101,  124, 1656,\n",
       "        2144, 3227,  448,  257,  455,  548, 4047, 1325,  455,  698, 1782,\n",
       "         448, 2421,   76, 3798,  264, 1325,  118, 2739, 2545, 4084, 1777,\n",
       "        2852, 3919, 3965,  741, 1763, 1953, 4081, 1446, 3892, 2255, 3151,\n",
       "        4065, 2486, 2138, 1190, 1626, 1654, 4050,  903, 3372,  867, 3643,\n",
       "         255, 1892, 3754,  792, 3147, 2104,  256, 2019, 4054, 1561, 2010,\n",
       "         322, 2669, 1220, 1839, 2852, 1305,    5, 2866, 2768, 1370, 1004,\n",
       "         268, 2631,   54,  968, 2562, 3728,  998,  473,  355, 4065, 2750,\n",
       "        3474, 2088, 1956, 1675, 3919, 1838, 1179,  113, 2907, 2421,  455,\n",
       "        2695, 3655, 3680,  429, 2687, 3690, 3064,  373, 4065, 3526,  336,\n",
       "        3551, 1321, 2750, 1799, 1463, 2088, 1463, 2852, 1463, 2140,  870,\n",
       "         676, 2268, 3933, 1110, 3769,  818,  802, 1998, 3656, 3656, 2640,\n",
       "        3656, 2118, 2687, 1782,  886, 3023, 1337, 3687, 2242, 3965,  429,\n",
       "         599, 3861, 2421, 1416, 1480, 1726, 1012, 1097, 2871, 2756, 2019,\n",
       "        1253, 1873, 3765, 3631, 1140,  741, 2085, 4047, 2744,  690,  273,\n",
       "        1207, 1853,  355, 2268, 2591, 1807,  520,   52, 1109, 4065, 2544,\n",
       "        1106,  788,   86,  530,  127, 1002,  700, 3359, 3085, 2181, 2774,\n",
       "        1090, 2614,  390,   85,  791, 3635,   22, 1695,  349, 3342,  711,\n",
       "         642,   93,  626,  774, 1268,  520, 3794, 3650,  454,  617, 2168,\n",
       "         192, 1680, 2118, 3252, 2138, 3945, 3558, 1463, 2128,  711, 1602,\n",
       "        3964,  791,   67, 1404, 2260, 2489, 2236, 1955,   52,  510,  456,\n",
       "        1280, 3690,  807, 3965,  589, 3726, 3832, 2421, 1738,  721,  720,\n",
       "         642,  595,  885, 1074, 4073, 2066, 3322, 4025, 2151, 2511,   49,\n",
       "        2112, 1341, 3551, 1576, 3151, 1306,  591, 3350, 3404, 1122,   95,\n",
       "        1047, 2454, 1396, 1196, 1799, 1785, 2609, 2687, 3064, 1587,  676,\n",
       "        3706, 2460, 2609, 3064, 2960, 3381, 2377, 2240, 1124, 3635, 2809,\n",
       "        1272, 2140, 2687, 3861, 1367, 2873, 1169,  397, 3981, 2796, 4037,\n",
       "        1576, 3276, 3589, 2271, 2753, 1868,   93, 1494,  342, 2295, 1675,\n",
       "          52, 3656, 4007, 1301, 2036, 2421, 3765, 1295, 1953, 2690, 1892,\n",
       "        2252, 4097]),\n",
       " 'second_t': array([ 0, 10, 10, 10, 10, 10,  6,  6,  4,  9,  6,  9, 10, 10, 10,  6,  4,\n",
       "         4,  4,  6,  4,  4,  4,  6, 10, 10,  6,  6, 10,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         5,  5,  4,  6, 10, 10,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  5,  4,  4,  4,  4,  4,  6,  9, 10,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  9,\n",
       "        10, 10,  6,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  9, 10, 10, 10,\n",
       "         9,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4, 10,  4,  4,  4,  4,  4,  4,  9,\n",
       "        10,  9,  4,  4,  4,  4,  4,  4,  6,  6,  9,  9, 10,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  6, 10,  9,  9, 10, 10,  6,  6,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  6,  6,  9,\n",
       "         9, 10, 10,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 10,  9, 10,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  9, 10, 10, 10, 10,  3,  6,  6,  6, 10,  9, 10, 10, 10,\n",
       "         9, 10, 10, 10,  9,  9, 10,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  9, 10,  6,  6,  9,  9,  9,\n",
       "         6,  6,  9, 10, 10,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  5,  5,  5,  5,  5,  9,  6,  6, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         6,  6,  6,  6, 10,  9,  9, 10, 10, 10, 10, 10, 10, 10,  6,  3,  3,\n",
       "         6,  6, 10,  6, 10, 10, 10,  6,  6,  6, 10, 10,  3,  9,  6, 10, 10,\n",
       "        10,  4,  3,  9, 10, 10, 10, 10, 10, 10, 10,  6,  6,  3,  3,  6,  4,\n",
       "         4,  4, 10, 10, 10,  6,  6, 10, 10, 10,  9, 10,  9, 10, 10, 10, 10,\n",
       "        10,  9, 10, 10, 10,  6, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  6,  6, 10,  9,  4,  4,\n",
       "         4,  4,  4,  9,  4,  4,  4,  4,  6,  4, 10,  4, 10, 10, 10,  6, 10,\n",
       "         6, 10, 10, 10, 10, 10, 10,  6, 10, 10, 10, 10, 10, 10, 10,  3,  3,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  6, 10, 10,  9, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         9,  4,  4,  4,  4,  4,  4,  9,  3, 10, 10, 10, 10, 10, 10,  6,  6,\n",
       "         9, 10, 10, 10, 10, 10,  7, 10, 10,  4, 10,  4,  4,  4,  4,  9,  9,\n",
       "         9,  9, 10, 10,  7,  7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  7,  7,\n",
       "        10, 10,  3,  6,  6,  7,  9,  7, 10,  9,  9, 10, 10, 10,  6, 10, 10,\n",
       "        10, 10,  9, 10,  9,  7,  7, 10,  9,  6, 10, 10,  7,  7,  7, 10,  7,\n",
       "        10, 10,  9,  9, 10, 10, 10,  3,  4, 10, 10, 10, 10,  7,  7,  8, 10,\n",
       "        10, 10, 10, 10, 10,  3, 10,  9,  7, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10,  4,  4,  4,  4,  6,  3, 10, 10,  6,  3,  4,  9,  9, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10,  6, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  0]),\n",
       " 'sasa_t': array([ 0, 13, 11, 11, 14, 10, 14, 14, 14, 12, 16, 16, 11, 16, 11, 17, 10,\n",
       "         9, 16, 17, 17,  9, 17, 18, 13, 17, 17,  8,  7, 12, 13, 13,  8,  7,\n",
       "        16, 17,  7,  8, 16, 10,  5, 10, 17, 15,  5, 15, 14,  8,  6, 15, 10,\n",
       "         4, 14, 18, 15,  9, 15, 17, 17, 13,  8, 16, 13,  6,  9, 16, 11,  9,\n",
       "        17, 17, 11,  8, 13, 10,  7, 17, 14,  7, 12,  7, 14, 18,  9,  3,  9,\n",
       "        17,  4,  7, 11, 14,  4, 12, 16, 11, 12, 17, 14,  5, 12, 17,  9,  3,\n",
       "         4, 16,  5, 12, 14,  5,  3, 16, 12,  3,  5, 14,  5,  3, 12,  6, 15,\n",
       "        13, 15, 18,  5, 12, 15,  7,  3, 14,  9,  3,  5,  8,  6,  4,  6,  3,\n",
       "         3,  3,  3,  6,  3,  3,  9, 16,  5,  3, 15,  6,  3, 16, 18, 12,  4,\n",
       "        12, 12,  9, 14, 12,  3,  7, 14, 14, 10,  5,  4, 15, 15, 12,  5,  9,\n",
       "        17, 14,  5, 11, 16, 10, 11, 18, 18,  8, 16, 17,  7, 10,  8,  9,  3,\n",
       "         3,  7, 12, 10,  3,  8, 14,  8,  3, 12, 17,  3, 14, 18, 12, 13,  8,\n",
       "         4, 17,  9, 14, 17, 11,  6, 14, 11,  3,  9, 13,  9,  4, 10, 11,  6,\n",
       "        14, 17, 14,  3,  7,  8,  4, 11,  9, 10, 10, 16, 16, 13,  9, 14, 16,\n",
       "        15, 12,  8, 14, 13,  4, 10, 16,  8,  3, 15, 10,  7,  5, 16,  5,  5,\n",
       "         7,  7,  5, 16,  4,  7,  6, 11, 12, 15, 11, 13, 13,  7, 18, 13, 18,\n",
       "        11, 12, 13, 12, 18, 13,  7, 16, 18,  8,  8, 13, 11,  5,  8, 16, 10,\n",
       "         5,  5, 15,  4,  7, 10,  4, 12,  7,  7, 12, 14,  6,  3,  5, 11,  8,\n",
       "         9,  7,  5,  4,  9,  3,  5, 18,  6,  3, 12, 13,  6,  9, 17,  8,  5,\n",
       "        13, 14, 13,  3,  7, 15, 15,  6, 14, 16, 17, 15, 18, 18, 17, 18, 13,\n",
       "        18, 17, 18, 16, 15, 17, 16, 13, 17, 17, 17, 16, 17, 15, 17, 15, 10,\n",
       "        15, 18, 10, 18, 12, 17, 14, 14, 18, 16,  8, 14, 15, 18, 14, 14, 17,\n",
       "        10, 16, 17, 17, 13, 16, 17, 15, 10, 18, 17, 18, 16,  7, 18, 15, 10,\n",
       "        18, 17, 13, 13, 18, 16, 12, 13, 17, 18, 12, 12, 13, 17, 17, 18, 15,\n",
       "        16, 13, 12, 13, 18, 18, 17, 18, 17, 18, 18, 17, 16, 17, 18, 16, 12,\n",
       "        17, 18, 17, 15, 13, 18, 16, 18, 16, 13,  7, 13,  6, 11, 18, 12,  7,\n",
       "        11, 18, 15, 18, 17, 15,  4, 18, 18,  9, 11, 18, 14, 18, 18, 18,  7,\n",
       "        17, 17,  7, 15, 18, 16, 18, 18, 17, 13, 17, 16, 16, 12, 12, 18, 17,\n",
       "        18, 17, 18, 17, 17, 15, 15, 14, 14, 14, 17, 15, 14, 14, 12, 18, 16,\n",
       "        16, 18, 16, 12, 17, 12, 12, 11, 15, 12, 14, 16, 17, 12, 15, 13, 17,\n",
       "        17, 12, 18, 18, 16, 14, 18, 12, 18, 18, 16, 15, 17, 18, 15, 17, 16,\n",
       "        17, 18, 15, 16, 18, 18, 18, 16, 18, 16, 15, 16, 15, 14, 13, 18, 11,\n",
       "         9,  3,  8, 10, 14,  7, 16, 13, 13, 17, 13, 15, 16, 17, 11, 17, 14,\n",
       "        10, 17, 17, 16, 13, 11, 16,  6,  6,  6,  3, 10,  7,  7,  7,  6,  6,\n",
       "         8,  5,  3,  8,  8,  9,  9, 14, 16, 16, 16, 17, 16, 18, 18, 17, 16,\n",
       "        17, 14, 17, 14, 16, 18, 15, 16, 18, 16, 18, 18, 17, 15, 16, 18, 18,\n",
       "        18, 13, 16, 13, 16, 13, 16, 15, 18, 16, 13, 15, 18, 17, 12, 18, 18,\n",
       "        17, 18, 15, 17, 16, 18, 17, 16, 16, 16, 17, 15, 16, 18, 13, 16, 18,\n",
       "        13, 16, 15, 17, 18, 18, 17, 12, 18, 17, 16, 18, 10, 13,  7,  7,  4,\n",
       "        10,  6, 17, 10,  4,  4,  6, 16,  5,  6, 14, 10,  6, 15, 13, 10, 16,\n",
       "        12,  8, 11,  7, 17,  7,  8,  5, 13, 17,  4, 10,  6,  5,  7,  5,  6,\n",
       "         8,  3, 15, 16, 12,  8, 13,  7, 16, 16, 14,  5,  5, 11,  6, 16,  7,\n",
       "        18,  6,  7, 11,  9, 11, 17, 11, 12, 10, 16, 17, 18, 18,  9, 13, 11,\n",
       "         9, 14, 18, 16, 11, 11, 17, 11, 16, 15, 18, 16, 18, 17, 13, 15, 17,\n",
       "        17, 18, 10, 18, 18, 16, 18, 14, 17, 13, 17, 10, 18, 17, 17, 15, 18,\n",
       "        15, 18, 17, 18, 12, 18, 17, 14, 15, 18, 18, 18, 13, 17, 16, 16, 16,\n",
       "        17, 17, 14, 15, 18, 15, 16, 16, 17, 17, 15, 16, 15, 15, 15, 17, 16,\n",
       "        18, 15, 17, 16, 17, 16, 17, 15, 15, 18, 18, 12, 17, 17, 17, 18,  0]),\n",
       " 'coordinates': array([[[         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         ...,\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf]],\n",
       " \n",
       "        [[-23.44344   ,  21.700306  , -11.886679  ],\n",
       "         [-22.17428   ,  22.377975  , -12.129819  ],\n",
       "         [-21.1845    ,  21.454918  , -12.832834  ],\n",
       "         ...,\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf]],\n",
       " \n",
       "        [[-20.589098  ,  21.875069  , -13.489918  ],\n",
       "         [-19.524837  ,  21.226439  , -14.248699  ],\n",
       "         [-18.16996   ,  21.856543  , -13.943408  ],\n",
       "         ...,\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0.2629366 ,  -2.638183  ,  20.717941  ],\n",
       "         [  0.49513626,  -1.1980643 ,  20.753435  ],\n",
       "         [  1.9670401 ,  -0.8848784 ,  21.001106  ],\n",
       "         ...,\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf]],\n",
       " \n",
       "        [[  2.1660185 ,  -0.90462875,  20.612522  ],\n",
       "         [  2.5918398 ,   0.30654418,  21.305962  ],\n",
       "         [  3.832883  ,   0.90593815,  20.65292   ],\n",
       "         ...,\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf]],\n",
       " \n",
       "        [[         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         ...,\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf],\n",
       "         [         inf,          inf,          inf]]], dtype=float32),\n",
       " 'Tmranges': [(33, 55), (68, 90)],\n",
       " 'classes': {'Voltage-gated': 0,\n",
       "  'Ligand-gated': 0,\n",
       "  'Mechanically-gated': 0,\n",
       "  'Other gating': 1,\n",
       "  'plasma membrane': 1,\n",
       "  'endoplasmic reticulum': 0,\n",
       "  'endosome': 0,\n",
       "  'apical membrane': 0,\n",
       "  'golgi': 0,\n",
       "  'mitochondria': 0,\n",
       "  'lysosome': 0,\n",
       "  'K+': 0,\n",
       "  'Ca2+': 0,\n",
       "  'Na+': 0,\n",
       "  'Mg2+': 0,\n",
       "  'Cl-': 1,\n",
       "  'H+': 0,\n",
       "  'Zn2+': 0,\n",
       "  'F-': 0,\n",
       "  'Selectivity': 1}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/data/tyfei/datasets/ion_channel/Interpro_v2/kingdom/Eukaryota1_label.pkl_v2_1125.pkl\", \"rb\") as f:\n",
    "    bacteria = pickle.load(f)\n",
    "bac = bacteria[0]\n",
    "bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 22, 10,  9,  4,  4,  5, 19,  4, 18, 18, 19, 19,  4, 12, 17, 18,\n",
       "        11, 19, 10, 19,  7]),\n",
       " array([19, 19, 18,  6, 17,  8,  8,  9,  8, 12, 14, 20,  8, 18,  7,  4,  6,\n",
       "        18, 19,  7,  8,  4]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bac[\"seq_t\"][33:55], bac[\"seq_t\"][68:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseds._augmentsample(bac, (-1, -1), 100)[\"seq_t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train loader\n",
      "called new epoch\n",
      "[{'seq_t': tensor([[ 0,  9,  9, 13, 11,  9,  8, 13,  4,  4,  5,  7, 17, 12, 12, 17, 10,  8,\n",
      "          6, 12,  4,  6,  8,  4,  4, 14,  4,  9, 18, 21, 14, 11,  4, 10,  9, 11,\n",
      "         10,  7, 14, 10, 14,  5,  8, 16,  7,  5, 15, 10,  5, 16, 16,  6,  2]])}, tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1]]), {'seq_t': tensor([[ 0, 20, 20, 15, 19, 15,  4,  4,  8, 15, 16, 15,  6, 18, 12,  8,  4,  6,\n",
      "          5, 12, 15,  8, 18, 18, 16, 18,  8, 17, 12,  8,  7, 19,  4, 12, 12,  6,\n",
      "         11, 12,  5,  4, 19,  6, 22, 19, 10, 12, 17,  8,  4,  9,  8,  9, 17, 16,\n",
      "          9,  4, 16,  8, 15, 17,  5, 13,  4, 17,  7,  9,  7,  9,  9, 11, 15, 15,\n",
      "          9, 19, 17, 13,  4,  7, 13, 13, 12, 17,  9, 18, 10, 15, 16, 12, 17, 16,\n",
      "         13, 12, 15, 15, 16, 16, 17, 12, 11, 15,  9,  4, 10,  9, 11, 10, 12, 15,\n",
      "         17, 16, 15,  9, 12,  8, 15,  4, 10, 16, 12, 18, 17, 16,  8,  8,  8,  6,\n",
      "          9, 16, 10, 13, 18,  9, 17,  4,  5, 10, 16, 15, 14, 13, 20, 12,  9, 17,\n",
      "         10, 12, 17, 13,  6, 11, 10,  9, 18, 15, 15, 10,  4, 15,  8, 12, 18, 17,\n",
      "         16, 17, 15,  4, 15, 17, 16, 17,  8, 17, 15,  9,  9, 17, 17, 17, 13,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 20,  6, 15, 18,  6,  5,  5,  7,  7, 12,  7,  5, 12,  4,  4,  5,  5,\n",
      "          5, 11,  5,  6,  4,  4,  4, 22, 22,  4,  8, 10,  4, 12,  6,  5, 10, 15,\n",
      "         10, 14, 11,  4, 17, 11,  6,  9,  6, 11, 17,  5,  6, 15,  7,  5,  6, 10,\n",
      "         10, 14,  4, 14, 16, 17, 10,  8, 17, 14,  5,  5, 11,  5,  5, 14,  5, 10,\n",
      "         16, 21, 21, 21, 21, 16, 11, 14,  8,  8, 15,  8, 12, 14, 12,  4,  7,  4,\n",
      "          4, 14,  4, 10, 22, 10, 21,  4, 13, 13,  6,  7,  5, 23,  7, 10, 10,  4,\n",
      "          4, 13,  4,  5,  8,  5, 14,  8, 10, 12, 17,  7,  5,  7, 23,  4, 16,  5,\n",
      "         14, 14, 14, 14, 23, 14, 23, 14, 14, 10, 10,  8, 14,  6, 19, 10, 23, 13,\n",
      "          4, 18, 13,  6, 14, 19, 16, 10,  4,  5,  5,  5, 11,  6,  8,  6, 16, 10,\n",
      "          5, 13,  4,  5, 10, 12, 15,  7, 12,  7,  9, 14, 21, 13,  9,  5, 12,  6,\n",
      "         23,  8, 16,  5,  4, 10, 11,  4,  7, 13, 23,  5, 18,  6, 13,  9,  7,  4,\n",
      "         12,  7,  7, 12, 15, 10,  8, 11, 10,  7, 10, 12,  8,  6, 22, 13,  9,  4,\n",
      "          7, 23, 16, 13, 20, 18, 10,  4, 14, 10,  5,  5, 17, 16, 23,  4,  7,  8,\n",
      "         14,  4,  7, 14, 23,  5, 13,  7, 13,  6, 11, 11, 10,  5,  8, 18,  5,  8,\n",
      "          7,  9,  6, 22, 13,  9, 22,  6, 18, 14,  8, 12, 12, 11, 10, 20,  8, 14,\n",
      "         11,  6,  6, 12,  6,  8,  8,  6, 10, 14,  7,  7, 11, 14, 21,  5,  8, 18,\n",
      "         13,  4, 20,  5, 20, 11, 10,  6,  8,  4,  8,  7,  4, 20,  6,  5,  6,  7,\n",
      "          4, 13,  8, 10, 22, 10, 21,  4,  5,  8,  9,  9,  4,  5, 22,  4,  5,  8,\n",
      "         12,  7,  4, 22, 11, 21,  6, 12, 13,  4, 21, 11,  8, 11,  8, 19,  4,  5,\n",
      "         13,  5, 14, 11,  5, 17, 13, 16,  6,  5,  9, 13,  8,  6, 10, 13, 23,  9,\n",
      "          5, 11, 18, 10,  4,  5, 22, 15,  7,  4, 13, 20,  7, 14, 14,  8, 15, 16,\n",
      "         13, 20,  5, 11, 10,  8, 11, 15, 14, 14,  6,  6, 13, 12,  4, 10,  4,  6,\n",
      "         11, 11, 10,  8,  4, 13,  8, 18, 12, 13,  5, 11,  6, 12, 13, 22, 16,  8,\n",
      "         10, 10,  4, 21, 14,  4,  5,  5, 11,  6,  4,  4, 23,  4,  8,  6, 17,  5,\n",
      "         17, 20, 14, 10, 13,  5, 13,  7,  4,  5, 10, 19,  6,  8, 18,  9, 10, 18,\n",
      "         19,  8,  5, 21, 14, 19, 18,  5, 15,  5,  8, 10,  8, 15,  8, 16,  2]])}, tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1]]), {'seq_t': tensor([[ 0, 20, 17, 12,  8, 23, 20, 23, 17,  9, 23, 12, 23, 12,  7, 20, 21, 20,\n",
      "         21,  8, 19,  6, 19,  4, 19, 20,  6, 11, 23, 12, 22,  7, 14,  7, 19,  6,\n",
      "         23,  4, 19, 20,  6,  5, 23, 12, 22,  7, 14,  7, 19,  6, 23,  4, 19, 20,\n",
      "          6,  5, 23, 12, 22,  7, 14,  7, 19,  6, 19,  4, 19, 20,  6, 11, 23, 12,\n",
      "         22,  7, 14,  7, 19,  6, 19,  4, 19, 20,  6, 11, 23, 12, 22,  7, 14,  7,\n",
      "         19, 11, 21,  5,  5, 10,  5,  8, 10, 11,  6, 19, 10, 23,  7, 14,  5,  5,\n",
      "          5, 10, 19, 20, 21,  4, 19, 10, 16, 10, 19, 11, 10,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 11, 18,  5,  7, 16, 11, 21,  5, 10,  9,  5, 14, 22, 13,  5, 14, 18,\n",
      "         10,  9,  4,  9,  7, 12,  4, 11,  6, 10,  7,  4,  4,  5,  8, 11,  8, 10,\n",
      "         16, 12, 14,  4, 10,  8,  5,  7,  8,  4,  6,  5, 11,  6, 22, 12,  6, 14,\n",
      "         16,  7,  7,  8, 21, 21,  6, 17, 13, 22, 18, 16, 16, 10,  7,  5,  4,  9,\n",
      "         10, 19,  6,  7,  5, 12, 15, 20, 18, 11, 17, 14,  5, 13, 18,  7,  5, 19,\n",
      "          5,  4,  8,  4,  5,  5, 17, 23,  7, 15,  7, 14, 15, 20, 16, 12,  6, 19,\n",
      "          5, 23,  7, 15, 23,  6,  8,  9,  4,  5,  7, 14,  8,  7,  7, 14, 12,  5,\n",
      "          5, 18,  7, 14, 14, 23, 14, 23,  6, 17,  6, 10, 12,  9, 18, 11, 10,  6,\n",
      "          8,  8, 17,  8,  5, 13,  5, 15, 17, 11, 20,  7,  8,  4,  7, 14,  4, 10,\n",
      "         15, 20, 10,  9, 12,  9,  7,  5, 11, 12,  9, 15,  7, 22,  6, 14, 18,  9,\n",
      "         10,  4, 11, 17, 17, 15,  9, 20, 14, 13, 10, 11, 17, 14,  7,  5, 22, 13,\n",
      "          6,  4, 12, 13,  7,  8, 14,  7, 22, 22,  9,  9, 17,  6,  7,  6,  6,  9,\n",
      "          6, 15, 11, 18, 13,  7,  6,  7,  8, 20, 15,  5,  5, 12, 10, 11,  4, 12,\n",
      "         12,  8,  8,  8, 12,  4, 12, 11,  7, 12, 12, 20, 10,  7,  6, 18, 15, 10,\n",
      "          8,  7, 11, 18,  4,  4, 18,  6,  4,  5,  4,  4, 12,  8, 18, 23, 12,  4,\n",
      "          7, 10, 10, 12,  4, 16, 13, 18, 17, 14, 18, 14,  8, 15, 12, 17, 13,  8,\n",
      "          9,  4,  4,  8,  8,  8,  9, 16, 14,  9, 14,  7,  6, 17, 16,  7,  5,  4,\n",
      "         12, 14,  7,  8, 16, 10, 18,  4, 12,  8, 11, 19,  6, 11, 10,  6, 13, 21,\n",
      "         11, 14, 20, 20, 19, 19,  5, 10,  4,  5,  5,  8,  4,  6,  7, 14, 11, 21,\n",
      "          7, 22, 10, 12, 21,  8,  5, 11, 21, 21,  9,  4,  9, 13,  4, 15, 15,  6,\n",
      "         15, 18, 22,  6, 18,  4, 14, 13, 19,  7, 13,  4,  5, 18,  8, 10, 22, 10,\n",
      "          6, 19, 15, 19,  7, 18, 16, 14, 21,  7, 14, 12, 11, 11,  8,  6,  9,  8,\n",
      "         19,  8,  4,  8, 14,  8, 22, 10, 22, 12, 10,  8, 12, 15, 19,  6,  6, 17,\n",
      "         10, 11,  4,  4,  5, 16, 18,  7,  8,  5,  4,  5, 19, 11, 18,  4, 14, 21,\n",
      "         22, 10, 12,  6, 23,  4, 14, 13,  8, 13,  4, 14, 10,  8,  5, 13,  6, 16,\n",
      "          8,  4, 12,  9, 15, 10,  9, 17, 11,  6,  9, 18,  9, 20,  6, 22, 23, 23,\n",
      "          6,  8,  5,  8,  9,  8,  7, 12, 14, 13, 22, 12, 15,  9, 17, 19, 14, 10,\n",
      "         12, 11,  8,  9, 13, 21, 16, 17,  2]])}, tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1]]), {'seq_t': tensor([[ 0, 20, 15,  5, 12, 12, 15, 12, 12, 20,  4,  8,  4, 12,  8, 11,  7, 18,\n",
      "         20,  4,  4,  8, 21, 17,  8, 19,  8, 16, 11, 20,  5, 17, 13, 18, 15, 19,\n",
      "         17, 15,  9, 12, 11,  4, 15,  7, 12, 13, 15, 12, 12,  9,  9, 14, 19, 11,\n",
      "         17, 19,  5, 12,  5,  4,  9, 17,  6, 13, 11, 19, 19, 23, 11, 18,  6,  9,\n",
      "         18,  5,  4, 12, 15, 12, 13, 13,  4, 12, 12, 18, 10, 15, 17, 18, 22,  6,\n",
      "         10, 19, 15, 18, 13, 10,  7, 12, 15, 15,  9,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 12,  5,  9, 17, 10,  6,  5, 13,  9,  7, 23, 21, 11, 11, 13,  6, 16,\n",
      "          6,  4,  6,  8, 20, 18,  7,  6,  8, 17,  6, 11,  5,  7,  5,  7, 11,  8,\n",
      "         11, 11,  6,  6, 13,  6, 23, 10, 14, 10,  4, 13, 18, 21, 13, 18, 18, 19,\n",
      "          4,  7, 12, 23, 11,  7,  8, 11,  7,  6, 19,  6, 13,  4,  5, 14,  8, 11,\n",
      "         13,  4,  6, 10, 20,  4, 20,  7,  4,  4, 12, 11,  7, 13,  4,  7,  7,  4,\n",
      "         11, 12,  9, 11, 15,  9,  4, 12, 10,  4,  4, 10,  6, 11, 17, 15, 18, 16,\n",
      "         10, 10,  7, 19, 12, 14, 15, 15, 21,  2]])}, tensor([[1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]]), {'seq_t': tensor([[ 0, 20,  5,  5, 14, 10,  6,  7, 18,  4,  7,  5, 14, 14, 16,  6,  5, 23,\n",
      "         14,  9, 13,  7, 10, 14,  4,  4, 11, 19,  4, 18,  6, 15, 14,  4,  5, 18,\n",
      "         18,  4, 18,  8, 19,  5,  4,  4, 18,  4, 18, 18, 19,  8, 19, 18,  8, 19,\n",
      "          8, 18, 18, 12,  4, 19,  4, 18, 18, 19,  7,  4, 23, 18,  4, 12, 18,  8,\n",
      "          2]])}]\n",
      "[{'seq_t': tensor([[ 0,  9, 19,  4,  8, 19, 15,  6, 12, 17,  4, 12,  5, 13, 12,  6,  6, 16,\n",
      "          4,  6,  4, 22, 12,  6, 12,  8,  7,  4, 11, 23, 23,  9,  7,  4,  9,  4,\n",
      "          7,  4,  4,  4,  6, 16, 11,  7, 18, 15, 10, 12, 11, 20, 15,  9,  8, 10,\n",
      "          7,  9,  2]])}, tensor([[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]]), {'seq_t': tensor([[ 0, 20, 11,  ..., 19,  8,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 20, 10,  6, 17,  6, 14, 23,  4, 12, 21, 18, 14, 18, 10,  9, 10, 12,\n",
      "         15,  6,  4, 19, 11,  4, 11,  6, 12, 11, 19, 12,  8,  5,  7, 15,  8, 12,\n",
      "          4,  4, 12,  6,  4,  6,  8, 18, 12,  6,  6,  7,  4, 10, 21,  4,  4,  8,\n",
      "         23, 23,  4, 17, 13,  8,  7, 12, 15,  6, 18, 14,  7,  6, 11,  4,  5,  7,\n",
      "         17,  4,  5,  6, 23, 18,  4, 12,  6, 12,  4, 19,  6, 18, 18, 13, 10,  6,\n",
      "         11,  4, 12, 17, 14,  9,  4, 10, 12, 18,  4, 11, 12,  6,  4, 23,  6,  6,\n",
      "         18, 11, 11, 18,  8, 11, 18, 20, 17,  9, 17, 18,  4, 20,  5, 16,  6,  5,\n",
      "         16, 18, 18, 11,  4,  4,  4, 19,  5,  7,  4,  8,  4,  5,  6,  6, 18,  4,\n",
      "          5,  7, 22,  4,  6, 19, 11,  4,  4, 15,  5,  7,  2]])}, tensor([[1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), {'seq_t': tensor([[ 0, 20, 17,  4, 18,  8,  7, 14,  6,  7, 12,  8, 12, 15, 23, 15,  8,  6,\n",
      "          7,  4,  7,  9,  4, 15, 10, 18,  6, 15, 23, 18, 12, 19, 18, 13,  6, 19,\n",
      "         12,  4, 12,  4,  5,  7, 12,  9, 13,  6, 19,  4, 17, 18, 19, 19, 21, 13,\n",
      "          7, 23, 19,  6,  6, 13, 19, 10, 18, 10,  9,  9, 18,  7,  4,  8,  4, 13,\n",
      "          9, 20, 13,  8,  4, 13, 22,  6, 12,  6, 13, 14,  6, 23, 18,  6, 19,  9,\n",
      "         13, 18,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 20,  8, 15, 11, 18,  5,  9, 18,  8,  4, 21,  9, 11,  4, 16, 16,  5,\n",
      "          4,  9,  6,  4,  6, 18, 11, 11, 14, 11, 14,  7, 16,  9, 16,  8, 12, 14,\n",
      "          5,  5,  4,  9,  6, 15, 13,  4,  4,  7,  8,  8, 16, 11,  6,  8,  6, 15,\n",
      "         11,  5,  5, 18,  4,  4, 14, 11,  4, 21, 17,  4,  5,  6, 16,  9, 11, 18,\n",
      "          7, 14, 18, 15,  9, 10, 20, 15,  5,  7, 11, 16, 14, 17, 12,  4,  7,  4,\n",
      "         23, 14, 11, 10,  9,  4,  5, 16, 16,  7,  8, 16, 13,  5, 12,  5, 18,  7,\n",
      "         10, 21, 20, 15,  6,  7, 10, 12,  5,  5, 12, 20,  6,  6, 20, 14, 18,  6,\n",
      "         15, 16, 12, 16, 16,  4, 15,  6,  5, 16,  7, 12,  7,  5, 11, 14,  6, 10,\n",
      "          4,  4, 13,  4,  7, 17, 10, 10, 16,  4, 15,  4, 13, 15,  7,  9,  5,  4,\n",
      "         12,  7, 13,  9,  5, 13, 10, 20,  4, 13,  4,  6, 18,  8,  9, 13,  4,  9,\n",
      "          5, 12,  8, 13,  4,  5,  6, 17, 10, 10, 16, 11,  4, 20, 18,  8,  5, 11,\n",
      "         18,  5, 13, 10, 12, 12, 10,  4,  5,  9, 10, 20, 20, 17,  9, 14,  9, 10,\n",
      "         12,  5, 12,  9, 11,  6, 21,  8, 11, 17, 11, 13, 12, 11, 16, 11,  4, 21,\n",
      "         22, 11, 13,  6, 18,  9, 21, 15, 15, 15,  4,  4, 11, 21, 22,  4,  5, 13,\n",
      "          9, 11,  4, 13, 16,  5,  7,  7, 18,  5,  8, 11, 16,  9, 13, 11, 13, 20,\n",
      "          4,  5,  9,  9,  4,  5,  9,  5,  6, 21,  8,  7,  7,  5,  4, 21,  6,  5,\n",
      "         20, 14, 16, 11,  7, 10, 17, 10, 10,  4, 10,  8, 12, 10,  9,  6, 10,  5,\n",
      "         15, 12,  4,  7,  5, 11, 13,  7,  5,  5, 10,  6,  4, 13,  7, 14, 11, 12,\n",
      "          8, 21,  7, 12, 17, 18,  6,  4, 14, 20, 15, 21,  9, 13, 19,  7, 21, 10,\n",
      "         12,  6, 10, 11,  6, 10,  5,  6, 10, 11,  6, 16,  5, 12, 11,  4,  5, 11,\n",
      "         19, 10,  9, 10,  6, 15, 12, 10,  5,  4,  9, 13, 19,  4, 13,  5, 10,  4,\n",
      "          8,  7,  8,  9, 12,  9,  6,  4,  9, 14,  8, 14, 14, 14,  5, 10,  8,  6,\n",
      "         10, 13,  6,  6,  6, 10,  6, 10,  6,  6, 17,  6,  6, 10, 13,  6, 10, 10,\n",
      "          6,  6,  6, 18,  6,  6,  6, 10, 10, 18,  9,  6,  9,  8, 17, 18, 15, 10,\n",
      "         10,  9,  6,  6, 10, 13, 13, 10, 14, 10, 10,  8, 18, 13, 13, 15, 14, 10,\n",
      "          6,  9, 10, 14, 18,  6,  6,  9, 13, 10, 14, 10, 10,  9, 18, 17,  8, 13,\n",
      "         10, 14, 10, 10,  9,  6,  6, 18,  9, 13, 10, 14, 10, 10,  9, 18, 17,  8,\n",
      "         13, 10, 14, 10, 10,  9,  6,  6, 18, 17, 13, 15, 14, 10, 18, 13,  8, 17,\n",
      "         13, 13, 17, 10,  6, 17, 10,  7, 13, 19, 15, 14, 10, 10,  9, 17,  6, 18,\n",
      "          6, 13, 10, 14, 15, 10,  8, 18,  6,  6,  9, 13, 10, 14, 10, 10,  8, 18,\n",
      "         13, 13, 15, 14, 10,  6,  9, 10, 14,  8, 18,  6,  6,  9, 13, 10, 14, 10,\n",
      "         10,  9, 18, 17,  8, 13, 10, 14, 10, 10,  9,  6,  6, 18, 17, 13, 15, 14,\n",
      "         10, 10,  8, 18, 13, 13, 15, 14, 10,  6,  9, 10, 14,  5, 18,  6,  6,  9,\n",
      "         13, 10, 14, 15, 10,  8, 18,  6,  6,  9, 13, 10, 14, 10, 10,  8, 18, 13,\n",
      "         13, 15, 14, 10, 10, 15, 18, 13, 10,  2]])}, tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1]]), {'seq_t': tensor([[ 0, 20,  4,  7, 18, 17, 18,  4,  4,  4, 18, 11,  8, 11,  7,  5,  7, 21,\n",
      "         10, 14, 12, 14, 17, 19, 13, 14, 13, 13, 15, 18, 22, 18,  8, 14, 14,  8,\n",
      "          5, 13,  5,  6, 15,  4, 11, 15, 20, 19, 13,  4, 21,  6,  6, 18, 15, 22,\n",
      "         23, 21,  6,  8,  7,  4, 10, 19,  6, 18, 22,  9,  6,  4, 17,  5, 18, 11,\n",
      "         12, 19, 20, 19,  4,  9, 15, 13,  4, 19, 10,  4, 10, 17, 15,  7, 11, 12,\n",
      "         21, 11,  6, 16, 19,  7, 19,  8,  4,  7,  6,  5, 21,  4,  4, 11, 17, 13,\n",
      "         10,  9, 18,  9, 17, 18, 17,  8, 15, 21,  8,  4,  5, 23, 18, 14, 13, 11,\n",
      "          6,  4, 14, 15, 23, 11,  4, 10,  9, 12, 23, 15,  5, 10, 11,  6, 15, 15,\n",
      "         14, 18, 14,  4,  7,  6, 11, 23,  9, 17, 15, 13, 18, 10, 12,  7,  9, 10,\n",
      "          6,  5, 13, 18, 13, 14, 16, 23,  7, 23, 19,  6, 13, 23, 13, 21, 13, 23,\n",
      "          9, 21,  8, 11,  4, 21, 17, 19, 15,  6, 12,  9, 18,  8,  4,  6, 19, 10,\n",
      "         18, 17,  8, 15, 14, 14,  4, 15,  7,  8,  7, 13, 10, 19,  6,  7,  6, 23,\n",
      "         12, 13,  6,  4, 11,  8,  4,  5, 21, 11, 19, 22, 16, 11,  5, 17, 19, 21,\n",
      "         14,  5, 19,  4,  5, 17,  8,  6,  7,  4, 16, 11, 18, 10,  4, 10, 15,  9,\n",
      "         18,  9, 13,  4, 10, 14, 11,  7,  4, 17,  7, 11, 12,  8, 17, 10, 19,  7,\n",
      "         23, 13, 13,  5, 20,  8, 10, 10, 19, 21,  5,  4, 13,  6,  7,  7, 12, 19,\n",
      "         14, 11,  5, 12, 13, 23, 11,  9, 18,  7, 14, 18, 13, 18, 21, 11,  4, 12,\n",
      "         23, 15, 13,  5,  4, 18,  7, 14, 20, 23, 14, 11,  9,  7, 18, 15, 14, 20,\n",
      "          7,  5, 10, 19, 10, 19, 19, 13, 12, 11, 17, 21, 10, 10,  7,  7,  6,  5,\n",
      "          4, 13,  4,  7,  6,  5, 12, 16, 17, 16, 11,  6, 13, 11, 20,  7, 13, 21,\n",
      "         18, 18, 13, 21, 18,  5,  6, 17,  5, 11, 10, 21, 21,  7,  7, 10, 10,  8,\n",
      "         12,  8, 13, 22, 12,  8,  6,  7,  7,  9, 15,  4, 18,  9, 14, 12, 15, 19,\n",
      "          4,  7, 16,  9, 12,  4, 13,  4, 12, 10, 14, 12,  4,  7,  9, 12,  5,  6,\n",
      "          9, 12, 18, 15,  4, 12,  4, 13, 12,  4, 18, 13,  4,  7,  6, 12,  4, 13,\n",
      "          8,  4,  4, 15, 15,  4,  9, 13, 16, 12, 21, 17,  4,  7, 13,  4, 12, 10,\n",
      "          7,  4,  4, 21, 21,  4, 18,  8, 12,  7, 11, 10,  4,  4,  4,  8,  4,  9,\n",
      "         23,  8, 19, 21,  4, 18,  9,  5,  4, 18,  4,  7,  5, 18, 18, 10, 19, 12,\n",
      "         18, 21,  8, 12, 19, 12,  5,  8,  4,  4, 11, 18, 18, 12, 18,  4, 12, 12,\n",
      "          6, 18,  9, 10, 12, 19, 14,  8, 14,  7, 19, 19, 19,  4,  8,  9, 15, 18,\n",
      "         10,  4,  8,  7, 23,  8,  7, 14,  7, 10, 19,  4, 20, 17,  4, 16,  6,  5,\n",
      "         19,  5,  7,  4, 15, 13,  4, 18,  9, 19,  4,  8,  6, 18, 20, 14, 17, 18,\n",
      "          4, 19, 16,  6,  8, 19, 13,  4, 18, 12, 12,  5,  8,  4, 20,  7,  8,  5,\n",
      "         18, 12, 12,  8, 18, 18, 18,  8, 13,  7,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 20, 19, 17, 15, 12, 12, 23, 22, 22,  8,  6,  6,  7, 11,  8,  5, 12,\n",
      "          5, 23,  4, 18, 11, 12, 16,  7, 19,  6, 15,  9, 17, 23, 10,  7, 12, 18,\n",
      "         12, 13, 11, 23, 17,  9, 21, 14, 13, 11, 19, 10, 18, 18, 12, 13, 23,  9,\n",
      "         15,  4, 18,  6,  4, 15, 12,  9, 12, 12, 11,  6, 12, 15, 13, 19,  7, 14,\n",
      "         15, 12,  9,  8, 19, 15, 17, 12, 18,  4, 13, 10,  5,  4, 19, 18, 16, 14,\n",
      "         17, 15, 17, 11, 21, 19, 15,  8, 12,  7, 13,  7, 22,  9, 22, 19,  5,  8,\n",
      "          4, 17,  7,  8, 18,  6,  5,  7, 23,  8, 11, 13,  4, 15, 10,  6,  7, 10,\n",
      "          9, 15, 22, 16,  4,  9, 17, 11, 19, 11, 21, 16,  7, 18,  6, 18,  9, 18,\n",
      "         13, 15, 15,  9, 18, 17, 10,  5, 20,  5, 20, 15,  4, 17, 21, 14, 15,  5,\n",
      "         17, 14, 12, 18, 14,  4,  4, 20, 19,  6, 18, 13, 15, 15, 13, 23, 12,  9,\n",
      "         11,  4, 17,  4,  4,  6,  7, 15, 12, 14, 16,  5, 19, 15, 13,  6,  4, 10,\n",
      "         17, 17, 17, 23,  4,  6,  8,  9, 19,  6, 23,  7, 16,  6,  6, 12,  6, 19,\n",
      "         22, 16, 20, 12, 21, 10, 11, 13, 14,  5, 20, 19, 18, 10, 10,  5, 10,  4,\n",
      "          9, 21,  4,  4, 11, 13, 15,  5,  6, 21, 16,  7, 11, 20, 23, 15, 13, 16,\n",
      "          8, 17, 13,  5, 15, 15,  4,  9, 17, 15, 15, 13,  5,  4,  4, 18,  4,  4,\n",
      "         14, 21, 14, 13, 19, 14, 16, 17, 15, 11,  7,  4, 13,  7, 15,  5, 10,  9,\n",
      "         14, 15, 14, 20,  7, 13, 23,  8,  6, 18, 23,  6, 11, 18, 13,  4,  8, 15,\n",
      "         10, 17, 14,  7,  9, 15,  9, 12, 17, 19,  2]])}, tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1]]), {'seq_t': tensor([[ 0, 20,  7, 11, 17, 16,  8, 11, 14,  8, 19, 18,  7,  4, 12, 11, 15, 21,\n",
      "          8,  4, 17, 23, 18, 19,  7, 20, 23, 16, 16, 13, 12, 12,  5, 15,  8, 11,\n",
      "         21, 17, 13, 12, 18,  8, 19,  9, 12, 18,  4,  8,  4, 23, 17, 18, 18, 12,\n",
      "         23,  4,  4, 15,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 20, 14, 13,  9, 10, 14, 14, 21,  4, 10, 22, 10,  4,  4,  6,  4,  7,\n",
      "          5,  7,  6,  6,  5, 12,  6, 11,  5,  5, 10,  5,  4,  4,  5,  9,  5, 18,\n",
      "         14,  5, 21, 13,  6, 12,  8, 22,  7, 12,  4,  7, 12, 17,  7,  7,  6,  5,\n",
      "         18, 23,  4,  6,  4,  4,  4, 13,  5,  4,  5, 10, 10,  6, 14, 13,  7,  6,\n",
      "         10, 10, 10, 11, 12, 10,  4,  4,  7,  6, 11,  6,  7,  4,  6,  6, 18, 11,\n",
      "         11, 19,  8, 11,  4,  5, 13, 13, 11,  5, 16,  4,  4, 13, 11,  6, 10, 22,\n",
      "          6,  5,  6,  8,  6, 19,  5,  4,  4, 11,  7,  7,  4,  6,  4,  4,  5,  7,\n",
      "          5,  5,  6,  7, 22,  7,  5,  8,  7, 11, 15, 14, 10,  9,  9, 17, 10,  2]])}, tensor([[1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), {'seq_t': tensor([[ 0, 20, 11, 10, 10, 14,  8, 20, 13,  6,  9, 10,  9, 20,  7, 17, 10,  4,\n",
      "          7,  5, 15,  8,  9, 10,  4,  9, 11,  5,  5, 10, 12, 15,  5,  8,  7, 13,\n",
      "          5, 12,  8,  8,  7,  4,  5, 18, 20, 12,  6,  5, 11,  4,  6, 22,  4, 12,\n",
      "          8,  6, 10, 13,  8, 12, 11,  4,  6, 16,  4, 22, 12, 19,  5,  5,  5, 18,\n",
      "         12,  7,  7,  4,  6,  7, 16, 12,  4, 19, 10, 10, 21, 10,  4, 16, 10, 10,\n",
      "          4, 10,  5,  4, 16,  4,  6,  7, 13, 21, 14,  5, 14, 10, 10,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 20, 17,  4, 19, 17,  4,  7, 10, 13,  5,  7, 10, 14,  8, 19,  5, 11,\n",
      "         12,  8, 14,  8,  7, 13, 13, 14, 15,  7, 13, 12, 17, 18,  7,  5,  4,  8,\n",
      "         23, 19,  5, 11,  4,  8,  7,  4,  4, 19, 19,  4, 16, 10,  7, 15, 16, 14,\n",
      "         19,  4,  8, 20,  4, 18, 21, 12,  4, 18, 23,  4,  4, 16,  7, 23, 20,  7,\n",
      "         12, 22,  4, 12, 18,  8,  5, 17, 18, 19,  7,  8,  4, 18,  5, 16, 23, 12,\n",
      "          4,  4,  7, 23,  5,  4,  6, 23, 18,  4,  9, 10, 11, 11,  4,  8, 12, 15,\n",
      "          4, 10,  8, 20,  5, 14, 18, 20,  8, 20,  5, 13, 17, 18,  5, 12, 12, 15,\n",
      "         11, 11, 23, 17, 17, 19,  7, 18, 14,  7,  9, 10,  8,  8, 13, 17,  4,  7,\n",
      "          7,  4, 11, 11,  8, 10,  6, 12, 19,  8, 17,  6,  7, 18, 20, 15,  6,  5,\n",
      "         12, 11,  7,  8, 13,  8,  5,  4,  7, 12,  8,  4, 18, 15,  8, 16,  8,  4,\n",
      "          4,  4, 13, 10,  7,  9, 21,  6, 19, 13, 19, 11,  7, 18, 12, 19, 12, 17,\n",
      "          8,  7, 12,  4, 16, 17,  7, 15, 14, 11,  7,  8,  7,  7, 17, 11,  9, 18,\n",
      "         11, 13,  7,  9,  4,  2]])}, tensor([[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]]), {'seq_t': tensor([[ 0, 20, 11, 20, 17, 17,  4, 12, 11,  4,  7,  9,  5,  4, 22,  9,  8,  6,\n",
      "         12,  4, 14, 11,  4,  4, 12,  4,  4, 12,  6, 22,  4,  8,  5, 10, 18,  5,\n",
      "         10, 17, 15, 10,  4, 11,  5,  4,  4,  6, 12,  5,  9,  9,  7,  7, 15, 22,\n",
      "          8,  9,  7, 11, 18, 13,  6,  6, 16, 11, 16, 15,  5, 16,  5, 12, 15, 20,\n",
      "         12, 11, 13, 19,  4, 20, 15,  5, 13, 15,  5, 21,  4, 18, 11,  5, 15, 16,\n",
      "         12, 13, 13,  5, 12,  9, 22,  5,  7, 15,  9, 20, 15, 15,  6, 11,  4, 15,\n",
      "          2]])}]\n",
      "[{'seq_t': tensor([[ 0, 20, 21, 12, 18, 17, 17,  6, 15, 19, 15,  8,  4, 21,  5, 16,  8,  9,\n",
      "          6,  9,  4, 17, 11, 12, 10, 16, 11,  7,  4,  8, 13,  5, 13, 18, 19, 14,\n",
      "         11, 19, 21,  4,  5, 14,  9, 11,  6,  4, 18, 17, 13, 14, 17,  6,  4, 12,\n",
      "         18, 13,  6,  9, 15, 19, 21, 12, 18,  5, 16, 22, 18, 14, 19,  6,  5,  4,\n",
      "         21,  6, 20, 15, 21, 22, 16, 21, 18, 20, 11, 15, 13, 18, 16, 11, 18,  9,\n",
      "         15,  6,  9,  7,  4, 12, 14, 13,  9,  4, 18,  9,  8, 21,  6, 23, 19,  8,\n",
      "          6,  6,  5, 12,  4, 22, 16, 13, 15, 12,  5,  5, 18, 19, 11,  6, 17, 11,\n",
      "         10, 10, 14,  8, 13, 17,  5, 10,  7, 14, 21, 16, 17, 12,  5, 12, 18, 13,\n",
      "         15,  8,  6, 15,  4,  4,  9, 15, 10, 23, 12, 12, 17, 16, 14, 14, 16,  6,\n",
      "         19, 11,  9, 21,  7, 10, 13, 14, 15, 14, 19, 12, 11, 23, 13,  6, 15, 12,\n",
      "         10, 18,  7,  4,  6,  5, 16, 10, 13, 17,  4, 11,  6, 11,  5,  4, 12, 19,\n",
      "          9, 20, 13, 17,  4,  9, 17, 11, 14, 10,  4, 12,  6,  9,  4,  5,  7, 16,\n",
      "         13, 18, 13, 17,  8, 21,  7, 18, 20, 22,  9, 23, 14, 13,  4, 18, 10,  4,\n",
      "         13,  9, 15, 13,  4, 18,  7, 22,  8, 14, 16,  6, 15,  4, 10,  9,  8, 21,\n",
      "         16, 18, 16, 17, 17, 19, 21,  5, 11, 19,  5,  4,  6, 15,  4,  9,  6, 17,\n",
      "          8,  4,  5,  5,  9, 21, 12,  9,  9,  4, 13, 19,  6, 18, 13, 18, 19,  5,\n",
      "         14, 16, 11,  7,  9, 17,  8, 13, 10, 12, 20, 18,  6, 22,  7,  6,  4, 14,\n",
      "         13,  4, 11, 19, 14, 11, 13, 10, 18, 16, 22, 21,  8, 20,  4, 11,  4, 14,\n",
      "         10, 15,  4,  8,  4, 15, 17,  6, 15,  7,  7, 16, 16, 14, 12,  7, 21,  4,\n",
      "         17, 13,  4, 16,  5,  7,  9, 12,  8,  9, 17, 18,  5,  8,  4, 17,  4, 13,\n",
      "         11,  5, 19,  4, 16, 12,  8,  7,  9, 17, 16, 14,  4,  7,  4, 13, 18, 18,\n",
      "         11, 17, 13, 15,  6, 16, 16,  4, 11, 20, 10, 18,  9, 17,  6,  7, 18,  8,\n",
      "          4, 13, 10,  8, 16,  8,  9, 16, 11,  9,  4, 20,  9, 15, 18,  6,  8,  7,\n",
      "         10, 21, 23, 15,  4,  9, 15,  4,  9, 11, 12,  9, 12, 18, 18, 13, 10,  8,\n",
      "         12,  7,  9, 12, 18,  4, 17,  6,  6,  9, 15,  7,  4, 11,  8, 10, 18, 18,\n",
      "         12,  5, 17, 10, 15, 17, 12, 12, 15,  5,  8, 10,  4,  7, 10,  5, 15,  7,\n",
      "          5, 17, 12,  5,  8, 12,  8,  7,  9,  2]])}, tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1]]), {'seq_t': tensor([[ 0, 20, 21,  7, 13,  4, 12, 12,  6,  5,  4,  6, 11, 18,  4,  6,  7,  4,\n",
      "         12, 11,  5, 19, 17,  5, 19, 21, 15, 17, 15, 10, 13, 11, 18, 16, 13, 12,\n",
      "          7,  9,  9,  4, 15,  8,  9, 10, 13, 13, 19, 15, 15, 16,  7, 15, 21,  4,\n",
      "         16, 10, 10,  9,  6, 15, 12,  9, 10,  6, 12, 11, 15, 20, 10,  4, 13, 12,\n",
      "         17,  4, 12, 22,  5, 12,  7,  7,  4,  4,  7,  5,  6, 20,  5, 11,  5, 19,\n",
      "          8, 20, 17, 15, 16, 15,  4,  9, 15,  4, 15,  4, 11, 21, 14, 15,  4,  5,\n",
      "         11,  7,  4,  9, 11,  5,  6,  9,  4,  5,  4, 15,  5, 11, 11, 19, 16,  5,\n",
      "          8,  4, 13, 13, 15,  9,  6,  8, 15, 15,  4, 19, 13,  5, 11, 13,  9,  7,\n",
      "         18, 19, 16,  4, 16, 15,  4, 19, 14, 17,  4, 14, 12, 13, 10,  5, 11,  7,\n",
      "         10, 17, 12,  7, 16, 21,  9, 19,  9, 15,  4,  7,  5, 13,  8, 15, 15,  9,\n",
      "          5,  2]])}]\n",
      "[{'seq_t': tensor([[ 0, 17,  4,  4, 17, 10, 19, 14, 19, 13, 12, 14, 19, 17, 12,  5, 12,  7,\n",
      "          6, 13, 19,  5, 12, 21, 17, 18, 15, 19, 11, 13, 15,  5, 12,  4, 13, 19,\n",
      "         12,  9, 17, 17, 15,  4, 15, 15, 12, 17, 12,  9, 16,  6, 19,  8, 15, 23,\n",
      "          8, 12, 23, 12,  7, 13, 10, 11,  8, 12, 12, 11,  8, 13, 15,  6, 12, 22,\n",
      "         15,  8, 20, 15, 17, 11,  9, 12,  9, 23,  4,  2]])}, tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1]]), {'seq_t': tensor([[ 0, 20, 15, 11, 14, 12,  9, 14, 17, 11, 18, 10,  4, 12, 16,  9, 10, 15,\n",
      "         12,  4, 10, 15, 15,  7,  5, 18, 20,  4,  6,  7, 19,  7,  5,  4,  7,  5,\n",
      "          5,  4,  7,  4,  5,  4, 12, 18,  5, 14, 17, 17, 13,  5, 15,  5,  4, 12,\n",
      "          6, 12, 18, 18, 11, 14,  7, 12,  4, 12,  7,  6,  6,  7, 17,  4,  6,  6,\n",
      "          7, 15, 15, 13, 12, 11, 17, 12,  8, 10,  4, 11,  5, 16, 12,  5,  9, 13,\n",
      "         23, 17,  4, 11,  5, 22, 17,  8,  4, 16, 15, 16, 16, 13,  9,  5, 10, 15,\n",
      "         12, 20,  8,  9,  4, 14, 11, 18, 14, 14, 14, 18, 14, 15, 15, 14,  2]])}]\n"
     ]
    }
   ],
   "source": [
    "dl = ds.train_dataloader() \n",
    "for i, d in enumerate(dl):\n",
    "    print(d) \n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][\"seq_t\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20,  5, 10, 18,  7,  5,  4,  7,  4,  4,  6,  4,  4,  8,  4,  8,  9,\n",
      "          4, 13,  5, 12, 16, 10, 10, 14, 15, 12, 16,  7, 19,  8, 10, 21, 14, 14,\n",
      "          9, 13,  6, 15, 14, 17, 19,  4, 17, 23, 19,  7,  8,  6, 18, 21, 14, 14,\n",
      "         16, 12,  9, 12, 13,  4,  4, 15, 17,  6,  9, 15, 12, 15,  8,  9, 16,  8,\n",
      "         13,  4,  8, 18,  8, 15, 13, 22,  8, 18, 19,  4,  4,  8, 21,  5,  9, 18,\n",
      "         11, 14, 17,  8, 15, 13, 16, 19,  8, 23, 10,  7, 15, 21,  7, 11,  4,  9,\n",
      "         16, 14, 16, 12,  7, 15, 22, 13, 10, 13,  4,  2]]) torch.Size([1, 120])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ionChannel/models.py:761\u001b[0m, in \u001b[0;36mIonclfESMC.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# if len(inputs.size()) == 1:\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m#     inputs = inputs.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs, inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 761\u001b[0m representations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m x \u001b[38;5;241m=\u001b[39m representations\u001b[38;5;241m.\u001b[39membeddings  \u001b[38;5;66;03m# [:, 0]\u001b[39;00m\n\u001b[1;32m    766\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverse(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/esm/models/esm3.py:369\u001b[0m, in \u001b[0;36mESM3.forward\u001b[0;34m(self, sequence_tokens, structure_tokens, ss8_tokens, sasa_tokens, function_tokens, residue_annotation_tokens, average_plddt, per_res_plddt, structure_coords, chain_id, sequence_id)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m structure_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    358\u001b[0m structure_tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    359\u001b[0m     structure_tokens\u001b[38;5;241m.\u001b[39mmasked_fill(structure_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C\u001b[38;5;241m.\u001b[39mSTRUCTURE_MASK_TOKEN)\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;241m.\u001b[39mmasked_fill(sequence_tokens \u001b[38;5;241m==\u001b[39m C\u001b[38;5;241m.\u001b[39mSEQUENCE_BOS_TOKEN, C\u001b[38;5;241m.\u001b[39mSTRUCTURE_BOS_TOKEN)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m )\n\u001b[0;32m--> 369\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstructure_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage_plddt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_res_plddt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mss8_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msasa_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidue_annotation_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m x, embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x, sequence_id, affine, affine_mask, chain_id)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_heads(x, embedding)\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/esm/models/esm3.py:116\u001b[0m, in \u001b[0;36mEncodeInputs.forward\u001b[0;34m(self, sequence_tokens, structure_tokens, average_plddt, per_res_plddt, ss8_tokens, sasa_tokens, function_tokens, residue_annotation_tokens)\u001b[0m\n\u001b[1;32m    113\u001b[0m rbf_16_fn \u001b[38;5;241m=\u001b[39m partial(rbf, v_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, v_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# the `masked_fill(padding_mask.unsqueeze(2), 0)` for the two below is unnecessary\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# as pad tokens never even interact with the \"real\" tokens (due to sequence_id)\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m plddt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplddt_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrbf_16_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43maverage_plddt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m structure_per_res_plddt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructure_per_res_plddt_projection(\n\u001b[1;32m    118\u001b[0m     rbf_16_fn(per_res_plddt)\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Structure + \"structural features\" embeds\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Float and BFloat16"
     ]
    }
   ],
   "source": [
    "model.forward(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForwardTrackData(sequence=tensor([[[-38.0000, -38.0000, -38.0000,  12.6250,  21.6250,  22.2500,  22.0000,\n",
      "           21.7500,  21.3750,  21.6250,  21.6250,  21.2500,  20.7500,  21.3750,\n",
      "           21.8750,  20.8750,  20.7500,  20.2500,  20.3750,  20.1250,  21.7500,\n",
      "           19.8750,  19.7500,  19.2500,  18.2500,   1.2656,  -1.6719,  -3.9688,\n",
      "          -20.6250, -38.2500, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000,\n",
      "          -38.0000, -38.2500, -38.0000, -38.2500, -38.0000, -38.0000, -38.2500,\n",
      "          -38.0000, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000, -38.2500,\n",
      "          -38.0000, -38.2500, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000,\n",
      "          -38.0000, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000, -38.0000,\n",
      "          -38.0000],\n",
      "         [-40.2500, -40.0000, -40.2500,   5.3438,  20.1250,  19.8750,  18.7500,\n",
      "           20.6250,  18.7500,  18.3750,  18.6250,  18.5000,  18.2500,  18.0000,\n",
      "           18.7500,  17.7500,  17.3750,  17.1250,  17.7500,  16.8750,  22.1250,\n",
      "           17.1250,  16.5000,  16.7500,  11.5625,  -2.4375,  -3.6719,  -5.5312,\n",
      "          -25.3750, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500,\n",
      "          -40.2500, -40.2500, -40.2500, -40.2500, -40.0000, -40.0000, -40.0000,\n",
      "          -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000, -40.2500,\n",
      "          -40.2500, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000,\n",
      "          -40.2500, -40.0000, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500,\n",
      "          -40.2500],\n",
      "         [-36.2500, -36.2500, -36.2500,  13.1875,  27.2500,  28.1250,  26.8750,\n",
      "           27.1250,  26.7500,  26.7500,  26.8750,  26.7500,  26.3750,  26.2500,\n",
      "           26.6250,  26.2500,  25.7500,  25.3750,  25.8750,  25.1250,  25.7500,\n",
      "           25.3750,  24.8750,  25.0000,  17.6250,  -4.2188,  -4.1250,  -7.4375,\n",
      "          -19.1250, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500],\n",
      "         [-36.7500, -36.7500, -36.7500,  12.8125,  27.1250,  28.0000,  26.7500,\n",
      "           27.1250,  26.6250,  26.6250,  26.6250,  26.5000,  26.2500,  26.1250,\n",
      "           26.5000,  26.0000,  25.5000,  25.1250,  25.7500,  24.8750,  25.6250,\n",
      "           25.1250,  24.6250,  24.8750,  17.3750,  -4.1562,  -4.7500,  -7.7812,\n",
      "          -19.5000, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500],\n",
      "         [-37.2500, -37.2500, -37.2500,  13.3125,  27.0000,  28.1250,  26.8750,\n",
      "           27.0000,  26.7500,  26.5000,  26.7500,  26.6250,  26.1250,  26.0000,\n",
      "           26.5000,  26.0000,  25.7500,  25.2500,  25.6250,  25.0000,  25.5000,\n",
      "           25.3750,  24.6250,  25.0000,  17.3750,  -4.0938,  -3.7969,  -7.9062,\n",
      "          -20.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500],\n",
      "         [-33.5000, -33.5000, -33.5000,  25.5000,  23.7500,  24.7500,  23.8750,\n",
      "           23.7500,  23.7500,  23.2500,  23.8750,  23.3750,  22.7500,  22.8750,\n",
      "           23.3750,  22.7500,  22.5000,  22.0000,  22.3750,  21.5000,  21.8750,\n",
      "           22.0000,  22.0000,  22.0000,  17.7500,   4.2812,   4.6875,   1.8203,\n",
      "          -21.1250, -33.7500, -33.5000, -33.5000, -33.7500, -33.7500, -33.7500,\n",
      "          -33.5000, -33.7500, -33.5000, -33.5000, -33.5000, -33.5000, -33.7500,\n",
      "          -33.7500, -33.5000, -33.7500, -33.5000, -33.7500, -33.5000, -33.7500,\n",
      "          -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500, -33.7500,\n",
      "          -33.5000, -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500,\n",
      "          -33.7500],\n",
      "         [-33.0000, -33.0000, -33.0000,  24.7500,  22.7500,  24.3750,  23.1250,\n",
      "           22.8750,  23.1250,  22.5000,  23.2500,  22.7500,  21.7500,  22.2500,\n",
      "           23.1250,  22.1250,  22.0000,  21.5000,  21.3750,  20.7500,  21.1250,\n",
      "           21.2500,  20.7500,  21.1250,  19.8750,   5.5000,   5.0312,   2.6094,\n",
      "          -20.1250, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000]]], device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None) tensor([[[ 9.2204e-03, -8.5667e-03,  3.3698e-03,  ...,  1.7316e-02,\n",
      "           9.5319e-03, -6.1652e-03],\n",
      "         [ 4.5684e-03, -2.4373e-02,  2.6824e-02,  ...,  5.8312e-02,\n",
      "           2.7336e-02,  1.1116e-02],\n",
      "         [ 1.3113e-02, -4.5797e-02, -8.1466e-03,  ...,  4.5390e-02,\n",
      "           1.2297e-02, -7.8486e-03],\n",
      "         ...,\n",
      "         [ 8.5795e-03, -4.6057e-02, -1.4902e-02,  ...,  3.4243e-02,\n",
      "           5.7829e-03,  2.9621e-03],\n",
      "         [ 2.1430e-02, -3.4423e-02, -1.9981e-02,  ...,  1.6433e-02,\n",
      "           6.9777e-05, -4.5012e-03],\n",
      "         [-2.4575e-03, -2.1008e-02, -2.0242e-02,  ..., -4.0257e-03,\n",
      "          -1.3826e-03, -1.7940e-02]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "protein = ESMProtein(sequence=\"AAAAA\")\n",
    "client = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\") # or \"cpu\"\n",
    "protein_tensor = client.encode(protein)\n",
    "logits_output = client.logits(\n",
    "   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    ")\n",
    "print(logits_output.logits, logits_output.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMCOutput(sequence_logits=tensor([[[-36.2500, -36.2500, -36.2500,  ..., -36.2500, -36.2500, -36.2500],\n",
       "         [-36.5000, -36.5000, -36.5000,  ..., -36.5000, -36.5000, -36.5000],\n",
       "         [-32.7500, -32.7500, -32.7500,  ..., -32.7500, -32.7500, -32.7500],\n",
       "         ...,\n",
       "         [-34.7500, -34.5000, -34.5000,  ..., -34.5000, -34.5000, -34.5000],\n",
       "         [-30.0000, -30.0000, -30.0000,  ..., -30.0000, -30.0000, -29.8750],\n",
       "         [-31.0000, -31.0000, -31.0000,  ..., -31.0000, -31.0000, -31.0000]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>), embeddings=tensor([[[ 0.0026,  0.0060,  0.0051,  ...,  0.0038, -0.0041, -0.0036],\n",
       "         [-0.0266,  0.0315,  0.0239,  ..., -0.0066, -0.0077, -0.0089],\n",
       "         [-0.0270,  0.0322, -0.0048,  ..., -0.0430, -0.0479,  0.0398],\n",
       "         ...,\n",
       "         [-0.0444,  0.0344,  0.0262,  ...,  0.0153, -0.0640,  0.0262],\n",
       "         [-0.0535,  0.0649, -0.0220,  ..., -0.0165, -0.0659,  0.0044],\n",
       "         [-0.0203,  0.0305,  0.0304,  ..., -0.0135, -0.0464, -0.0276]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client(sequence_tokens=d[0][\"seq_t\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.utils.sampling import _BatchedESMProteinTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMCOutput(sequence_logits=tensor([[[-38.0000, -38.0000, -38.0000,  12.6250,  21.6250,  22.2500,  22.0000,\n",
       "           21.7500,  21.3750,  21.6250,  21.6250,  21.2500,  20.7500,  21.3750,\n",
       "           21.8750,  20.8750,  20.7500,  20.2500,  20.3750,  20.1250,  21.7500,\n",
       "           19.8750,  19.7500,  19.2500,  18.2500,   1.2656,  -1.6719,  -3.9688,\n",
       "          -20.6250, -38.2500, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000,\n",
       "          -38.0000, -38.2500, -38.0000, -38.2500, -38.0000, -38.0000, -38.2500,\n",
       "          -38.0000, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000, -38.2500,\n",
       "          -38.0000, -38.2500, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000,\n",
       "          -38.0000, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000, -38.0000,\n",
       "          -38.0000],\n",
       "         [-40.2500, -40.0000, -40.2500,   5.3438,  20.1250,  19.8750,  18.7500,\n",
       "           20.6250,  18.7500,  18.3750,  18.6250,  18.5000,  18.2500,  18.0000,\n",
       "           18.7500,  17.7500,  17.3750,  17.1250,  17.7500,  16.8750,  22.1250,\n",
       "           17.1250,  16.5000,  16.7500,  11.5625,  -2.4375,  -3.6719,  -5.5312,\n",
       "          -25.3750, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500,\n",
       "          -40.2500, -40.2500, -40.2500, -40.2500, -40.0000, -40.0000, -40.0000,\n",
       "          -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000, -40.2500,\n",
       "          -40.2500, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000,\n",
       "          -40.2500, -40.0000, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500,\n",
       "          -40.2500],\n",
       "         [-36.2500, -36.2500, -36.2500,  13.1875,  27.2500,  28.1250,  26.8750,\n",
       "           27.1250,  26.7500,  26.7500,  26.8750,  26.7500,  26.3750,  26.2500,\n",
       "           26.6250,  26.2500,  25.7500,  25.3750,  25.8750,  25.1250,  25.7500,\n",
       "           25.3750,  24.8750,  25.0000,  17.6250,  -4.2188,  -4.1250,  -7.4375,\n",
       "          -19.1250, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500],\n",
       "         [-36.7500, -36.7500, -36.7500,  12.8125,  27.1250,  28.0000,  26.7500,\n",
       "           27.1250,  26.6250,  26.6250,  26.6250,  26.5000,  26.2500,  26.1250,\n",
       "           26.5000,  26.0000,  25.5000,  25.1250,  25.7500,  24.8750,  25.6250,\n",
       "           25.1250,  24.6250,  24.8750,  17.3750,  -4.1562,  -4.7500,  -7.7812,\n",
       "          -19.5000, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500],\n",
       "         [-37.2500, -37.2500, -37.2500,  13.3125,  27.0000,  28.1250,  26.8750,\n",
       "           27.0000,  26.7500,  26.5000,  26.7500,  26.6250,  26.1250,  26.0000,\n",
       "           26.5000,  26.0000,  25.7500,  25.2500,  25.6250,  25.0000,  25.5000,\n",
       "           25.3750,  24.6250,  25.0000,  17.3750,  -4.0938,  -3.7969,  -7.9062,\n",
       "          -20.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500],\n",
       "         [-33.5000, -33.5000, -33.5000,  25.5000,  23.7500,  24.7500,  23.8750,\n",
       "           23.7500,  23.7500,  23.2500,  23.8750,  23.3750,  22.7500,  22.8750,\n",
       "           23.3750,  22.7500,  22.5000,  22.0000,  22.3750,  21.5000,  21.8750,\n",
       "           22.0000,  22.0000,  22.0000,  17.7500,   4.2812,   4.6875,   1.8203,\n",
       "          -21.1250, -33.7500, -33.5000, -33.5000, -33.7500, -33.7500, -33.7500,\n",
       "          -33.5000, -33.7500, -33.5000, -33.5000, -33.5000, -33.5000, -33.7500,\n",
       "          -33.7500, -33.5000, -33.7500, -33.5000, -33.7500, -33.5000, -33.7500,\n",
       "          -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500, -33.7500,\n",
       "          -33.5000, -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500,\n",
       "          -33.7500],\n",
       "         [-33.0000, -33.0000, -33.0000,  24.7500,  22.7500,  24.3750,  23.1250,\n",
       "           22.8750,  23.1250,  22.5000,  23.2500,  22.7500,  21.7500,  22.2500,\n",
       "           23.1250,  22.1250,  22.0000,  21.5000,  21.3750,  20.7500,  21.1250,\n",
       "           21.2500,  20.7500,  21.1250,  19.8750,   5.5000,   5.0312,   2.6094,\n",
       "          -20.1250, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<ViewBackward0>), embeddings=tensor([[[ 9.2163e-03, -8.5449e-03,  3.3722e-03,  ...,  1.7334e-02,\n",
       "           9.5215e-03, -6.1646e-03],\n",
       "         [ 4.5776e-03, -2.4414e-02,  2.6855e-02,  ...,  5.8350e-02,\n",
       "           2.7344e-02,  1.1108e-02],\n",
       "         [ 1.3123e-02, -4.5898e-02, -8.1177e-03,  ...,  4.5410e-02,\n",
       "           1.2268e-02, -7.8735e-03],\n",
       "         ...,\n",
       "         [ 8.6060e-03, -4.6143e-02, -1.4893e-02,  ...,  3.4180e-02,\n",
       "           5.7678e-03,  2.9602e-03],\n",
       "         [ 2.1484e-02, -3.4424e-02, -2.0020e-02,  ...,  1.6479e-02,\n",
       "           6.9618e-05, -4.4861e-03],\n",
       "         [-2.4567e-03, -2.0996e-02, -2.0264e-02,  ..., -4.0283e-03,\n",
       "          -1.3809e-03, -1.7944e-02]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.forward(sequence_tokens = protein_tensor.sequence.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextstrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
