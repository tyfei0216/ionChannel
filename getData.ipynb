{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import trainUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data2/tyfei/trainresults/ionChannels/ESMC/tryesmc/\" \n",
    "with open(os.path.join(path, \"config.json\"), \"r\") as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'save': 5,\n",
       "  'epoch': 40,\n",
       "  'batch_size': 1,\n",
       "  'accumulate_grad_batches': 8,\n",
       "  'seed': 1509},\n",
       " 'augmentation': {'step_points': [-1, 5000, 10000],\n",
       "  'maskp': [-1, -1, -1],\n",
       "  'maskpc': [-1, -1, -1],\n",
       "  'crop': [-1, 0.2, 0.4],\n",
       "  'tracks': {'seq_t': 1.0}},\n",
       " 'dataset': {'type': 'balancedesm3',\n",
       "  'dataset_train_sample': [[900, 2400, 1500, 600, 150, 300],\n",
       "   [1000, 500, 500, 500, 3000]],\n",
       "  'dataset_val_sample': [[90, 240, 150, 60, 15, 30], [100, 50, 50, 50, 300]],\n",
       "  'seed': 1509,\n",
       "  'tracks': ['seq_t'],\n",
       "  'train_test_ratio': [0.85, 0.15],\n",
       "  'pos': ['/data/tyfei/datasets/ion_channel/Interpro_v2/kingdom/Archaea1_label.pkl_v2_1125.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interpro_v2/kingdom/Bacteria1_label.pkl_v2_1125.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interpro_v2/kingdom/Eukaryota1_label.pkl_v2_1125.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interpro_v2/kingdom/Viruses1_label.pkl_v2_1125.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interpro_v2/kingdom/None1_label.pkl_v2_1125.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interprot/all_Known_Virus_Ion_pro.rename_0.99_TMHMM_1.pkl_label3.pkl'],\n",
       "  'neg': ['/data/tyfei/datasets/ion_channel/Interprot/Negative_sample/refseq_rmdup_mmseq_noTMHMM_label2.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interprot/Negative_sample/refseq_rmdup_mmseq_TMHMM_keywords2_reid_label2.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interprot/Negative_sample/refseq_rmdup_mmseq_TMHMM_keywords_reid_label2.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interprot/Negative_sample/virus_proteins_rmdup_clean_X0.1_TMHMM_mmseq_keywords_norefseq_reid_label2.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interprot/Negative_sample/interpro_neg_reid2_label2.pkl'],\n",
       "  'test': ['/data/tyfei/datasets/ion_channel/Interprot/Negative_sample/TEST_virus_proteins_rmdup_clean_X0.1_TMHMM_mmseq_remain_label2.pkl',\n",
       "   '/data/tyfei/datasets/ion_channel/Interprot/Negative_sample/TEST_human_virus_wu_1_label2.pkl'],\n",
       "  'required_labels': ['Voltage-gated',\n",
       "   'Ligand-gated',\n",
       "   'Mechanically-gated',\n",
       "   'Other gating',\n",
       "   'plasma membrane',\n",
       "   'endoplasmic reticulum',\n",
       "   'endosome',\n",
       "   'apical membrane',\n",
       "   'golgi',\n",
       "   'mitochondria',\n",
       "   'lysosome',\n",
       "   'K+',\n",
       "   'Ca2+',\n",
       "   'Na+',\n",
       "   'Mg2+',\n",
       "   'Cl-',\n",
       "   'H+',\n",
       "   'Zn2+',\n",
       "   'F-',\n",
       "   'Selectivity']},\n",
       " 'pretrain_model': {'model': 'esmc_600m',\n",
       "  'unfix_layers': [],\n",
       "  'add_lora': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35],\n",
       "  'rank': [4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   16,\n",
       "   16,\n",
       "   32],\n",
       "  'alpha': [8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   16,\n",
       "   16,\n",
       "   16,\n",
       "   32,\n",
       "   32,\n",
       "   64]},\n",
       " 'model': {'type': 'esmc',\n",
       "  'lambda_adapt': True,\n",
       "  'embed_dim': 1152,\n",
       "  'weight_decay': 0.001,\n",
       "  'strategy': {'warmup': 10000, 'check': 1000},\n",
       "  'lambda_step': 1.5,\n",
       "  'lambda_ini': 0.1,\n",
       "  'lambda_thres': [0.9, 0.95],\n",
       "  'max_lambda': 0.5,\n",
       "  'lr': 0.0001,\n",
       "  'lr_backbone': 1e-05,\n",
       "  'clf': 'linear',\n",
       "  'clf_params': {'take_embed': 'first', 'p0': 0.2},\n",
       "  'addition_clf': 'linear',\n",
       "  'addition_clf_params': {'take_embed': 'first', 'p0': 0.2, 'output_dim': 20},\n",
       "  'dis': 'linear',\n",
       "  'additional_label_weights': [0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1],\n",
       "  'weight_step': 1.2,\n",
       "  'weight_max': 15,\n",
       "  'dis_params': {'take_embed': 'first', 'p0': 0.2}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = trainUtils.loadPretrainModel(configs)\n",
    "\n",
    "model = trainUtils.buildModel(configs, pretrain_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = trainUtils.loadDataset(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train loader\n",
      "called new epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'seq_t': tensor([[ 0, 20,  5, 10, 18,  7,  5,  4,  7,  4,  4,  6,  4,  4,  8,  4,  8,  9,\n",
       "            4, 13,  5, 12, 16, 10, 10, 14, 15, 12, 16,  7, 19,  8, 10, 21, 14, 14,\n",
       "            9, 13,  6, 15, 14, 17, 19,  4, 17, 23, 19,  7,  8,  6, 18, 21, 14, 14,\n",
       "           16, 12,  9, 12, 13,  4,  4, 15, 17,  6,  9, 15, 12, 15,  8,  9, 16,  8,\n",
       "           13,  4,  8, 18,  8, 15, 13, 22,  8, 18, 19,  4,  4,  8, 21,  5,  9, 18,\n",
       "           11, 14, 17,  8, 15, 13, 16, 19,  8, 23, 10,  7, 15, 21,  7, 11,  4,  9,\n",
       "           16, 14, 16, 12,  7, 15, 22, 13, 10, 13,  4,  2]])},\n",
       " tensor([[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "          -1, -1, -1]]),\n",
       " {'seq_t': tensor([[ 0, 20, 20, 13, 12, 12, 11, 10, 11, 16, 10, 17,  4, 15,  4, 18,  4, 18,\n",
       "           22, 15, 15,  7, 15, 15,  5,  7, 21, 13,  4,  4, 12, 12, 11,  5, 11, 23,\n",
       "            7, 12,  7,  6,  6,  6, 11, 19, 20, 19, 11, 11, 19,  4, 16, 13, 21,  6,\n",
       "           23,  7, 12,  6, 11, 11, 23, 12,  8, 12, 11, 10,  2]])}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = ds.train_dataloader() \n",
    "for i, d in enumerate(dl):\n",
    "    break \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][\"seq_t\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20,  5, 10, 18,  7,  5,  4,  7,  4,  4,  6,  4,  4,  8,  4,  8,  9,\n",
      "          4, 13,  5, 12, 16, 10, 10, 14, 15, 12, 16,  7, 19,  8, 10, 21, 14, 14,\n",
      "          9, 13,  6, 15, 14, 17, 19,  4, 17, 23, 19,  7,  8,  6, 18, 21, 14, 14,\n",
      "         16, 12,  9, 12, 13,  4,  4, 15, 17,  6,  9, 15, 12, 15,  8,  9, 16,  8,\n",
      "         13,  4,  8, 18,  8, 15, 13, 22,  8, 18, 19,  4,  4,  8, 21,  5,  9, 18,\n",
      "         11, 14, 17,  8, 15, 13, 16, 19,  8, 23, 10,  7, 15, 21,  7, 11,  4,  9,\n",
      "         16, 14, 16, 12,  7, 15, 22, 13, 10, 13,  4,  2]]) torch.Size([1, 120])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ionChannel/models.py:761\u001b[0m, in \u001b[0;36mIonclfESMC.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# if len(inputs.size()) == 1:\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m#     inputs = inputs.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs, inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 761\u001b[0m representations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m x \u001b[38;5;241m=\u001b[39m representations\u001b[38;5;241m.\u001b[39membeddings  \u001b[38;5;66;03m# [:, 0]\u001b[39;00m\n\u001b[1;32m    766\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverse(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/esm/models/esm3.py:369\u001b[0m, in \u001b[0;36mESM3.forward\u001b[0;34m(self, sequence_tokens, structure_tokens, ss8_tokens, sasa_tokens, function_tokens, residue_annotation_tokens, average_plddt, per_res_plddt, structure_coords, chain_id, sequence_id)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m structure_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    358\u001b[0m structure_tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    359\u001b[0m     structure_tokens\u001b[38;5;241m.\u001b[39mmasked_fill(structure_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C\u001b[38;5;241m.\u001b[39mSTRUCTURE_MASK_TOKEN)\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;241m.\u001b[39mmasked_fill(sequence_tokens \u001b[38;5;241m==\u001b[39m C\u001b[38;5;241m.\u001b[39mSEQUENCE_BOS_TOKEN, C\u001b[38;5;241m.\u001b[39mSTRUCTURE_BOS_TOKEN)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m )\n\u001b[0;32m--> 369\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstructure_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage_plddt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_res_plddt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mss8_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msasa_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidue_annotation_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m x, embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x, sequence_id, affine, affine_mask, chain_id)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_heads(x, embedding)\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/esm/models/esm3.py:116\u001b[0m, in \u001b[0;36mEncodeInputs.forward\u001b[0;34m(self, sequence_tokens, structure_tokens, average_plddt, per_res_plddt, ss8_tokens, sasa_tokens, function_tokens, residue_annotation_tokens)\u001b[0m\n\u001b[1;32m    113\u001b[0m rbf_16_fn \u001b[38;5;241m=\u001b[39m partial(rbf, v_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, v_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# the `masked_fill(padding_mask.unsqueeze(2), 0)` for the two below is unnecessary\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# as pad tokens never even interact with the \"real\" tokens (due to sequence_id)\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m plddt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplddt_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrbf_16_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43maverage_plddt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m structure_per_res_plddt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructure_per_res_plddt_projection(\n\u001b[1;32m    118\u001b[0m     rbf_16_fn(per_res_plddt)\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Structure + \"structural features\" embeds\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Float and BFloat16"
     ]
    }
   ],
   "source": [
    "model.forward(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForwardTrackData(sequence=tensor([[[-38.0000, -38.0000, -38.0000,  12.6250,  21.6250,  22.2500,  22.0000,\n",
      "           21.7500,  21.3750,  21.6250,  21.6250,  21.2500,  20.7500,  21.3750,\n",
      "           21.8750,  20.8750,  20.7500,  20.2500,  20.3750,  20.1250,  21.7500,\n",
      "           19.8750,  19.7500,  19.2500,  18.2500,   1.2656,  -1.6719,  -3.9688,\n",
      "          -20.6250, -38.2500, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000,\n",
      "          -38.0000, -38.2500, -38.0000, -38.2500, -38.0000, -38.0000, -38.2500,\n",
      "          -38.0000, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000, -38.2500,\n",
      "          -38.0000, -38.2500, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000,\n",
      "          -38.0000, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000, -38.0000,\n",
      "          -38.0000],\n",
      "         [-40.2500, -40.0000, -40.2500,   5.3438,  20.1250,  19.8750,  18.7500,\n",
      "           20.6250,  18.7500,  18.3750,  18.6250,  18.5000,  18.2500,  18.0000,\n",
      "           18.7500,  17.7500,  17.3750,  17.1250,  17.7500,  16.8750,  22.1250,\n",
      "           17.1250,  16.5000,  16.7500,  11.5625,  -2.4375,  -3.6719,  -5.5312,\n",
      "          -25.3750, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500,\n",
      "          -40.2500, -40.2500, -40.2500, -40.2500, -40.0000, -40.0000, -40.0000,\n",
      "          -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000, -40.2500,\n",
      "          -40.2500, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000,\n",
      "          -40.2500, -40.0000, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500,\n",
      "          -40.2500],\n",
      "         [-36.2500, -36.2500, -36.2500,  13.1875,  27.2500,  28.1250,  26.8750,\n",
      "           27.1250,  26.7500,  26.7500,  26.8750,  26.7500,  26.3750,  26.2500,\n",
      "           26.6250,  26.2500,  25.7500,  25.3750,  25.8750,  25.1250,  25.7500,\n",
      "           25.3750,  24.8750,  25.0000,  17.6250,  -4.2188,  -4.1250,  -7.4375,\n",
      "          -19.1250, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
      "          -36.2500],\n",
      "         [-36.7500, -36.7500, -36.7500,  12.8125,  27.1250,  28.0000,  26.7500,\n",
      "           27.1250,  26.6250,  26.6250,  26.6250,  26.5000,  26.2500,  26.1250,\n",
      "           26.5000,  26.0000,  25.5000,  25.1250,  25.7500,  24.8750,  25.6250,\n",
      "           25.1250,  24.6250,  24.8750,  17.3750,  -4.1562,  -4.7500,  -7.7812,\n",
      "          -19.5000, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
      "          -36.7500],\n",
      "         [-37.2500, -37.2500, -37.2500,  13.3125,  27.0000,  28.1250,  26.8750,\n",
      "           27.0000,  26.7500,  26.5000,  26.7500,  26.6250,  26.1250,  26.0000,\n",
      "           26.5000,  26.0000,  25.7500,  25.2500,  25.6250,  25.0000,  25.5000,\n",
      "           25.3750,  24.6250,  25.0000,  17.3750,  -4.0938,  -3.7969,  -7.9062,\n",
      "          -20.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
      "          -37.2500],\n",
      "         [-33.5000, -33.5000, -33.5000,  25.5000,  23.7500,  24.7500,  23.8750,\n",
      "           23.7500,  23.7500,  23.2500,  23.8750,  23.3750,  22.7500,  22.8750,\n",
      "           23.3750,  22.7500,  22.5000,  22.0000,  22.3750,  21.5000,  21.8750,\n",
      "           22.0000,  22.0000,  22.0000,  17.7500,   4.2812,   4.6875,   1.8203,\n",
      "          -21.1250, -33.7500, -33.5000, -33.5000, -33.7500, -33.7500, -33.7500,\n",
      "          -33.5000, -33.7500, -33.5000, -33.5000, -33.5000, -33.5000, -33.7500,\n",
      "          -33.7500, -33.5000, -33.7500, -33.5000, -33.7500, -33.5000, -33.7500,\n",
      "          -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500, -33.7500,\n",
      "          -33.5000, -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500,\n",
      "          -33.7500],\n",
      "         [-33.0000, -33.0000, -33.0000,  24.7500,  22.7500,  24.3750,  23.1250,\n",
      "           22.8750,  23.1250,  22.5000,  23.2500,  22.7500,  21.7500,  22.2500,\n",
      "           23.1250,  22.1250,  22.0000,  21.5000,  21.3750,  20.7500,  21.1250,\n",
      "           21.2500,  20.7500,  21.1250,  19.8750,   5.5000,   5.0312,   2.6094,\n",
      "          -20.1250, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
      "          -33.0000]]], device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None) tensor([[[ 9.2204e-03, -8.5667e-03,  3.3698e-03,  ...,  1.7316e-02,\n",
      "           9.5319e-03, -6.1652e-03],\n",
      "         [ 4.5684e-03, -2.4373e-02,  2.6824e-02,  ...,  5.8312e-02,\n",
      "           2.7336e-02,  1.1116e-02],\n",
      "         [ 1.3113e-02, -4.5797e-02, -8.1466e-03,  ...,  4.5390e-02,\n",
      "           1.2297e-02, -7.8486e-03],\n",
      "         ...,\n",
      "         [ 8.5795e-03, -4.6057e-02, -1.4902e-02,  ...,  3.4243e-02,\n",
      "           5.7829e-03,  2.9621e-03],\n",
      "         [ 2.1430e-02, -3.4423e-02, -1.9981e-02,  ...,  1.6433e-02,\n",
      "           6.9777e-05, -4.5012e-03],\n",
      "         [-2.4575e-03, -2.1008e-02, -2.0242e-02,  ..., -4.0257e-03,\n",
      "          -1.3826e-03, -1.7940e-02]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "protein = ESMProtein(sequence=\"AAAAA\")\n",
    "client = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\") # or \"cpu\"\n",
    "protein_tensor = client.encode(protein)\n",
    "logits_output = client.logits(\n",
    "   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    ")\n",
    "print(logits_output.logits, logits_output.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMCOutput(sequence_logits=tensor([[[-36.2500, -36.2500, -36.2500,  ..., -36.2500, -36.2500, -36.2500],\n",
       "         [-36.5000, -36.5000, -36.5000,  ..., -36.5000, -36.5000, -36.5000],\n",
       "         [-32.7500, -32.7500, -32.7500,  ..., -32.7500, -32.7500, -32.7500],\n",
       "         ...,\n",
       "         [-34.7500, -34.5000, -34.5000,  ..., -34.5000, -34.5000, -34.5000],\n",
       "         [-30.0000, -30.0000, -30.0000,  ..., -30.0000, -30.0000, -29.8750],\n",
       "         [-31.0000, -31.0000, -31.0000,  ..., -31.0000, -31.0000, -31.0000]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>), embeddings=tensor([[[ 0.0026,  0.0060,  0.0051,  ...,  0.0038, -0.0041, -0.0036],\n",
       "         [-0.0266,  0.0315,  0.0239,  ..., -0.0066, -0.0077, -0.0089],\n",
       "         [-0.0270,  0.0322, -0.0048,  ..., -0.0430, -0.0479,  0.0398],\n",
       "         ...,\n",
       "         [-0.0444,  0.0344,  0.0262,  ...,  0.0153, -0.0640,  0.0262],\n",
       "         [-0.0535,  0.0649, -0.0220,  ..., -0.0165, -0.0659,  0.0044],\n",
       "         [-0.0203,  0.0305,  0.0304,  ..., -0.0135, -0.0464, -0.0276]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client(sequence_tokens=d[0][\"seq_t\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.utils.sampling import _BatchedESMProteinTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMCOutput(sequence_logits=tensor([[[-38.0000, -38.0000, -38.0000,  12.6250,  21.6250,  22.2500,  22.0000,\n",
       "           21.7500,  21.3750,  21.6250,  21.6250,  21.2500,  20.7500,  21.3750,\n",
       "           21.8750,  20.8750,  20.7500,  20.2500,  20.3750,  20.1250,  21.7500,\n",
       "           19.8750,  19.7500,  19.2500,  18.2500,   1.2656,  -1.6719,  -3.9688,\n",
       "          -20.6250, -38.2500, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000,\n",
       "          -38.0000, -38.2500, -38.0000, -38.2500, -38.0000, -38.0000, -38.2500,\n",
       "          -38.0000, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000, -38.2500,\n",
       "          -38.0000, -38.2500, -38.0000, -38.0000, -38.2500, -38.2500, -38.0000,\n",
       "          -38.0000, -38.0000, -38.0000, -38.0000, -38.2500, -38.0000, -38.0000,\n",
       "          -38.0000],\n",
       "         [-40.2500, -40.0000, -40.2500,   5.3438,  20.1250,  19.8750,  18.7500,\n",
       "           20.6250,  18.7500,  18.3750,  18.6250,  18.5000,  18.2500,  18.0000,\n",
       "           18.7500,  17.7500,  17.3750,  17.1250,  17.7500,  16.8750,  22.1250,\n",
       "           17.1250,  16.5000,  16.7500,  11.5625,  -2.4375,  -3.6719,  -5.5312,\n",
       "          -25.3750, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500, -40.2500,\n",
       "          -40.2500, -40.2500, -40.2500, -40.2500, -40.0000, -40.0000, -40.0000,\n",
       "          -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000, -40.2500,\n",
       "          -40.2500, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500, -40.0000,\n",
       "          -40.2500, -40.0000, -40.2500, -40.0000, -40.2500, -40.2500, -40.2500,\n",
       "          -40.2500],\n",
       "         [-36.2500, -36.2500, -36.2500,  13.1875,  27.2500,  28.1250,  26.8750,\n",
       "           27.1250,  26.7500,  26.7500,  26.8750,  26.7500,  26.3750,  26.2500,\n",
       "           26.6250,  26.2500,  25.7500,  25.3750,  25.8750,  25.1250,  25.7500,\n",
       "           25.3750,  24.8750,  25.0000,  17.6250,  -4.2188,  -4.1250,  -7.4375,\n",
       "          -19.1250, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500, -36.2500,\n",
       "          -36.2500],\n",
       "         [-36.7500, -36.7500, -36.7500,  12.8125,  27.1250,  28.0000,  26.7500,\n",
       "           27.1250,  26.6250,  26.6250,  26.6250,  26.5000,  26.2500,  26.1250,\n",
       "           26.5000,  26.0000,  25.5000,  25.1250,  25.7500,  24.8750,  25.6250,\n",
       "           25.1250,  24.6250,  24.8750,  17.3750,  -4.1562,  -4.7500,  -7.7812,\n",
       "          -19.5000, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500, -36.7500,\n",
       "          -36.7500],\n",
       "         [-37.2500, -37.2500, -37.2500,  13.3125,  27.0000,  28.1250,  26.8750,\n",
       "           27.0000,  26.7500,  26.5000,  26.7500,  26.6250,  26.1250,  26.0000,\n",
       "           26.5000,  26.0000,  25.7500,  25.2500,  25.6250,  25.0000,  25.5000,\n",
       "           25.3750,  24.6250,  25.0000,  17.3750,  -4.0938,  -3.7969,  -7.9062,\n",
       "          -20.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500, -37.2500,\n",
       "          -37.2500],\n",
       "         [-33.5000, -33.5000, -33.5000,  25.5000,  23.7500,  24.7500,  23.8750,\n",
       "           23.7500,  23.7500,  23.2500,  23.8750,  23.3750,  22.7500,  22.8750,\n",
       "           23.3750,  22.7500,  22.5000,  22.0000,  22.3750,  21.5000,  21.8750,\n",
       "           22.0000,  22.0000,  22.0000,  17.7500,   4.2812,   4.6875,   1.8203,\n",
       "          -21.1250, -33.7500, -33.5000, -33.5000, -33.7500, -33.7500, -33.7500,\n",
       "          -33.5000, -33.7500, -33.5000, -33.5000, -33.5000, -33.5000, -33.7500,\n",
       "          -33.7500, -33.5000, -33.7500, -33.5000, -33.7500, -33.5000, -33.7500,\n",
       "          -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500, -33.7500,\n",
       "          -33.5000, -33.7500, -33.7500, -33.5000, -33.7500, -33.7500, -33.7500,\n",
       "          -33.7500],\n",
       "         [-33.0000, -33.0000, -33.0000,  24.7500,  22.7500,  24.3750,  23.1250,\n",
       "           22.8750,  23.1250,  22.5000,  23.2500,  22.7500,  21.7500,  22.2500,\n",
       "           23.1250,  22.1250,  22.0000,  21.5000,  21.3750,  20.7500,  21.1250,\n",
       "           21.2500,  20.7500,  21.1250,  19.8750,   5.5000,   5.0312,   2.6094,\n",
       "          -20.1250, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000, -33.0000,\n",
       "          -33.0000]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<ViewBackward0>), embeddings=tensor([[[ 9.2163e-03, -8.5449e-03,  3.3722e-03,  ...,  1.7334e-02,\n",
       "           9.5215e-03, -6.1646e-03],\n",
       "         [ 4.5776e-03, -2.4414e-02,  2.6855e-02,  ...,  5.8350e-02,\n",
       "           2.7344e-02,  1.1108e-02],\n",
       "         [ 1.3123e-02, -4.5898e-02, -8.1177e-03,  ...,  4.5410e-02,\n",
       "           1.2268e-02, -7.8735e-03],\n",
       "         ...,\n",
       "         [ 8.6060e-03, -4.6143e-02, -1.4893e-02,  ...,  3.4180e-02,\n",
       "           5.7678e-03,  2.9602e-03],\n",
       "         [ 2.1484e-02, -3.4424e-02, -2.0020e-02,  ...,  1.6479e-02,\n",
       "           6.9618e-05, -4.4861e-03],\n",
       "         [-2.4567e-03, -2.0996e-02, -2.0264e-02,  ..., -4.0283e-03,\n",
       "          -1.3809e-03, -1.7944e-02]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.forward(sequence_tokens = protein_tensor.sequence.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
