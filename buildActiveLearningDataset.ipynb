{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test train mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import trainUtils\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation: 0.8660254037844386\n",
      "P-value: 0.05766888562243733\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Example data\n",
    "x = [10, 20, 30, 40, 50]\n",
    "y = [5, 5, 15, 15, 15]\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "corr, p_value = spearmanr(x, y)\n",
    "print(f\"Spearman Correlation: {corr}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f20996848b0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([3, 7, 15], [0.36449424,0.341414802,0.211435704,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interpro_v2/kingdom/Eukaryota1_label.pkl_v2_1125.pkl\", \"rb\") as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   7.,   26.,   31.,   66.,   64.,   93.,  126.,  124.,  179.,\n",
       "         182.,  216.,  230.,  224.,  241.,  211.,  286.,  273.,  219.,\n",
       "         229.,  245.,  279.,  234.,  304.,  287.,  216.,  296.,  238.,\n",
       "         261.,  260.,  260.,  273.,  258.,  285.,  293.,  231.,  294.,\n",
       "         265.,  300.,  310.,  258.,  295.,  234.,  277.,  272.,  302.,\n",
       "         294.,  260.,  280.,  272.,  303.,  353.,  247.,  265.,  274.,\n",
       "         320.,  326.,  288.,  294.,  237.,  264.,  213.,  271.,  226.,\n",
       "         187.,  224.,  199.,  221.,  212.,  183.,  183.,  173.,  170.,\n",
       "         197.,  149.,  200.,  143.,  157.,  166.,  154.,  160.,  150.,\n",
       "         137.,  120.,  129.,  114.,  120.,  118.,   84.,  117.,  110.,\n",
       "         107.,  115.,   89.,  121.,  122.,  101.,  103.,   78.,   96.,\n",
       "        5146.]),\n",
       " array([ 41.  ,  48.59,  56.18,  63.77,  71.36,  78.95,  86.54,  94.13,\n",
       "        101.72, 109.31, 116.9 , 124.49, 132.08, 139.67, 147.26, 154.85,\n",
       "        162.44, 170.03, 177.62, 185.21, 192.8 , 200.39, 207.98, 215.57,\n",
       "        223.16, 230.75, 238.34, 245.93, 253.52, 261.11, 268.7 , 276.29,\n",
       "        283.88, 291.47, 299.06, 306.65, 314.24, 321.83, 329.42, 337.01,\n",
       "        344.6 , 352.19, 359.78, 367.37, 374.96, 382.55, 390.14, 397.73,\n",
       "        405.32, 412.91, 420.5 , 428.09, 435.68, 443.27, 450.86, 458.45,\n",
       "        466.04, 473.63, 481.22, 488.81, 496.4 , 503.99, 511.58, 519.17,\n",
       "        526.76, 534.35, 541.94, 549.53, 557.12, 564.71, 572.3 , 579.89,\n",
       "        587.48, 595.07, 602.66, 610.25, 617.84, 625.43, 633.02, 640.61,\n",
       "        648.2 , 655.79, 663.38, 670.97, 678.56, 686.15, 693.74, 701.33,\n",
       "        708.92, 716.51, 724.1 , 731.69, 739.28, 746.87, 754.46, 762.05,\n",
       "        769.64, 777.23, 784.82, 792.41, 800.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = [] \n",
    "for i in d:\n",
    "    lens.append(len(i[\"ori_seq\"]))\n",
    "lens = [800 if i>800 else i for i in lens]\n",
    "plt.hist(lens, bins=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/data2/tyfei/trainresults/ionChannels/ESMC/final3/\"\n",
    "path = \"/data2/tyfei/trainresults/ionChannels/ESMCFinal/logit3/\"\n",
    "with open(os.path.join(path, \"config.json\"), \"r\") as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from  /data/tyfei/datasets/ion_channel/Interprot/test885.pkl  with size  885\n",
      "load data from  /data/tyfei/datasets/ion_channel/Interprot/Negative_sample/TEST_virus_proteins_rmdup_clean_X0.1_TMHMM_mmseq_remain_splitpro_TMHMM0213.pkl  with size  133108\n",
      "load data from  /data/tyfei/datasets/ion_channel/Interprot/Negative_sample/TEST_human_virus_wu_splitpro_TMHMM_plus0213.pkl  with size  9932\n",
      "using list size 50\n",
      "using list size 50\n",
      "get val loader\n"
     ]
    }
   ],
   "source": [
    "ds = trainUtils.loadDataset(configs)\n",
    "dl = ds.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[164,\n",
       "  266,\n",
       "  248,\n",
       "  251,\n",
       "  265,\n",
       "  250,\n",
       "  229,\n",
       "  323,\n",
       "  359,\n",
       "  331,\n",
       "  343,\n",
       "  497,\n",
       "  366,\n",
       "  435,\n",
       "  67,\n",
       "  235,\n",
       "  120,\n",
       "  201,\n",
       "  358,\n",
       "  360,\n",
       "  718,\n",
       "  103,\n",
       "  876,\n",
       "  297,\n",
       "  135,\n",
       "  307,\n",
       "  315,\n",
       "  101,\n",
       "  255,\n",
       "  209,\n",
       "  198,\n",
       "  68,\n",
       "  58,\n",
       "  80,\n",
       "  72,\n",
       "  204,\n",
       "  315,\n",
       "  299,\n",
       "  317,\n",
       "  198,\n",
       "  233,\n",
       "  237,\n",
       "  123,\n",
       "  276,\n",
       "  205,\n",
       "  211,\n",
       "  404,\n",
       "  306,\n",
       "  96,\n",
       "  66,\n",
       "  461,\n",
       "  54,\n",
       "  398,\n",
       "  381,\n",
       "  188,\n",
       "  78,\n",
       "  121,\n",
       "  289,\n",
       "  877,\n",
       "  104,\n",
       "  718,\n",
       "  357,\n",
       "  355,\n",
       "  205,\n",
       "  180,\n",
       "  125,\n",
       "  213,\n",
       "  323,\n",
       "  66,\n",
       "  171,\n",
       "  367,\n",
       "  77,\n",
       "  177,\n",
       "  167,\n",
       "  216,\n",
       "  297,\n",
       "  175,\n",
       "  400,\n",
       "  340,\n",
       "  191,\n",
       "  212,\n",
       "  220,\n",
       "  189,\n",
       "  160,\n",
       "  298,\n",
       "  182,\n",
       "  221,\n",
       "  248,\n",
       "  250,\n",
       "  281,\n",
       "  229,\n",
       "  245,\n",
       "  240,\n",
       "  265,\n",
       "  266,\n",
       "  259,\n",
       "  250,\n",
       "  229,\n",
       "  327,\n",
       "  339,\n",
       "  330,\n",
       "  341,\n",
       "  477,\n",
       "  455,\n",
       "  260,\n",
       "  90,\n",
       "  47,\n",
       "  170,\n",
       "  234,\n",
       "  414,\n",
       "  293,\n",
       "  218,\n",
       "  59,\n",
       "  280,\n",
       "  324,\n",
       "  222,\n",
       "  233,\n",
       "  254,\n",
       "  272,\n",
       "  327,\n",
       "  102,\n",
       "  230,\n",
       "  368,\n",
       "  338,\n",
       "  412,\n",
       "  488,\n",
       "  221,\n",
       "  78,\n",
       "  125,\n",
       "  398,\n",
       "  907,\n",
       "  135,\n",
       "  70,\n",
       "  742,\n",
       "  431,\n",
       "  371,\n",
       "  345,\n",
       "  201,\n",
       "  180,\n",
       "  150,\n",
       "  270,\n",
       "  316,\n",
       "  75,\n",
       "  176,\n",
       "  305,\n",
       "  338,\n",
       "  191,\n",
       "  138,\n",
       "  169,\n",
       "  240,\n",
       "  308,\n",
       "  257,\n",
       "  80,\n",
       "  80,\n",
       "  62,\n",
       "  199,\n",
       "  186,\n",
       "  182,\n",
       "  225,\n",
       "  227,\n",
       "  247,\n",
       "  185,\n",
       "  215,\n",
       "  281,\n",
       "  261,\n",
       "  310,\n",
       "  262,\n",
       "  309,\n",
       "  293,\n",
       "  274,\n",
       "  240,\n",
       "  254,\n",
       "  243,\n",
       "  364,\n",
       "  354,\n",
       "  462,\n",
       "  349,\n",
       "  497,\n",
       "  378,\n",
       "  336,\n",
       "  357,\n",
       "  102,\n",
       "  907,\n",
       "  223,\n",
       "  405,\n",
       "  234,\n",
       "  420,\n",
       "  706,\n",
       "  248,\n",
       "  312,\n",
       "  60,\n",
       "  386,\n",
       "  279,\n",
       "  550,\n",
       "  845,\n",
       "  322,\n",
       "  256,\n",
       "  730,\n",
       "  290,\n",
       "  102,\n",
       "  400,\n",
       "  228,\n",
       "  110,\n",
       "  357,\n",
       "  271,\n",
       "  271,\n",
       "  342,\n",
       "  489,\n",
       "  347,\n",
       "  293,\n",
       "  422,\n",
       "  433,\n",
       "  202,\n",
       "  236,\n",
       "  87,\n",
       "  295,\n",
       "  276,\n",
       "  830,\n",
       "  84,\n",
       "  54,\n",
       "  694,\n",
       "  301,\n",
       "  344,\n",
       "  290,\n",
       "  153,\n",
       "  318,\n",
       "  70,\n",
       "  189,\n",
       "  205,\n",
       "  143,\n",
       "  434,\n",
       "  500,\n",
       "  299,\n",
       "  88,\n",
       "  84,\n",
       "  156,\n",
       "  500,\n",
       "  202,\n",
       "  299,\n",
       "  333,\n",
       "  295,\n",
       "  391,\n",
       "  430,\n",
       "  171,\n",
       "  82,\n",
       "  56,\n",
       "  293,\n",
       "  258,\n",
       "  822,\n",
       "  379,\n",
       "  86,\n",
       "  690,\n",
       "  294,\n",
       "  346,\n",
       "  280,\n",
       "  153,\n",
       "  473,\n",
       "  222,\n",
       "  838,\n",
       "  904,\n",
       "  275,\n",
       "  417,\n",
       "  511,\n",
       "  91,\n",
       "  338,\n",
       "  234,\n",
       "  238,\n",
       "  92,\n",
       "  394,\n",
       "  390,\n",
       "  550,\n",
       "  159,\n",
       "  90,\n",
       "  455,\n",
       "  225,\n",
       "  857,\n",
       "  893,\n",
       "  269,\n",
       "  386,\n",
       "  462,\n",
       "  703,\n",
       "  78,\n",
       "  337,\n",
       "  226,\n",
       "  657,\n",
       "  117,\n",
       "  394,\n",
       "  386,\n",
       "  539,\n",
       "  114,\n",
       "  91,\n",
       "  129,\n",
       "  108,\n",
       "  340,\n",
       "  87,\n",
       "  560,\n",
       "  406,\n",
       "  269,\n",
       "  931,\n",
       "  841,\n",
       "  222,\n",
       "  435,\n",
       "  102,\n",
       "  354,\n",
       "  623,\n",
       "  207,\n",
       "  312,\n",
       "  98,\n",
       "  913,\n",
       "  262,\n",
       "  373,\n",
       "  479,\n",
       "  686,\n",
       "  161,\n",
       "  393,\n",
       "  498,\n",
       "  400,\n",
       "  366,\n",
       "  577,\n",
       "  98,\n",
       "  159,\n",
       "  137,\n",
       "  55,\n",
       "  322,\n",
       "  212,\n",
       "  74,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  100,\n",
       "  72,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  207,\n",
       "  184,\n",
       "  168,\n",
       "  214,\n",
       "  277,\n",
       "  173,\n",
       "  202,\n",
       "  313,\n",
       "  317,\n",
       "  107,\n",
       "  515,\n",
       "  69,\n",
       "  86,\n",
       "  111,\n",
       "  244,\n",
       "  89,\n",
       "  129,\n",
       "  167,\n",
       "  194,\n",
       "  340,\n",
       "  96,\n",
       "  78,\n",
       "  90,\n",
       "  53,\n",
       "  359,\n",
       "  201,\n",
       "  164,\n",
       "  140,\n",
       "  162,\n",
       "  199,\n",
       "  140,\n",
       "  224,\n",
       "  237,\n",
       "  139,\n",
       "  349,\n",
       "  212,\n",
       "  81,\n",
       "  72,\n",
       "  79,\n",
       "  111,\n",
       "  340,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  139,\n",
       "  70,\n",
       "  377,\n",
       "  203,\n",
       "  184,\n",
       "  168,\n",
       "  218,\n",
       "  277,\n",
       "  195,\n",
       "  319,\n",
       "  317,\n",
       "  139,\n",
       "  323,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  88,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  121,\n",
       "  70,\n",
       "  90,\n",
       "  377,\n",
       "  202,\n",
       "  187,\n",
       "  168,\n",
       "  224,\n",
       "  277,\n",
       "  160,\n",
       "  194,\n",
       "  297,\n",
       "  317,\n",
       "  212,\n",
       "  73,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  110,\n",
       "  66,\n",
       "  377,\n",
       "  202,\n",
       "  185,\n",
       "  168,\n",
       "  277,\n",
       "  211,\n",
       "  281,\n",
       "  317,\n",
       "  670,\n",
       "  82,\n",
       "  856,\n",
       "  860,\n",
       "  264,\n",
       "  268,\n",
       "  91,\n",
       "  110,\n",
       "  142,\n",
       "  139,\n",
       "  131,\n",
       "  166,\n",
       "  181,\n",
       "  187,\n",
       "  91,\n",
       "  134,\n",
       "  80,\n",
       "  61,\n",
       "  159,\n",
       "  101,\n",
       "  91,\n",
       "  130,\n",
       "  188,\n",
       "  157,\n",
       "  415,\n",
       "  261,\n",
       "  91,\n",
       "  127,\n",
       "  129,\n",
       "  196,\n",
       "  167,\n",
       "  409,\n",
       "  262,\n",
       "  91,\n",
       "  129,\n",
       "  209,\n",
       "  176,\n",
       "  204,\n",
       "  204,\n",
       "  295,\n",
       "  91,\n",
       "  143,\n",
       "  173,\n",
       "  276,\n",
       "  90,\n",
       "  107,\n",
       "  172,\n",
       "  269,\n",
       "  90,\n",
       "  111,\n",
       "  280,\n",
       "  90,\n",
       "  110,\n",
       "  172,\n",
       "  270,\n",
       "  90,\n",
       "  111,\n",
       "  378,\n",
       "  130,\n",
       "  86,\n",
       "  856,\n",
       "  91,\n",
       "  451,\n",
       "  445,\n",
       "  147,\n",
       "  213,\n",
       "  88,\n",
       "  579,\n",
       "  72,\n",
       "  82,\n",
       "  108,\n",
       "  342,\n",
       "  243,\n",
       "  93,\n",
       "  146,\n",
       "  191,\n",
       "  298,\n",
       "  128,\n",
       "  68,\n",
       "  94,\n",
       "  53,\n",
       "  364,\n",
       "  179,\n",
       "  183,\n",
       "  159,\n",
       "  406,\n",
       "  452,\n",
       "  182,\n",
       "  478,\n",
       "  532,\n",
       "  620,\n",
       "  142,\n",
       "  317,\n",
       "  212,\n",
       "  35,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  92,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  100,\n",
       "  70,\n",
       "  53,\n",
       "  377,\n",
       "  204,\n",
       "  181,\n",
       "  168,\n",
       "  212,\n",
       "  277,\n",
       "  197,\n",
       "  313,\n",
       "  317,\n",
       "  688,\n",
       "  320,\n",
       "  665,\n",
       "  644,\n",
       "  676,\n",
       "  70,\n",
       "  78,\n",
       "  110,\n",
       "  244,\n",
       "  90,\n",
       "  128,\n",
       "  321,\n",
       "  342,\n",
       "  93,\n",
       "  92,\n",
       "  91,\n",
       "  53,\n",
       "  358,\n",
       "  196,\n",
       "  140,\n",
       "  164,\n",
       "  200,\n",
       "  194,\n",
       "  660,\n",
       "  486,\n",
       "  486,\n",
       "  229,\n",
       "  60,\n",
       "  70,\n",
       "  78,\n",
       "  110,\n",
       "  244,\n",
       "  90,\n",
       "  127,\n",
       "  191,\n",
       "  354,\n",
       "  94,\n",
       "  92,\n",
       "  92,\n",
       "  53,\n",
       "  358,\n",
       "  199,\n",
       "  140,\n",
       "  159,\n",
       "  165,\n",
       "  214,\n",
       "  866,\n",
       "  156,\n",
       "  351,\n",
       "  82,\n",
       "  214,\n",
       "  75,\n",
       "  79,\n",
       "  111,\n",
       "  247,\n",
       "  91,\n",
       "  128,\n",
       "  190,\n",
       "  323,\n",
       "  79,\n",
       "  68,\n",
       "  93,\n",
       "  53,\n",
       "  381,\n",
       "  191,\n",
       "  139,\n",
       "  186,\n",
       "  170,\n",
       "  223,\n",
       "  272,\n",
       "  267,\n",
       "  323,\n",
       "  140,\n",
       "  322,\n",
       "  212,\n",
       "  341,\n",
       "  35,\n",
       "  250,\n",
       "  87,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  108,\n",
       "  70,\n",
       "  377,\n",
       "  203,\n",
       "  185,\n",
       "  168,\n",
       "  221,\n",
       "  277,\n",
       "  159,\n",
       "  194,\n",
       "  314,\n",
       "  317,\n",
       "  140,\n",
       "  322,\n",
       "  212,\n",
       "  79,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  325,\n",
       "  304,\n",
       "  95,\n",
       "  68,\n",
       "  90,\n",
       "  377,\n",
       "  184,\n",
       "  168,\n",
       "  216,\n",
       "  277,\n",
       "  61,\n",
       "  195,\n",
       "  313,\n",
       "  317,\n",
       "  91,\n",
       "  72,\n",
       "  47,\n",
       "  47,\n",
       "  47,\n",
       "  46,\n",
       "  48,\n",
       "  46,\n",
       "  47,\n",
       "  375,\n",
       "  92,\n",
       "  109,\n",
       "  43,\n",
       "  332,\n",
       "  131,\n",
       "  332,\n",
       "  248,\n",
       "  299,\n",
       "  202,\n",
       "  351,\n",
       "  352,\n",
       "  401,\n",
       "  222,\n",
       "  139,\n",
       "  530,\n",
       "  598,\n",
       "  81,\n",
       "  63,\n",
       "  583,\n",
       "  111,\n",
       "  92,\n",
       "  530,\n",
       "  180,\n",
       "  815,\n",
       "  834,\n",
       "  171,\n",
       "  214,\n",
       "  214,\n",
       "  214,\n",
       "  214,\n",
       "  214,\n",
       "  214,\n",
       "  214,\n",
       "  214,\n",
       "  63,\n",
       "  54,\n",
       "  119,\n",
       "  48,\n",
       "  213,\n",
       "  75,\n",
       "  60,\n",
       "  95,\n",
       "  61,\n",
       "  76,\n",
       "  75,\n",
       "  127,\n",
       "  130,\n",
       "  75,\n",
       "  126,\n",
       "  131,\n",
       "  126,\n",
       "  131,\n",
       "  75,\n",
       "  75,\n",
       "  75,\n",
       "  75,\n",
       "  127,\n",
       "  130,\n",
       "  75,\n",
       "  227,\n",
       "  68,\n",
       "  70,\n",
       "  91,\n",
       "  60,\n",
       "  107,\n",
       "  150,\n",
       "  213,\n",
       "  60,\n",
       "  55,\n",
       "  56,\n",
       "  55,\n",
       "  546,\n",
       "  604,\n",
       "  573,\n",
       "  525,\n",
       "  533,\n",
       "  524,\n",
       "  522,\n",
       "  524,\n",
       "  488,\n",
       "  484,\n",
       "  498,\n",
       "  485,\n",
       "  485,\n",
       "  491,\n",
       "  454,\n",
       "  496,\n",
       "  496,\n",
       "  539,\n",
       "  183,\n",
       "  236,\n",
       "  546,\n",
       "  609,\n",
       "  550,\n",
       "  617,\n",
       "  503,\n",
       "  503,\n",
       "  676,\n",
       "  677,\n",
       "  676,\n",
       "  676,\n",
       "  676,\n",
       "  676,\n",
       "  429,\n",
       "  429,\n",
       "  433,\n",
       "  428,\n",
       "  681,\n",
       "  64,\n",
       "  298,\n",
       "  574,\n",
       "  298,\n",
       "  574,\n",
       "  65,\n",
       "  299,\n",
       "  574,\n",
       "  543,\n",
       "  574,\n",
       "  551,\n",
       "  571,\n",
       "  551,\n",
       "  565,\n",
       "  44,\n",
       "  538,\n",
       "  57,\n",
       "  582,\n",
       "  534,\n",
       "  582,\n",
       "  555,\n",
       "  575,\n",
       "  539,\n",
       "  572,\n",
       "  685,\n",
       "  92,\n",
       "  630,\n",
       "  125,\n",
       "  629,\n",
       "  94,\n",
       "  530,\n",
       "  530,\n",
       "  530,\n",
       "  524,\n",
       "  97,\n",
       "  566,\n",
       "  109,\n",
       "  225,\n",
       "  287,\n",
       "  424,\n",
       "  84,\n",
       "  230,\n",
       "  584,\n",
       "  100,\n",
       "  233,\n",
       "  419,\n",
       "  233,\n",
       "  418,\n",
       "  182,\n",
       "  754,\n",
       "  832,\n",
       "  389,\n",
       "  226,\n",
       "  400,\n",
       "  226,\n",
       "  305,\n",
       "  790,\n",
       "  337,\n",
       "  226,\n",
       "  77,\n",
       "  226,\n",
       "  386,\n",
       "  82,\n",
       "  223,\n",
       "  225,\n",
       "  77,\n",
       "  226,\n",
       "  906,\n",
       "  849,\n",
       "  849,\n",
       "  224,\n",
       "  82,\n",
       "  219,\n",
       "  528,\n",
       "  545,\n",
       "  541,\n",
       "  275,\n",
       "  61,\n",
       "  121,\n",
       "  419,\n",
       "  38,\n",
       "  274,\n",
       "  122,\n",
       "  44,\n",
       "  39,\n",
       "  84,\n",
       "  98,\n",
       "  70,\n",
       "  512,\n",
       "  520,\n",
       "  701],\n",
       " [79,\n",
       "  309,\n",
       "  515,\n",
       "  72,\n",
       "  616,\n",
       "  65,\n",
       "  125,\n",
       "  109,\n",
       "  74,\n",
       "  37,\n",
       "  31,\n",
       "  72,\n",
       "  94,\n",
       "  86,\n",
       "  37,\n",
       "  33,\n",
       "  53,\n",
       "  194,\n",
       "  420,\n",
       "  179,\n",
       "  146,\n",
       "  194,\n",
       "  79,\n",
       "  63,\n",
       "  116,\n",
       "  41,\n",
       "  214,\n",
       "  129,\n",
       "  257,\n",
       "  39,\n",
       "  39,\n",
       "  54,\n",
       "  71,\n",
       "  75,\n",
       "  83,\n",
       "  50,\n",
       "  33,\n",
       "  56,\n",
       "  507,\n",
       "  49,\n",
       "  96,\n",
       "  47,\n",
       "  68,\n",
       "  80,\n",
       "  212,\n",
       "  344,\n",
       "  35,\n",
       "  149,\n",
       "  49,\n",
       "  63,\n",
       "  115,\n",
       "  63,\n",
       "  83,\n",
       "  65,\n",
       "  120,\n",
       "  198,\n",
       "  174,\n",
       "  216,\n",
       "  199,\n",
       "  116,\n",
       "  78,\n",
       "  167,\n",
       "  52,\n",
       "  167,\n",
       "  74,\n",
       "  50,\n",
       "  52,\n",
       "  39,\n",
       "  82,\n",
       "  178,\n",
       "  99,\n",
       "  87,\n",
       "  211,\n",
       "  713,\n",
       "  125,\n",
       "  147,\n",
       "  132,\n",
       "  62,\n",
       "  81,\n",
       "  100,\n",
       "  135,\n",
       "  100,\n",
       "  140,\n",
       "  89,\n",
       "  157,\n",
       "  289,\n",
       "  33,\n",
       "  209,\n",
       "  1148,\n",
       "  812,\n",
       "  47,\n",
       "  179,\n",
       "  147,\n",
       "  56,\n",
       "  183,\n",
       "  101,\n",
       "  56,\n",
       "  247,\n",
       "  399,\n",
       "  278,\n",
       "  155,\n",
       "  213,\n",
       "  178,\n",
       "  190,\n",
       "  530,\n",
       "  291,\n",
       "  64,\n",
       "  107,\n",
       "  54,\n",
       "  163,\n",
       "  66,\n",
       "  167,\n",
       "  270,\n",
       "  80,\n",
       "  47,\n",
       "  29,\n",
       "  88,\n",
       "  181,\n",
       "  61,\n",
       "  80,\n",
       "  138,\n",
       "  123,\n",
       "  372,\n",
       "  129,\n",
       "  319,\n",
       "  136,\n",
       "  59,\n",
       "  42,\n",
       "  107,\n",
       "  56,\n",
       "  137,\n",
       "  203,\n",
       "  397,\n",
       "  83,\n",
       "  183,\n",
       "  204,\n",
       "  54,\n",
       "  101,\n",
       "  162,\n",
       "  52,\n",
       "  38,\n",
       "  145,\n",
       "  75,\n",
       "  93,\n",
       "  60,\n",
       "  84,\n",
       "  81,\n",
       "  57,\n",
       "  135,\n",
       "  101,\n",
       "  29,\n",
       "  399,\n",
       "  581,\n",
       "  76,\n",
       "  376,\n",
       "  169,\n",
       "  67,\n",
       "  67,\n",
       "  45,\n",
       "  40,\n",
       "  86,\n",
       "  121,\n",
       "  43,\n",
       "  111,\n",
       "  195,\n",
       "  126,\n",
       "  53,\n",
       "  735,\n",
       "  137,\n",
       "  98,\n",
       "  64,\n",
       "  481,\n",
       "  187,\n",
       "  233,\n",
       "  101,\n",
       "  340,\n",
       "  150,\n",
       "  221,\n",
       "  84,\n",
       "  58,\n",
       "  120,\n",
       "  94,\n",
       "  65,\n",
       "  100,\n",
       "  173,\n",
       "  188,\n",
       "  416,\n",
       "  130,\n",
       "  81,\n",
       "  111,\n",
       "  359,\n",
       "  352,\n",
       "  82,\n",
       "  44,\n",
       "  497,\n",
       "  853,\n",
       "  50,\n",
       "  36,\n",
       "  32,\n",
       "  42,\n",
       "  260,\n",
       "  599,\n",
       "  190,\n",
       "  69,\n",
       "  55,\n",
       "  161,\n",
       "  69,\n",
       "  423,\n",
       "  52,\n",
       "  46,\n",
       "  124,\n",
       "  71,\n",
       "  43,\n",
       "  62,\n",
       "  136,\n",
       "  138,\n",
       "  233,\n",
       "  413,\n",
       "  86,\n",
       "  128,\n",
       "  183,\n",
       "  74,\n",
       "  55,\n",
       "  1058,\n",
       "  244,\n",
       "  212,\n",
       "  454,\n",
       "  87,\n",
       "  70,\n",
       "  584,\n",
       "  312,\n",
       "  95,\n",
       "  108,\n",
       "  187,\n",
       "  109,\n",
       "  73,\n",
       "  217,\n",
       "  719,\n",
       "  33,\n",
       "  41,\n",
       "  51,\n",
       "  46,\n",
       "  149,\n",
       "  137,\n",
       "  39,\n",
       "  182,\n",
       "  93,\n",
       "  64,\n",
       "  96,\n",
       "  183,\n",
       "  49,\n",
       "  130,\n",
       "  101,\n",
       "  116,\n",
       "  99,\n",
       "  191,\n",
       "  434,\n",
       "  117,\n",
       "  34,\n",
       "  80,\n",
       "  94,\n",
       "  39,\n",
       "  41,\n",
       "  117,\n",
       "  1165,\n",
       "  406,\n",
       "  80,\n",
       "  185,\n",
       "  118,\n",
       "  39,\n",
       "  51,\n",
       "  278,\n",
       "  574,\n",
       "  94,\n",
       "  85,\n",
       "  54,\n",
       "  79,\n",
       "  115,\n",
       "  89,\n",
       "  57,\n",
       "  54,\n",
       "  261,\n",
       "  1366,\n",
       "  59,\n",
       "  362,\n",
       "  116,\n",
       "  180,\n",
       "  462,\n",
       "  113,\n",
       "  140,\n",
       "  264,\n",
       "  209,\n",
       "  79,\n",
       "  69,\n",
       "  124,\n",
       "  80,\n",
       "  29,\n",
       "  121,\n",
       "  46,\n",
       "  38,\n",
       "  109,\n",
       "  122,\n",
       "  101,\n",
       "  73,\n",
       "  90,\n",
       "  79,\n",
       "  192,\n",
       "  610,\n",
       "  300,\n",
       "  141,\n",
       "  73,\n",
       "  337,\n",
       "  45,\n",
       "  115,\n",
       "  67,\n",
       "  63,\n",
       "  31,\n",
       "  65,\n",
       "  90,\n",
       "  53,\n",
       "  76,\n",
       "  249,\n",
       "  88,\n",
       "  162,\n",
       "  213,\n",
       "  507,\n",
       "  336,\n",
       "  288,\n",
       "  201,\n",
       "  186,\n",
       "  683,\n",
       "  59,\n",
       "  69,\n",
       "  156,\n",
       "  206,\n",
       "  131,\n",
       "  70,\n",
       "  54,\n",
       "  58,\n",
       "  34,\n",
       "  95,\n",
       "  68,\n",
       "  71,\n",
       "  617,\n",
       "  75,\n",
       "  41,\n",
       "  102,\n",
       "  54,\n",
       "  32,\n",
       "  93,\n",
       "  92,\n",
       "  172,\n",
       "  30,\n",
       "  61,\n",
       "  72,\n",
       "  30,\n",
       "  31,\n",
       "  61,\n",
       "  126,\n",
       "  42,\n",
       "  55,\n",
       "  78,\n",
       "  66,\n",
       "  69,\n",
       "  100,\n",
       "  108,\n",
       "  689,\n",
       "  76,\n",
       "  57,\n",
       "  279,\n",
       "  61,\n",
       "  109,\n",
       "  100,\n",
       "  52,\n",
       "  36,\n",
       "  359,\n",
       "  108,\n",
       "  74,\n",
       "  80,\n",
       "  98,\n",
       "  369,\n",
       "  124,\n",
       "  47,\n",
       "  86,\n",
       "  72,\n",
       "  171,\n",
       "  82,\n",
       "  76,\n",
       "  74,\n",
       "  558,\n",
       "  73,\n",
       "  59,\n",
       "  109,\n",
       "  43,\n",
       "  35,\n",
       "  22,\n",
       "  78,\n",
       "  58,\n",
       "  108,\n",
       "  163,\n",
       "  234,\n",
       "  46,\n",
       "  41,\n",
       "  56,\n",
       "  52,\n",
       "  211,\n",
       "  128,\n",
       "  86,\n",
       "  449,\n",
       "  66,\n",
       "  83,\n",
       "  108,\n",
       "  116,\n",
       "  121,\n",
       "  198,\n",
       "  140,\n",
       "  99,\n",
       "  108,\n",
       "  69,\n",
       "  121,\n",
       "  91,\n",
       "  1290,\n",
       "  187,\n",
       "  130,\n",
       "  276,\n",
       "  76,\n",
       "  259,\n",
       "  100,\n",
       "  82,\n",
       "  69,\n",
       "  85,\n",
       "  81,\n",
       "  67,\n",
       "  41,\n",
       "  69,\n",
       "  1300,\n",
       "  68,\n",
       "  113,\n",
       "  378,\n",
       "  81,\n",
       "  754,\n",
       "  107,\n",
       "  101,\n",
       "  176,\n",
       "  80,\n",
       "  63,\n",
       "  89,\n",
       "  45,\n",
       "  200,\n",
       "  179,\n",
       "  40,\n",
       "  45,\n",
       "  96,\n",
       "  278,\n",
       "  53,\n",
       "  67,\n",
       "  142,\n",
       "  41,\n",
       "  81,\n",
       "  77,\n",
       "  136,\n",
       "  88,\n",
       "  100,\n",
       "  169,\n",
       "  326,\n",
       "  87,\n",
       "  309,\n",
       "  82,\n",
       "  49,\n",
       "  97,\n",
       "  38,\n",
       "  45,\n",
       "  30,\n",
       "  49,\n",
       "  122,\n",
       "  40,\n",
       "  40,\n",
       "  65,\n",
       "  57,\n",
       "  70,\n",
       "  76,\n",
       "  188,\n",
       "  46,\n",
       "  240,\n",
       "  57,\n",
       "  44,\n",
       "  88,\n",
       "  66,\n",
       "  102,\n",
       "  107,\n",
       "  159,\n",
       "  67,\n",
       "  26,\n",
       "  100,\n",
       "  131,\n",
       "  82,\n",
       "  122,\n",
       "  92,\n",
       "  137,\n",
       "  57,\n",
       "  372,\n",
       "  29,\n",
       "  1137,\n",
       "  464,\n",
       "  560,\n",
       "  268,\n",
       "  384,\n",
       "  478,\n",
       "  67,\n",
       "  73,\n",
       "  124,\n",
       "  588,\n",
       "  279,\n",
       "  105,\n",
       "  64,\n",
       "  402,\n",
       "  119,\n",
       "  390,\n",
       "  157,\n",
       "  198,\n",
       "  162,\n",
       "  141,\n",
       "  41,\n",
       "  77,\n",
       "  591,\n",
       "  153,\n",
       "  78,\n",
       "  81,\n",
       "  423,\n",
       "  86,\n",
       "  346,\n",
       "  525,\n",
       "  65,\n",
       "  120,\n",
       "  226,\n",
       "  42,\n",
       "  323,\n",
       "  30,\n",
       "  48,\n",
       "  99,\n",
       "  43,\n",
       "  84,\n",
       "  135,\n",
       "  44,\n",
       "  79,\n",
       "  31,\n",
       "  55,\n",
       "  40,\n",
       "  536,\n",
       "  48,\n",
       "  30,\n",
       "  47,\n",
       "  64,\n",
       "  51,\n",
       "  55,\n",
       "  41,\n",
       "  58,\n",
       "  47,\n",
       "  70,\n",
       "  130,\n",
       "  99,\n",
       "  65,\n",
       "  95,\n",
       "  81,\n",
       "  109,\n",
       "  124,\n",
       "  84,\n",
       "  70,\n",
       "  93,\n",
       "  158,\n",
       "  70,\n",
       "  218,\n",
       "  119,\n",
       "  54,\n",
       "  112,\n",
       "  90,\n",
       "  52,\n",
       "  70,\n",
       "  110,\n",
       "  136,\n",
       "  303,\n",
       "  96,\n",
       "  40,\n",
       "  83,\n",
       "  651,\n",
       "  373,\n",
       "  499,\n",
       "  167,\n",
       "  176,\n",
       "  95,\n",
       "  61,\n",
       "  72,\n",
       "  52,\n",
       "  93,\n",
       "  1094,\n",
       "  112,\n",
       "  225,\n",
       "  35,\n",
       "  50,\n",
       "  65,\n",
       "  138,\n",
       "  125,\n",
       "  140,\n",
       "  124,\n",
       "  146,\n",
       "  77,\n",
       "  72,\n",
       "  486,\n",
       "  170,\n",
       "  64,\n",
       "  97,\n",
       "  76,\n",
       "  76,\n",
       "  44,\n",
       "  118,\n",
       "  46,\n",
       "  796,\n",
       "  150,\n",
       "  85,\n",
       "  55,\n",
       "  68,\n",
       "  90,\n",
       "  53,\n",
       "  76,\n",
       "  138,\n",
       "  62,\n",
       "  55,\n",
       "  65,\n",
       "  65,\n",
       "  97,\n",
       "  64,\n",
       "  59,\n",
       "  108,\n",
       "  24,\n",
       "  80,\n",
       "  100,\n",
       "  79,\n",
       "  95,\n",
       "  243,\n",
       "  196,\n",
       "  57,\n",
       "  149,\n",
       "  52,\n",
       "  34,\n",
       "  34,\n",
       "  124,\n",
       "  49,\n",
       "  124,\n",
       "  46,\n",
       "  142,\n",
       "  71,\n",
       "  105,\n",
       "  86,\n",
       "  241,\n",
       "  221,\n",
       "  38,\n",
       "  41,\n",
       "  99,\n",
       "  173,\n",
       "  61,\n",
       "  58,\n",
       "  118,\n",
       "  69,\n",
       "  83,\n",
       "  73,\n",
       "  118,\n",
       "  50,\n",
       "  109,\n",
       "  144,\n",
       "  279,\n",
       "  70,\n",
       "  71,\n",
       "  65,\n",
       "  119,\n",
       "  155,\n",
       "  207,\n",
       "  95,\n",
       "  96,\n",
       "  65,\n",
       "  150,\n",
       "  52,\n",
       "  95,\n",
       "  130,\n",
       "  98,\n",
       "  194,\n",
       "  48,\n",
       "  107,\n",
       "  77,\n",
       "  142,\n",
       "  65,\n",
       "  99,\n",
       "  82,\n",
       "  98,\n",
       "  150,\n",
       "  65,\n",
       "  128,\n",
       "  148,\n",
       "  95,\n",
       "  70,\n",
       "  138,\n",
       "  36,\n",
       "  117,\n",
       "  160,\n",
       "  185,\n",
       "  63,\n",
       "  568,\n",
       "  157,\n",
       "  69,\n",
       "  115,\n",
       "  90,\n",
       "  70,\n",
       "  123,\n",
       "  48,\n",
       "  133,\n",
       "  47,\n",
       "  104,\n",
       "  64,\n",
       "  69,\n",
       "  243,\n",
       "  79,\n",
       "  174,\n",
       "  92,\n",
       "  642,\n",
       "  79,\n",
       "  70,\n",
       "  130,\n",
       "  58,\n",
       "  125,\n",
       "  86,\n",
       "  69,\n",
       "  165,\n",
       "  62,\n",
       "  87,\n",
       "  134,\n",
       "  39,\n",
       "  203,\n",
       "  132,\n",
       "  46,\n",
       "  79,\n",
       "  175,\n",
       "  79,\n",
       "  45,\n",
       "  41,\n",
       "  53,\n",
       "  72,\n",
       "  70,\n",
       "  92,\n",
       "  57,\n",
       "  135,\n",
       "  49,\n",
       "  33,\n",
       "  50,\n",
       "  56,\n",
       "  46,\n",
       "  41,\n",
       "  36,\n",
       "  51,\n",
       "  73,\n",
       "  149,\n",
       "  590,\n",
       "  63,\n",
       "  40,\n",
       "  31,\n",
       "  73,\n",
       "  137,\n",
       "  41,\n",
       "  170,\n",
       "  116,\n",
       "  132,\n",
       "  38,\n",
       "  77,\n",
       "  46,\n",
       "  97,\n",
       "  134,\n",
       "  567,\n",
       "  152,\n",
       "  148,\n",
       "  125,\n",
       "  142,\n",
       "  65,\n",
       "  182,\n",
       "  149,\n",
       "  103,\n",
       "  41,\n",
       "  184,\n",
       "  124,\n",
       "  81,\n",
       "  72,\n",
       "  137,\n",
       "  135,\n",
       "  160,\n",
       "  97,\n",
       "  136,\n",
       "  693,\n",
       "  103,\n",
       "  117,\n",
       "  117,\n",
       "  349,\n",
       "  113,\n",
       "  131,\n",
       "  45,\n",
       "  68,\n",
       "  37,\n",
       "  132,\n",
       "  37,\n",
       "  81,\n",
       "  67,\n",
       "  37,\n",
       "  196,\n",
       "  179,\n",
       "  179,\n",
       "  68,\n",
       "  95,\n",
       "  72,\n",
       "  67,\n",
       "  44,\n",
       "  958,\n",
       "  164,\n",
       "  236,\n",
       "  589,\n",
       "  105,\n",
       "  79,\n",
       "  52,\n",
       "  52,\n",
       "  78,\n",
       "  172,\n",
       "  34,\n",
       "  92,\n",
       "  143,\n",
       "  264,\n",
       "  44,\n",
       "  76,\n",
       "  60,\n",
       "  106,\n",
       "  70,\n",
       "  156,\n",
       "  130,\n",
       "  176,\n",
       "  45,\n",
       "  60,\n",
       "  125,\n",
       "  101,\n",
       "  99,\n",
       "  66,\n",
       "  202,\n",
       "  337,\n",
       "  167,\n",
       "  80,\n",
       "  97,\n",
       "  202,\n",
       "  101,\n",
       "  214,\n",
       "  52,\n",
       "  149,\n",
       "  98,\n",
       "  184,\n",
       "  47,\n",
       "  58,\n",
       "  48,\n",
       "  44,\n",
       "  66,\n",
       "  67,\n",
       "  67,\n",
       "  172,\n",
       "  381,\n",
       "  107,\n",
       "  66,\n",
       "  66,\n",
       "  123,\n",
       "  147,\n",
       "  517,\n",
       "  55,\n",
       "  101,\n",
       "  163,\n",
       "  248,\n",
       "  139,\n",
       "  55,\n",
       "  44,\n",
       "  418,\n",
       "  52,\n",
       "  363,\n",
       "  264,\n",
       "  88,\n",
       "  59,\n",
       "  66,\n",
       "  117,\n",
       "  72,\n",
       "  70,\n",
       "  59,\n",
       "  40,\n",
       "  69,\n",
       "  78,\n",
       "  80,\n",
       "  69,\n",
       "  129,\n",
       "  55,\n",
       "  165,\n",
       "  127,\n",
       "  139,\n",
       "  101,\n",
       "  68,\n",
       "  153,\n",
       "  53,\n",
       "  118,\n",
       "  68,\n",
       "  126,\n",
       "  610,\n",
       "  117,\n",
       "  76,\n",
       "  101,\n",
       "  81,\n",
       "  51,\n",
       "  136,\n",
       "  75,\n",
       "  121,\n",
       "  79,\n",
       "  56,\n",
       "  642,\n",
       "  107,\n",
       "  41,\n",
       "  388,\n",
       "  199,\n",
       "  135,\n",
       "  68,\n",
       "  179,\n",
       "  112,\n",
       "  111,\n",
       "  174,\n",
       "  575,\n",
       "  611,\n",
       "  194,\n",
       "  828,\n",
       "  96,\n",
       "  166,\n",
       "  82,\n",
       "  69,\n",
       "  86,\n",
       "  50,\n",
       "  108,\n",
       "  100,\n",
       "  67,\n",
       "  1297,\n",
       "  66,\n",
       "  212,\n",
       "  58,\n",
       "  182,\n",
       "  145,\n",
       "  262,\n",
       "  134,\n",
       "  64,\n",
       "  66,\n",
       "  172,\n",
       "  61,\n",
       "  116,\n",
       "  717,\n",
       "  130,\n",
       "  64,\n",
       "  111,\n",
       "  139,\n",
       "  90,\n",
       "  159,\n",
       "  87,\n",
       "  431,\n",
       "  67,\n",
       "  55,\n",
       "  55,\n",
       "  69,\n",
       "  46,\n",
       "  40,\n",
       "  175,\n",
       "  113,\n",
       "  44,\n",
       "  62,\n",
       "  37,\n",
       "  826,\n",
       "  33,\n",
       "  35,\n",
       "  83,\n",
       "  68,\n",
       "  88,\n",
       "  61,\n",
       "  65,\n",
       "  62,\n",
       "  74,\n",
       "  64,\n",
       "  63,\n",
       "  183,\n",
       "  33,\n",
       "  158,\n",
       "  58,\n",
       "  45,\n",
       "  45,\n",
       "  75,\n",
       "  57,\n",
       "  76,\n",
       "  70,\n",
       "  75,\n",
       "  160,\n",
       "  72,\n",
       "  105,\n",
       "  ...],\n",
       " [335,\n",
       "  61,\n",
       "  439,\n",
       "  423,\n",
       "  1239,\n",
       "  670,\n",
       "  546,\n",
       "  604,\n",
       "  61,\n",
       "  439,\n",
       "  439,\n",
       "  61,\n",
       "  317,\n",
       "  525,\n",
       "  499,\n",
       "  164,\n",
       "  230,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  60,\n",
       "  438,\n",
       "  422,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  110,\n",
       "  66,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  185,\n",
       "  168,\n",
       "  277,\n",
       "  211,\n",
       "  281,\n",
       "  677,\n",
       "  526,\n",
       "  492,\n",
       "  491,\n",
       "  472,\n",
       "  223,\n",
       "  847,\n",
       "  892,\n",
       "  271,\n",
       "  379,\n",
       "  467,\n",
       "  79,\n",
       "  337,\n",
       "  226,\n",
       "  673,\n",
       "  121,\n",
       "  394,\n",
       "  401,\n",
       "  539,\n",
       "  122,\n",
       "  90,\n",
       "  1434,\n",
       "  1434,\n",
       "  1195,\n",
       "  863,\n",
       "  247,\n",
       "  95,\n",
       "  69,\n",
       "  86,\n",
       "  111,\n",
       "  244,\n",
       "  89,\n",
       "  129,\n",
       "  167,\n",
       "  194,\n",
       "  340,\n",
       "  96,\n",
       "  78,\n",
       "  90,\n",
       "  53,\n",
       "  359,\n",
       "  201,\n",
       "  164,\n",
       "  140,\n",
       "  162,\n",
       "  199,\n",
       "  140,\n",
       "  224,\n",
       "  496,\n",
       "  247,\n",
       "  238,\n",
       "  485,\n",
       "  676,\n",
       "  677,\n",
       "  172,\n",
       "  269,\n",
       "  90,\n",
       "  111,\n",
       "  491,\n",
       "  455,\n",
       "  225,\n",
       "  857,\n",
       "  893,\n",
       "  269,\n",
       "  386,\n",
       "  462,\n",
       "  78,\n",
       "  337,\n",
       "  226,\n",
       "  657,\n",
       "  117,\n",
       "  394,\n",
       "  386,\n",
       "  539,\n",
       "  114,\n",
       "  91,\n",
       "  57,\n",
       "  441,\n",
       "  420,\n",
       "  493,\n",
       "  485,\n",
       "  485,\n",
       "  485,\n",
       "  485,\n",
       "  485,\n",
       "  485,\n",
       "  172,\n",
       "  270,\n",
       "  90,\n",
       "  111,\n",
       "  57,\n",
       "  441,\n",
       "  420,\n",
       "  524,\n",
       "  156,\n",
       "  351,\n",
       "  85,\n",
       "  214,\n",
       "  57,\n",
       "  74,\n",
       "  79,\n",
       "  111,\n",
       "  247,\n",
       "  91,\n",
       "  128,\n",
       "  190,\n",
       "  323,\n",
       "  79,\n",
       "  68,\n",
       "  93,\n",
       "  53,\n",
       "  381,\n",
       "  191,\n",
       "  139,\n",
       "  186,\n",
       "  170,\n",
       "  223,\n",
       "  270,\n",
       "  268,\n",
       "  323,\n",
       "  156,\n",
       "  351,\n",
       "  85,\n",
       "  214,\n",
       "  57,\n",
       "  74,\n",
       "  79,\n",
       "  111,\n",
       "  247,\n",
       "  91,\n",
       "  128,\n",
       "  190,\n",
       "  323,\n",
       "  79,\n",
       "  68,\n",
       "  93,\n",
       "  53,\n",
       "  381,\n",
       "  191,\n",
       "  139,\n",
       "  186,\n",
       "  170,\n",
       "  223,\n",
       "  268,\n",
       "  268,\n",
       "  323,\n",
       "  676,\n",
       "  522,\n",
       "  484,\n",
       "  522,\n",
       "  522,\n",
       "  524,\n",
       "  533,\n",
       "  533,\n",
       "  1253,\n",
       "  657,\n",
       "  533,\n",
       "  493,\n",
       "  499,\n",
       "  164,\n",
       "  230,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  676,\n",
       "  676,\n",
       "  473,\n",
       "  222,\n",
       "  838,\n",
       "  904,\n",
       "  275,\n",
       "  417,\n",
       "  511,\n",
       "  91,\n",
       "  338,\n",
       "  234,\n",
       "  238,\n",
       "  92,\n",
       "  394,\n",
       "  390,\n",
       "  550,\n",
       "  90,\n",
       "  1394,\n",
       "  886,\n",
       "  676,\n",
       "  1401,\n",
       "  1434,\n",
       "  1405,\n",
       "  1434,\n",
       "  677,\n",
       "  677,\n",
       "  677,\n",
       "  1236,\n",
       "  1236,\n",
       "  1236,\n",
       "  1236,\n",
       "  1236,\n",
       "  1236,\n",
       "  1233,\n",
       "  1236,\n",
       "  60,\n",
       "  70,\n",
       "  78,\n",
       "  110,\n",
       "  244,\n",
       "  90,\n",
       "  127,\n",
       "  191,\n",
       "  356,\n",
       "  94,\n",
       "  92,\n",
       "  92,\n",
       "  53,\n",
       "  358,\n",
       "  199,\n",
       "  140,\n",
       "  161,\n",
       "  210,\n",
       "  226,\n",
       "  229,\n",
       "  60,\n",
       "  70,\n",
       "  78,\n",
       "  110,\n",
       "  244,\n",
       "  90,\n",
       "  127,\n",
       "  191,\n",
       "  354,\n",
       "  94,\n",
       "  92,\n",
       "  92,\n",
       "  53,\n",
       "  358,\n",
       "  199,\n",
       "  140,\n",
       "  159,\n",
       "  165,\n",
       "  214,\n",
       "  229,\n",
       "  222,\n",
       "  838,\n",
       "  904,\n",
       "  91,\n",
       "  234,\n",
       "  473,\n",
       "  275,\n",
       "  417,\n",
       "  511,\n",
       "  338,\n",
       "  238,\n",
       "  92,\n",
       "  394,\n",
       "  390,\n",
       "  552,\n",
       "  90,\n",
       "  222,\n",
       "  838,\n",
       "  904,\n",
       "  91,\n",
       "  234,\n",
       "  473,\n",
       "  275,\n",
       "  417,\n",
       "  511,\n",
       "  338,\n",
       "  238,\n",
       "  92,\n",
       "  394,\n",
       "  383,\n",
       "  550,\n",
       "  90,\n",
       "  522,\n",
       "  1141,\n",
       "  493,\n",
       "  520,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1441,\n",
       "  1393,\n",
       "  885,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  654,\n",
       "  681,\n",
       "  530,\n",
       "  530,\n",
       "  47,\n",
       "  138,\n",
       "  319,\n",
       "  212,\n",
       "  74,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  109,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  203,\n",
       "  185,\n",
       "  168,\n",
       "  221,\n",
       "  277,\n",
       "  161,\n",
       "  195,\n",
       "  318,\n",
       "  317,\n",
       "  237,\n",
       "  139,\n",
       "  319,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  108,\n",
       "  68,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  203,\n",
       "  185,\n",
       "  168,\n",
       "  221,\n",
       "  277,\n",
       "  165,\n",
       "  195,\n",
       "  315,\n",
       "  317,\n",
       "  237,\n",
       "  138,\n",
       "  321,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  88,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  111,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  185,\n",
       "  168,\n",
       "  224,\n",
       "  277,\n",
       "  160,\n",
       "  193,\n",
       "  305,\n",
       "  317,\n",
       "  140,\n",
       "  325,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  118,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  185,\n",
       "  168,\n",
       "  224,\n",
       "  277,\n",
       "  163,\n",
       "  197,\n",
       "  312,\n",
       "  317,\n",
       "  237,\n",
       "  139,\n",
       "  319,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  88,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  115,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  186,\n",
       "  168,\n",
       "  222,\n",
       "  277,\n",
       "  160,\n",
       "  193,\n",
       "  300,\n",
       "  317,\n",
       "  138,\n",
       "  320,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  106,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  185,\n",
       "  168,\n",
       "  219,\n",
       "  277,\n",
       "  159,\n",
       "  196,\n",
       "  319,\n",
       "  317,\n",
       "  237,\n",
       "  140,\n",
       "  325,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  117,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  185,\n",
       "  168,\n",
       "  218,\n",
       "  277,\n",
       "  159,\n",
       "  193,\n",
       "  306,\n",
       "  317,\n",
       "  237,\n",
       "  140,\n",
       "  321,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  88,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  120,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  204,\n",
       "  168,\n",
       "  222,\n",
       "  277,\n",
       "  160,\n",
       "  193,\n",
       "  304,\n",
       "  317,\n",
       "  817,\n",
       "  139,\n",
       "  321,\n",
       "  212,\n",
       "  73,\n",
       "  79,\n",
       "  111,\n",
       "  250,\n",
       "  88,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  325,\n",
       "  304,\n",
       "  115,\n",
       "  70,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  187,\n",
       "  168,\n",
       "  224,\n",
       "  277,\n",
       "  160,\n",
       "  194,\n",
       "  297,\n",
       "  317,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  485,\n",
       "  485,\n",
       "  530,\n",
       "  530,\n",
       "  660,\n",
       "  654,\n",
       "  657,\n",
       "  524,\n",
       "  1440,\n",
       "  1440,\n",
       "  485,\n",
       "  1304,\n",
       "  485,\n",
       "  485,\n",
       "  485,\n",
       "  485,\n",
       "  1438,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  546,\n",
       "  604,\n",
       "  473,\n",
       "  222,\n",
       "  838,\n",
       "  904,\n",
       "  275,\n",
       "  417,\n",
       "  511,\n",
       "  91,\n",
       "  338,\n",
       "  234,\n",
       "  238,\n",
       "  92,\n",
       "  394,\n",
       "  390,\n",
       "  550,\n",
       "  90,\n",
       "  95,\n",
       "  101,\n",
       "  255,\n",
       "  209,\n",
       "  198,\n",
       "  68,\n",
       "  58,\n",
       "  80,\n",
       "  72,\n",
       "  204,\n",
       "  315,\n",
       "  299,\n",
       "  317,\n",
       "  198,\n",
       "  233,\n",
       "  237,\n",
       "  123,\n",
       "  276,\n",
       "  205,\n",
       "  211,\n",
       "  404,\n",
       "  306,\n",
       "  96,\n",
       "  66,\n",
       "  461,\n",
       "  54,\n",
       "  398,\n",
       "  381,\n",
       "  188,\n",
       "  78,\n",
       "  121,\n",
       "  289,\n",
       "  877,\n",
       "  104,\n",
       "  718,\n",
       "  357,\n",
       "  355,\n",
       "  205,\n",
       "  180,\n",
       "  125,\n",
       "  213,\n",
       "  323,\n",
       "  66,\n",
       "  171,\n",
       "  367,\n",
       "  77,\n",
       "  177,\n",
       "  167,\n",
       "  216,\n",
       "  297,\n",
       "  175,\n",
       "  400,\n",
       "  340,\n",
       "  191,\n",
       "  212,\n",
       "  220,\n",
       "  189,\n",
       "  160,\n",
       "  298,\n",
       "  182,\n",
       "  221,\n",
       "  248,\n",
       "  250,\n",
       "  281,\n",
       "  229,\n",
       "  245,\n",
       "  240,\n",
       "  265,\n",
       "  266,\n",
       "  259,\n",
       "  250,\n",
       "  229,\n",
       "  327,\n",
       "  339,\n",
       "  330,\n",
       "  341,\n",
       "  477,\n",
       "  101,\n",
       "  255,\n",
       "  206,\n",
       "  175,\n",
       "  65,\n",
       "  68,\n",
       "  59,\n",
       "  80,\n",
       "  72,\n",
       "  203,\n",
       "  298,\n",
       "  323,\n",
       "  198,\n",
       "  236,\n",
       "  237,\n",
       "  123,\n",
       "  231,\n",
       "  275,\n",
       "  211,\n",
       "  211,\n",
       "  404,\n",
       "  452,\n",
       "  305,\n",
       "  96,\n",
       "  66,\n",
       "  465,\n",
       "  398,\n",
       "  381,\n",
       "  188,\n",
       "  78,\n",
       "  119,\n",
       "  289,\n",
       "  876,\n",
       "  105,\n",
       "  718,\n",
       "  357,\n",
       "  355,\n",
       "  409,\n",
       "  204,\n",
       "  181,\n",
       "  125,\n",
       "  213,\n",
       "  323,\n",
       "  66,\n",
       "  171,\n",
       "  77,\n",
       "  177,\n",
       "  167,\n",
       "  217,\n",
       "  310,\n",
       "  175,\n",
       "  400,\n",
       "  355,\n",
       "  191,\n",
       "  215,\n",
       "  220,\n",
       "  189,\n",
       "  161,\n",
       "  299,\n",
       "  182,\n",
       "  248,\n",
       "  250,\n",
       "  281,\n",
       "  253,\n",
       "  245,\n",
       "  265,\n",
       "  266,\n",
       "  259,\n",
       "  250,\n",
       "  229,\n",
       "  486,\n",
       "  55,\n",
       "  240,\n",
       "  681,\n",
       "  1417,\n",
       "  1419,\n",
       "  1417,\n",
       "  1419,\n",
       "  1419,\n",
       "  485,\n",
       "  485,\n",
       "  473,\n",
       "  222,\n",
       "  838,\n",
       "  904,\n",
       "  275,\n",
       "  415,\n",
       "  511,\n",
       "  91,\n",
       "  338,\n",
       "  234,\n",
       "  239,\n",
       "  92,\n",
       "  394,\n",
       "  383,\n",
       "  550,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  164,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  117,\n",
       "  496,\n",
       "  229,\n",
       "  131,\n",
       "  149,\n",
       "  252,\n",
       "  212,\n",
       "  35,\n",
       "  73,\n",
       "  79,\n",
       "  250,\n",
       "  87,\n",
       "  128,\n",
       "  133,\n",
       "  189,\n",
       "  324,\n",
       "  304,\n",
       "  108,\n",
       "  66,\n",
       "  90,\n",
       "  53,\n",
       "  377,\n",
       "  202,\n",
       "  185,\n",
       "  168,\n",
       "  160,\n",
       "  277,\n",
       "  211,\n",
       "  281,\n",
       "  317,\n",
       "  473,\n",
       "  222,\n",
       "  838,\n",
       "  904,\n",
       "  275,\n",
       "  415,\n",
       "  511,\n",
       "  91,\n",
       "  338,\n",
       "  234,\n",
       "  239,\n",
       "  92,\n",
       "  394,\n",
       "  383,\n",
       "  550,\n",
       "  490,\n",
       "  75,\n",
       "  226,\n",
       "  130,\n",
       "  126,\n",
       "  248,\n",
       "  116,\n",
       "  23,\n",
       "  533,\n",
       "  1069,\n",
       "  454,\n",
       "  676,\n",
       "  1076,\n",
       "  1076,\n",
       "  681,\n",
       "  681,\n",
       "  681,\n",
       "  681,\n",
       "  ...]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug = dl.dataset.aug\n",
    "aug.croprange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([206., 140., 171., 126.,  82.,  37.,  48.,  24.,  21.,   6.]),\n",
       " array([ 35. , 110.5, 186. , 261.5, 337. , 412.5, 488. , 563.5, 639. ,\n",
       "        714.5, 790. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = aug.croprange[0]\n",
    "t = [x for x in t if x < 800]\n",
    "plt.hist(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([81966., 26183., 10538.,  4965.,  2913.,  1776.,  1224.,   877.,\n",
       "          600.,   458.]),\n",
       " array([ 18. ,  96.1, 174.2, 252.3, 330.4, 408.5, 486.6, 564.7, 642.8,\n",
       "        720.9, 799. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = aug.croprange[1]\n",
    "t = [x for x in t if x < 800]\n",
    "plt.hist(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(embed2)\n",
      "\u001b[1;32m      2\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data2/tyfei/COVID/20241209/tipS1_esmc.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, q, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "q = np.array(embed2)\n",
    "q = q.squeeze()\n",
    "np.savetxt(\"/data2/tyfei/COVID/20241209/tipS1_esmc.csv\", q, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"params\" in configs[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initized model for base_learning stage\n"
     ]
    }
   ],
   "source": [
    "pretrain_model = trainUtils.loadPretrainModel(configs)\n",
    "model = trainUtils.buildModel(configs, pretrain_model)\n",
    "# ds = trainUtils.loadDataset(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_label_weights False\n",
      "pos_weights False\n",
      "clf.l1.weight True\n",
      "clf.l1.bias True\n",
      "clf.l2.weight True\n",
      "clf.l2.bias True\n",
      "clf.l3.weight True\n",
      "clf.l3.bias True\n",
      "clf.ln1.weight True\n",
      "clf.ln1.bias True\n",
      "clf.ln2.weight True\n",
      "clf.ln2.bias True\n",
      "dis.l1.weight True\n",
      "dis.l1.bias True\n",
      "dis.l2.weight True\n",
      "dis.l2.bias True\n",
      "dis.l3.weight True\n",
      "dis.l3.bias True\n",
      "dis.ln1.weight True\n",
      "dis.ln1.bias True\n",
      "dis.ln2.weight True\n",
      "dis.ln2.bias True\n",
      "additional_clf.l1.weight True\n",
      "additional_clf.l1.bias True\n",
      "additional_clf.l2.weight True\n",
      "additional_clf.l2.bias True\n",
      "additional_clf.l3.weight True\n",
      "additional_clf.l3.bias True\n",
      "additional_clf.ln1.weight True\n",
      "additional_clf.ln1.bias True\n",
      "additional_clf.ln2.weight True\n",
      "additional_clf.ln2.bias True\n",
      "esm_model.encoder.sequence_embed.weight False\n",
      "esm_model.encoder.plddt_projection.weight False\n",
      "esm_model.encoder.plddt_projection.bias False\n",
      "esm_model.encoder.structure_per_res_plddt_projection.weight False\n",
      "esm_model.encoder.structure_per_res_plddt_projection.bias False\n",
      "esm_model.encoder.structure_tokens_embed.weight False\n",
      "esm_model.encoder.ss8_embed.weight False\n",
      "esm_model.encoder.sasa_embed.weight False\n",
      "esm_model.encoder.function_embed.0.weight False\n",
      "esm_model.encoder.function_embed.1.weight False\n",
      "esm_model.encoder.function_embed.2.weight False\n",
      "esm_model.encoder.function_embed.3.weight False\n",
      "esm_model.encoder.function_embed.4.weight False\n",
      "esm_model.encoder.function_embed.5.weight False\n",
      "esm_model.encoder.function_embed.6.weight False\n",
      "esm_model.encoder.function_embed.7.weight False\n",
      "esm_model.encoder.residue_embed.weight False\n",
      "esm_model.transformer.blocks.0.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.0.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.0.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.0.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.0.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.0.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.0.geom_attn.distance_scale_per_head False\n",
      "esm_model.transformer.blocks.0.geom_attn.rotation_scale_per_head False\n",
      "esm_model.transformer.blocks.0.geom_attn.s_norm.weight False\n",
      "esm_model.transformer.blocks.0.geom_attn.proj.weight False\n",
      "esm_model.transformer.blocks.0.geom_attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.0.ffn.0.weight False\n",
      "esm_model.transformer.blocks.0.ffn.0.bias False\n",
      "esm_model.transformer.blocks.0.ffn.1.weight False\n",
      "esm_model.transformer.blocks.0.ffn.3.weight False\n",
      "esm_model.transformer.blocks.1.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.1.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.1.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.1.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.1.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.1.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.1.ffn.0.weight False\n",
      "esm_model.transformer.blocks.1.ffn.0.bias False\n",
      "esm_model.transformer.blocks.1.ffn.1.weight False\n",
      "esm_model.transformer.blocks.1.ffn.3.weight False\n",
      "esm_model.transformer.blocks.2.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.2.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.2.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.2.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.2.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.2.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.2.ffn.0.weight False\n",
      "esm_model.transformer.blocks.2.ffn.0.bias False\n",
      "esm_model.transformer.blocks.2.ffn.1.weight False\n",
      "esm_model.transformer.blocks.2.ffn.3.weight False\n",
      "esm_model.transformer.blocks.3.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.3.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.3.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.3.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.3.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.3.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.3.ffn.0.weight False\n",
      "esm_model.transformer.blocks.3.ffn.0.bias False\n",
      "esm_model.transformer.blocks.3.ffn.1.weight False\n",
      "esm_model.transformer.blocks.3.ffn.3.weight False\n",
      "esm_model.transformer.blocks.4.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.4.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.4.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.4.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.4.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.4.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.4.ffn.0.weight False\n",
      "esm_model.transformer.blocks.4.ffn.0.bias False\n",
      "esm_model.transformer.blocks.4.ffn.1.weight False\n",
      "esm_model.transformer.blocks.4.ffn.3.weight False\n",
      "esm_model.transformer.blocks.5.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.5.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.5.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.5.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.5.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.5.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.5.ffn.0.weight False\n",
      "esm_model.transformer.blocks.5.ffn.0.bias False\n",
      "esm_model.transformer.blocks.5.ffn.1.weight False\n",
      "esm_model.transformer.blocks.5.ffn.3.weight False\n",
      "esm_model.transformer.blocks.6.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.6.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.6.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.6.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.6.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.6.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.6.ffn.0.weight False\n",
      "esm_model.transformer.blocks.6.ffn.0.bias False\n",
      "esm_model.transformer.blocks.6.ffn.1.weight False\n",
      "esm_model.transformer.blocks.6.ffn.3.weight False\n",
      "esm_model.transformer.blocks.7.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.7.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.7.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.7.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.7.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.7.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.7.ffn.0.weight False\n",
      "esm_model.transformer.blocks.7.ffn.0.bias False\n",
      "esm_model.transformer.blocks.7.ffn.1.weight False\n",
      "esm_model.transformer.blocks.7.ffn.3.weight False\n",
      "esm_model.transformer.blocks.8.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.8.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.8.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.8.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.8.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.8.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.8.ffn.0.weight False\n",
      "esm_model.transformer.blocks.8.ffn.0.bias False\n",
      "esm_model.transformer.blocks.8.ffn.1.weight False\n",
      "esm_model.transformer.blocks.8.ffn.3.weight False\n",
      "esm_model.transformer.blocks.9.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.9.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.9.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.9.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.9.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.9.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.9.ffn.0.weight False\n",
      "esm_model.transformer.blocks.9.ffn.0.bias False\n",
      "esm_model.transformer.blocks.9.ffn.1.weight False\n",
      "esm_model.transformer.blocks.9.ffn.3.weight False\n",
      "esm_model.transformer.blocks.10.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.10.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.10.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.10.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.10.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.10.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.10.ffn.0.weight False\n",
      "esm_model.transformer.blocks.10.ffn.0.bias False\n",
      "esm_model.transformer.blocks.10.ffn.1.weight False\n",
      "esm_model.transformer.blocks.10.ffn.3.weight False\n",
      "esm_model.transformer.blocks.11.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.11.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.11.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.11.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.11.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.11.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.11.ffn.0.weight False\n",
      "esm_model.transformer.blocks.11.ffn.0.bias False\n",
      "esm_model.transformer.blocks.11.ffn.1.weight False\n",
      "esm_model.transformer.blocks.11.ffn.3.weight False\n",
      "esm_model.transformer.blocks.12.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.12.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.12.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.12.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.12.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.12.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.12.ffn.0.weight False\n",
      "esm_model.transformer.blocks.12.ffn.0.bias False\n",
      "esm_model.transformer.blocks.12.ffn.1.weight False\n",
      "esm_model.transformer.blocks.12.ffn.3.weight False\n",
      "esm_model.transformer.blocks.13.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.13.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.13.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.13.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.13.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.13.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.13.ffn.0.weight False\n",
      "esm_model.transformer.blocks.13.ffn.0.bias False\n",
      "esm_model.transformer.blocks.13.ffn.1.weight False\n",
      "esm_model.transformer.blocks.13.ffn.3.weight False\n",
      "esm_model.transformer.blocks.14.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.14.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.14.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.14.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.14.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.14.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.14.ffn.0.weight False\n",
      "esm_model.transformer.blocks.14.ffn.0.bias False\n",
      "esm_model.transformer.blocks.14.ffn.1.weight False\n",
      "esm_model.transformer.blocks.14.ffn.3.weight False\n",
      "esm_model.transformer.blocks.15.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.15.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.15.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.15.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.15.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.15.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.15.ffn.0.weight False\n",
      "esm_model.transformer.blocks.15.ffn.0.bias False\n",
      "esm_model.transformer.blocks.15.ffn.1.weight False\n",
      "esm_model.transformer.blocks.15.ffn.3.weight False\n",
      "esm_model.transformer.blocks.16.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.16.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.16.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.16.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.16.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.16.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.16.ffn.0.weight False\n",
      "esm_model.transformer.blocks.16.ffn.0.bias False\n",
      "esm_model.transformer.blocks.16.ffn.1.weight False\n",
      "esm_model.transformer.blocks.16.ffn.3.weight False\n",
      "esm_model.transformer.blocks.17.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.17.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.17.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.17.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.17.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.17.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.17.ffn.0.weight False\n",
      "esm_model.transformer.blocks.17.ffn.0.bias False\n",
      "esm_model.transformer.blocks.17.ffn.1.weight False\n",
      "esm_model.transformer.blocks.17.ffn.3.weight False\n",
      "esm_model.transformer.blocks.18.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.18.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.18.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.18.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.18.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.18.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.18.ffn.0.weight False\n",
      "esm_model.transformer.blocks.18.ffn.0.bias False\n",
      "esm_model.transformer.blocks.18.ffn.1.weight False\n",
      "esm_model.transformer.blocks.18.ffn.3.weight False\n",
      "esm_model.transformer.blocks.19.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.19.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.19.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.19.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.19.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.19.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.19.ffn.0.weight False\n",
      "esm_model.transformer.blocks.19.ffn.0.bias False\n",
      "esm_model.transformer.blocks.19.ffn.1.weight False\n",
      "esm_model.transformer.blocks.19.ffn.3.weight False\n",
      "esm_model.transformer.blocks.20.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.20.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.20.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.20.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.20.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.20.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.20.ffn.0.weight False\n",
      "esm_model.transformer.blocks.20.ffn.0.bias False\n",
      "esm_model.transformer.blocks.20.ffn.1.weight False\n",
      "esm_model.transformer.blocks.20.ffn.3.weight False\n",
      "esm_model.transformer.blocks.21.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.21.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.21.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.21.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.21.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.21.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.21.ffn.0.weight False\n",
      "esm_model.transformer.blocks.21.ffn.0.bias False\n",
      "esm_model.transformer.blocks.21.ffn.1.weight False\n",
      "esm_model.transformer.blocks.21.ffn.3.weight False\n",
      "esm_model.transformer.blocks.22.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.22.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.22.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.22.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.22.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.22.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.22.ffn.0.weight False\n",
      "esm_model.transformer.blocks.22.ffn.0.bias False\n",
      "esm_model.transformer.blocks.22.ffn.1.weight False\n",
      "esm_model.transformer.blocks.22.ffn.3.weight False\n",
      "esm_model.transformer.blocks.23.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.23.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.23.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.23.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.23.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.23.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.23.ffn.0.weight False\n",
      "esm_model.transformer.blocks.23.ffn.0.bias False\n",
      "esm_model.transformer.blocks.23.ffn.1.weight False\n",
      "esm_model.transformer.blocks.23.ffn.3.weight False\n",
      "esm_model.transformer.blocks.24.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.24.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.24.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.24.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.24.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.24.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.24.ffn.0.weight False\n",
      "esm_model.transformer.blocks.24.ffn.0.bias False\n",
      "esm_model.transformer.blocks.24.ffn.1.weight False\n",
      "esm_model.transformer.blocks.24.ffn.3.weight False\n",
      "esm_model.transformer.blocks.25.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.25.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.25.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.25.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.25.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.25.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.25.ffn.0.weight False\n",
      "esm_model.transformer.blocks.25.ffn.0.bias False\n",
      "esm_model.transformer.blocks.25.ffn.1.weight False\n",
      "esm_model.transformer.blocks.25.ffn.3.weight False\n",
      "esm_model.transformer.blocks.26.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.26.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.26.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.26.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.26.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.26.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.26.ffn.0.weight False\n",
      "esm_model.transformer.blocks.26.ffn.0.bias False\n",
      "esm_model.transformer.blocks.26.ffn.1.weight False\n",
      "esm_model.transformer.blocks.26.ffn.3.weight False\n",
      "esm_model.transformer.blocks.27.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.27.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.27.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.27.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.27.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.27.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.27.ffn.0.weight False\n",
      "esm_model.transformer.blocks.27.ffn.0.bias False\n",
      "esm_model.transformer.blocks.27.ffn.1.weight False\n",
      "esm_model.transformer.blocks.27.ffn.3.weight False\n",
      "esm_model.transformer.blocks.28.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.28.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.28.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.28.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.28.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.28.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.28.ffn.0.weight False\n",
      "esm_model.transformer.blocks.28.ffn.0.bias False\n",
      "esm_model.transformer.blocks.28.ffn.1.weight False\n",
      "esm_model.transformer.blocks.28.ffn.3.weight False\n",
      "esm_model.transformer.blocks.29.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.29.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.29.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.29.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.29.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.29.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.29.ffn.0.weight False\n",
      "esm_model.transformer.blocks.29.ffn.0.bias False\n",
      "esm_model.transformer.blocks.29.ffn.1.weight False\n",
      "esm_model.transformer.blocks.29.ffn.3.weight False\n",
      "esm_model.transformer.blocks.30.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.30.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.30.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.30.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.30.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.30.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.30.ffn.0.weight False\n",
      "esm_model.transformer.blocks.30.ffn.0.bias False\n",
      "esm_model.transformer.blocks.30.ffn.1.weight False\n",
      "esm_model.transformer.blocks.30.ffn.3.weight False\n",
      "esm_model.transformer.blocks.31.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.31.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.31.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.31.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.31.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.31.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.31.ffn.0.weight False\n",
      "esm_model.transformer.blocks.31.ffn.0.bias False\n",
      "esm_model.transformer.blocks.31.ffn.1.weight False\n",
      "esm_model.transformer.blocks.31.ffn.3.weight False\n",
      "esm_model.transformer.blocks.32.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.32.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.32.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.32.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.32.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.32.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.32.ffn.0.weight False\n",
      "esm_model.transformer.blocks.32.ffn.0.bias False\n",
      "esm_model.transformer.blocks.32.ffn.1.weight False\n",
      "esm_model.transformer.blocks.32.ffn.3.weight False\n",
      "esm_model.transformer.blocks.33.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.33.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.33.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.33.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.33.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.33.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.33.ffn.0.weight False\n",
      "esm_model.transformer.blocks.33.ffn.0.bias False\n",
      "esm_model.transformer.blocks.33.ffn.1.weight False\n",
      "esm_model.transformer.blocks.33.ffn.3.weight False\n",
      "esm_model.transformer.blocks.34.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.34.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.34.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.34.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.34.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.34.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.34.ffn.0.weight False\n",
      "esm_model.transformer.blocks.34.ffn.0.bias False\n",
      "esm_model.transformer.blocks.34.ffn.1.weight False\n",
      "esm_model.transformer.blocks.34.ffn.3.weight False\n",
      "esm_model.transformer.blocks.35.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.35.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.35.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.35.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.35.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.35.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.35.ffn.0.weight False\n",
      "esm_model.transformer.blocks.35.ffn.0.bias False\n",
      "esm_model.transformer.blocks.35.ffn.1.weight False\n",
      "esm_model.transformer.blocks.35.ffn.3.weight False\n",
      "esm_model.transformer.blocks.36.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.36.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.36.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.36.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.36.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.36.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.36.ffn.0.weight False\n",
      "esm_model.transformer.blocks.36.ffn.0.bias False\n",
      "esm_model.transformer.blocks.36.ffn.1.weight False\n",
      "esm_model.transformer.blocks.36.ffn.3.weight False\n",
      "esm_model.transformer.blocks.37.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.37.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.37.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.37.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.37.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.37.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.37.ffn.0.weight False\n",
      "esm_model.transformer.blocks.37.ffn.0.bias False\n",
      "esm_model.transformer.blocks.37.ffn.1.weight False\n",
      "esm_model.transformer.blocks.37.ffn.3.weight False\n",
      "esm_model.transformer.blocks.38.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.38.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.38.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.38.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.38.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.38.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.38.ffn.0.weight False\n",
      "esm_model.transformer.blocks.38.ffn.0.bias False\n",
      "esm_model.transformer.blocks.38.ffn.1.weight False\n",
      "esm_model.transformer.blocks.38.ffn.3.weight False\n",
      "esm_model.transformer.blocks.39.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.39.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.39.attn.layernorm_qkv.1.weight False\n",
      "esm_model.transformer.blocks.39.attn.out_proj.weight False\n",
      "esm_model.transformer.blocks.39.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.39.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.39.ffn.0.weight False\n",
      "esm_model.transformer.blocks.39.ffn.0.bias False\n",
      "esm_model.transformer.blocks.39.ffn.1.weight False\n",
      "esm_model.transformer.blocks.39.ffn.3.weight False\n",
      "esm_model.transformer.blocks.40.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.40.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.40.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.40.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.40.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.40.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.40.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.40.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.40.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.40.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.40.ffn.0.weight False\n",
      "esm_model.transformer.blocks.40.ffn.0.bias False\n",
      "esm_model.transformer.blocks.40.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.40.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.40.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.40.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.40.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.40.ffn.3.lora.B True\n",
      "esm_model.transformer.blocks.41.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.41.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.41.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.41.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.41.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.41.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.41.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.41.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.41.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.41.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.41.ffn.0.weight False\n",
      "esm_model.transformer.blocks.41.ffn.0.bias False\n",
      "esm_model.transformer.blocks.41.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.41.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.41.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.41.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.41.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.41.ffn.3.lora.B True\n",
      "esm_model.transformer.blocks.42.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.42.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.42.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.42.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.42.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.42.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.42.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.42.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.42.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.42.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.42.ffn.0.weight False\n",
      "esm_model.transformer.blocks.42.ffn.0.bias False\n",
      "esm_model.transformer.blocks.42.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.42.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.42.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.42.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.42.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.42.ffn.3.lora.B True\n",
      "esm_model.transformer.blocks.43.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.43.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.43.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.43.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.43.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.43.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.43.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.43.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.43.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.43.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.43.ffn.0.weight False\n",
      "esm_model.transformer.blocks.43.ffn.0.bias False\n",
      "esm_model.transformer.blocks.43.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.43.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.43.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.43.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.43.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.43.ffn.3.lora.B True\n",
      "esm_model.transformer.blocks.44.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.44.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.44.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.44.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.44.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.44.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.44.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.44.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.44.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.44.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.44.ffn.0.weight False\n",
      "esm_model.transformer.blocks.44.ffn.0.bias False\n",
      "esm_model.transformer.blocks.44.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.44.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.44.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.44.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.44.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.44.ffn.3.lora.B True\n",
      "esm_model.transformer.blocks.45.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.45.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.45.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.45.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.45.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.45.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.45.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.45.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.45.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.45.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.45.ffn.0.weight False\n",
      "esm_model.transformer.blocks.45.ffn.0.bias False\n",
      "esm_model.transformer.blocks.45.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.45.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.45.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.45.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.45.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.45.ffn.3.lora.B True\n",
      "esm_model.transformer.blocks.46.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.46.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.46.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.46.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.46.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.46.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.46.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.46.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.46.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.46.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.46.ffn.0.weight False\n",
      "esm_model.transformer.blocks.46.ffn.0.bias False\n",
      "esm_model.transformer.blocks.46.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.46.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.46.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.46.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.46.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.46.ffn.3.lora.B True\n",
      "esm_model.transformer.blocks.47.attn.layernorm_qkv.0.weight False\n",
      "esm_model.transformer.blocks.47.attn.layernorm_qkv.0.bias False\n",
      "esm_model.transformer.blocks.47.attn.layernorm_qkv.1.linear.weight False\n",
      "esm_model.transformer.blocks.47.attn.layernorm_qkv.1.lora.A True\n",
      "esm_model.transformer.blocks.47.attn.layernorm_qkv.1.lora.B True\n",
      "esm_model.transformer.blocks.47.attn.out_proj.linear.weight False\n",
      "esm_model.transformer.blocks.47.attn.out_proj.lora.A True\n",
      "esm_model.transformer.blocks.47.attn.out_proj.lora.B True\n",
      "esm_model.transformer.blocks.47.attn.q_ln.weight False\n",
      "esm_model.transformer.blocks.47.attn.k_ln.weight False\n",
      "esm_model.transformer.blocks.47.ffn.0.weight False\n",
      "esm_model.transformer.blocks.47.ffn.0.bias False\n",
      "esm_model.transformer.blocks.47.ffn.1.linear.weight False\n",
      "esm_model.transformer.blocks.47.ffn.1.lora.A True\n",
      "esm_model.transformer.blocks.47.ffn.1.lora.B True\n",
      "esm_model.transformer.blocks.47.ffn.3.linear.weight False\n",
      "esm_model.transformer.blocks.47.ffn.3.lora.A True\n",
      "esm_model.transformer.blocks.47.ffn.3.lora.B True\n",
      "esm_model.transformer.norm.weight False\n",
      "esm_model.output_heads.sequence_head.0.weight False\n",
      "esm_model.output_heads.sequence_head.0.bias False\n",
      "esm_model.output_heads.sequence_head.2.weight False\n",
      "esm_model.output_heads.sequence_head.2.bias False\n",
      "esm_model.output_heads.sequence_head.3.weight False\n",
      "esm_model.output_heads.sequence_head.3.bias False\n",
      "esm_model.output_heads.structure_head.0.weight False\n",
      "esm_model.output_heads.structure_head.0.bias False\n",
      "esm_model.output_heads.structure_head.2.weight False\n",
      "esm_model.output_heads.structure_head.2.bias False\n",
      "esm_model.output_heads.structure_head.3.weight False\n",
      "esm_model.output_heads.structure_head.3.bias False\n",
      "esm_model.output_heads.ss8_head.0.weight False\n",
      "esm_model.output_heads.ss8_head.0.bias False\n",
      "esm_model.output_heads.ss8_head.2.weight False\n",
      "esm_model.output_heads.ss8_head.2.bias False\n",
      "esm_model.output_heads.ss8_head.3.weight False\n",
      "esm_model.output_heads.ss8_head.3.bias False\n",
      "esm_model.output_heads.sasa_head.0.weight False\n",
      "esm_model.output_heads.sasa_head.0.bias False\n",
      "esm_model.output_heads.sasa_head.2.weight False\n",
      "esm_model.output_heads.sasa_head.2.bias False\n",
      "esm_model.output_heads.sasa_head.3.weight False\n",
      "esm_model.output_heads.sasa_head.3.bias False\n",
      "esm_model.output_heads.function_head.0.weight False\n",
      "esm_model.output_heads.function_head.0.bias False\n",
      "esm_model.output_heads.function_head.2.weight False\n",
      "esm_model.output_heads.function_head.2.bias False\n",
      "esm_model.output_heads.function_head.3.weight False\n",
      "esm_model.output_heads.function_head.3.bias False\n",
      "esm_model.output_heads.residue_head.0.weight False\n",
      "esm_model.output_heads.residue_head.0.bias False\n",
      "esm_model.output_heads.residue_head.2.weight False\n",
      "esm_model.output_heads.residue_head.2.bias False\n",
      "esm_model.output_heads.residue_head.3.weight False\n",
      "esm_model.output_heads.residue_head.3.bias False\n"
     ]
    }
   ],
   "source": [
    "for i, j in model.named_parameters():\n",
    "    print(i, j.requires_grad)\n",
    "    # if j.requires_grad:\n",
    "    #     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get val loader\n",
      "called new epoch\n"
     ]
    }
   ],
   "source": [
    "dl = ds.val_dataloader() \n",
    "for b, data in enumerate(dl):\n",
    "    # print(x.shape, y.shape)\n",
    "    # break\n",
    "    pass\n",
    "data = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = [] \n",
    "with torch.no_grad():\n",
    "    for i in data:\n",
    "        embeds.append(model.getEmbedding(i)[0, 0])\n",
    "embeds = torch.stack(embeds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0194, -0.0585, -0.0101,  ...,  0.0017,  0.0020,  0.0520],\n",
       "        [-0.0221, -0.0045, -0.0094,  ...,  0.0113, -0.0008,  0.0428],\n",
       "        [-0.0037, -0.0628, -0.0068,  ...,  0.0018, -0.0051,  0.0293],\n",
       "        ...,\n",
       "        [ 0.0645, -0.0462, -0.0138,  ...,  0.0043, -0.0055,  0.0442],\n",
       "        [-0.0057, -0.0111, -0.0052,  ...,  0.0007, -0.0075,  0.0222],\n",
       "        [ 0.0452, -0.0555, -0.0097,  ...,  0.0008,  0.0002,  0.0428]],\n",
       "       device='cuda:6')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([85, 1152])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsmodel = models.Linearcls(1152, dropout=0.2, p0=0.2, take_embed=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(186.8812, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.88116455078125\n",
      "tensor(194.4602, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.4601593017578\n",
      "tensor(193.8147, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.814697265625\n",
      "tensor(192.1247, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.12472534179688\n",
      "tensor(193.7268, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.72677612304688\n",
      "tensor(195.2809, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "195.28085327148438\n",
      "tensor(191.1854, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.1854248046875\n",
      "tensor(186.7961, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.79605102539062\n",
      "tensor(199.3803, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "199.3802947998047\n",
      "tensor(183.5143, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.51434326171875\n",
      "tensor(181.0327, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.03271484375\n",
      "tensor(191.5052, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.50521850585938\n",
      "tensor(186.5496, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.549560546875\n",
      "tensor(178.9835, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.9835205078125\n",
      "tensor(194.4560, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.45603942871094\n",
      "tensor(188.1657, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.16567993164062\n",
      "tensor(193.4260, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.426025390625\n",
      "tensor(190.2678, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.26783752441406\n",
      "tensor(182.4524, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.452392578125\n",
      "tensor(188.9237, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.92373657226562\n",
      "tensor(189.4861, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.486083984375\n",
      "tensor(188.9164, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.9163818359375\n",
      "tensor(179.3625, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.36253356933594\n",
      "tensor(186.8865, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.886474609375\n",
      "tensor(196.1399, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "196.13986206054688\n",
      "tensor(205.9596, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "205.95962524414062\n",
      "tensor(194.4141, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.41409301757812\n",
      "tensor(188.5396, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.53961181640625\n",
      "tensor(189.7874, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.78744506835938\n",
      "tensor(182.0814, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.08143615722656\n",
      "tensor(187.8572, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.85720825195312\n",
      "tensor(180.5676, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.56759643554688\n",
      "tensor(183.5662, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.5662384033203\n",
      "tensor(186.0309, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.03094482421875\n",
      "tensor(180.5508, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.55078125\n",
      "tensor(184.6789, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.6789093017578\n",
      "tensor(192.6535, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.65347290039062\n",
      "tensor(194.3947, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.39468383789062\n",
      "tensor(180.3535, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.35345458984375\n",
      "tensor(187.4848, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.48477172851562\n",
      "tensor(187.6941, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.69412231445312\n",
      "tensor(178.4069, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.40689086914062\n",
      "tensor(186.8712, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.8711700439453\n",
      "tensor(183.0212, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.021240234375\n",
      "tensor(179.7024, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.7024383544922\n",
      "tensor(187.2110, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.21104431152344\n",
      "tensor(181.8500, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.85003662109375\n",
      "tensor(184.4039, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.40390014648438\n",
      "tensor(183.2204, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.22039794921875\n",
      "tensor(187.7277, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.7276611328125\n",
      "tensor(193.2544, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.2544403076172\n",
      "tensor(185.7385, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.73846435546875\n",
      "tensor(179.0779, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.07794189453125\n",
      "tensor(191.1502, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.15020751953125\n",
      "tensor(191.8422, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.84219360351562\n",
      "tensor(188.4285, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.428466796875\n",
      "tensor(184.2118, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.21176147460938\n",
      "tensor(185.1756, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.17556762695312\n",
      "tensor(181.2435, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.24349975585938\n",
      "tensor(193.5081, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.50814819335938\n",
      "tensor(187.0435, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.04354858398438\n",
      "tensor(180.2315, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.23147583007812\n",
      "tensor(187.3816, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.3816375732422\n",
      "tensor(185.5624, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.56239318847656\n",
      "tensor(183.5549, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.5548553466797\n",
      "tensor(179.7239, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.72386169433594\n",
      "tensor(191.8283, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.82833862304688\n",
      "tensor(191.8168, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.81683349609375\n",
      "tensor(185.4517, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.45172119140625\n",
      "tensor(184.6176, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.61761474609375\n",
      "tensor(185.2831, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.2830810546875\n",
      "tensor(193.3658, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.36582946777344\n",
      "tensor(197.9593, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "197.95932006835938\n",
      "tensor(194.1941, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.194091796875\n",
      "tensor(183.4756, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.47555541992188\n",
      "tensor(191.5777, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.5777130126953\n",
      "tensor(188.6583, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.6583251953125\n",
      "tensor(186.3710, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.3709716796875\n",
      "tensor(190.0166, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.01663208007812\n",
      "tensor(190.3375, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.33749389648438\n",
      "tensor(188.1733, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.1732635498047\n",
      "tensor(179.8184, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.818359375\n",
      "tensor(181.5853, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.58526611328125\n",
      "tensor(181.2647, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.26473999023438\n",
      "tensor(196.1540, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "196.15396118164062\n",
      "tensor(183.2623, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.26234436035156\n",
      "tensor(181.1203, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.12034606933594\n",
      "tensor(180.9273, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.92727661132812\n",
      "tensor(180.8105, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.81051635742188\n",
      "tensor(185.4808, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.4808349609375\n",
      "tensor(180.7625, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.76248168945312\n",
      "tensor(175.8632, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.8632354736328\n",
      "tensor(184.2126, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.212646484375\n",
      "tensor(188.3157, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.31573486328125\n",
      "tensor(190.0596, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.05958557128906\n",
      "tensor(179.2664, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.26638793945312\n",
      "tensor(183.8872, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.88723754882812\n",
      "tensor(183.4684, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.46839904785156\n",
      "tensor(179.6560, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.65602111816406\n",
      "tensor(179.1071, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.1070556640625\n",
      "tensor(185.5156, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.51559448242188\n",
      "tensor(186.4872, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.48721313476562\n",
      "tensor(198.9074, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "198.90744018554688\n",
      "tensor(188.9193, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.91925048828125\n",
      "tensor(183.1429, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.14285278320312\n",
      "tensor(183.8147, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.81466674804688\n",
      "tensor(182.6377, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.63772583007812\n",
      "tensor(186.6885, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.6884765625\n",
      "tensor(182.9949, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.99490356445312\n",
      "tensor(179.2299, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.22988891601562\n",
      "tensor(183.1693, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.1692657470703\n",
      "tensor(178.3774, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.37742614746094\n",
      "tensor(191.5306, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.5306396484375\n",
      "tensor(175.1455, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.14547729492188\n",
      "tensor(184.4996, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.49960327148438\n",
      "tensor(192.2224, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.222412109375\n",
      "tensor(186.6169, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.616943359375\n",
      "tensor(184.9898, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.98983764648438\n",
      "tensor(195.3287, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "195.32872009277344\n",
      "tensor(191.1494, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.14944458007812\n",
      "tensor(180.8158, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.8157501220703\n",
      "tensor(181.2868, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.28677368164062\n",
      "tensor(197.5495, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "197.54945373535156\n",
      "tensor(195.8656, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "195.86563110351562\n",
      "tensor(189.5249, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.5249481201172\n",
      "tensor(185.4942, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.49424743652344\n",
      "tensor(183.3764, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.37643432617188\n",
      "tensor(191.9168, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.91683959960938\n",
      "tensor(191.5080, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.50804138183594\n",
      "tensor(186.9056, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.9055633544922\n",
      "tensor(174.5321, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.5321044921875\n",
      "tensor(195.7542, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "195.7541961669922\n",
      "tensor(174.7431, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.74305725097656\n",
      "tensor(200.3130, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "200.31301879882812\n",
      "tensor(179.9616, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.9616241455078\n",
      "tensor(184.4098, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.40980529785156\n",
      "tensor(188.2486, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.24864196777344\n",
      "tensor(185.4972, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.49720764160156\n",
      "tensor(188.6806, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.68057250976562\n",
      "tensor(188.2877, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.2877197265625\n",
      "tensor(181.9366, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.93658447265625\n",
      "tensor(194.3261, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.32608032226562\n",
      "tensor(194.5067, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.50668334960938\n",
      "tensor(190.9571, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.95706176757812\n",
      "tensor(189.3587, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.35873413085938\n",
      "tensor(183.4635, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.4634552001953\n",
      "tensor(183.9635, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.96353149414062\n",
      "tensor(182.8098, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.80984497070312\n",
      "tensor(190.0880, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.0879669189453\n",
      "tensor(187.9739, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.97393798828125\n",
      "tensor(188.7755, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.77545166015625\n",
      "tensor(180.3418, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.34182739257812\n",
      "tensor(188.0901, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.09014892578125\n",
      "tensor(179.4728, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.4728240966797\n",
      "tensor(187.5359, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.53587341308594\n",
      "tensor(183.2808, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.28076171875\n",
      "tensor(185.5240, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.5240478515625\n",
      "tensor(181.0831, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.08306884765625\n",
      "tensor(181.0042, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.00418090820312\n",
      "tensor(177.6085, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.60845947265625\n",
      "tensor(179.4129, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.41287231445312\n",
      "tensor(184.9067, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.90670776367188\n",
      "tensor(182.9064, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.90643310546875\n",
      "tensor(186.9471, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.94708251953125\n",
      "tensor(207.5709, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "207.57090759277344\n",
      "tensor(181.1978, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.19775390625\n",
      "tensor(179.1165, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.116455078125\n",
      "tensor(184.5232, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.523193359375\n",
      "tensor(181.7301, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.73013305664062\n",
      "tensor(185.0099, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.0098876953125\n",
      "tensor(186.7021, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.7021484375\n",
      "tensor(178.6734, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.67337036132812\n",
      "tensor(179.6548, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.65476989746094\n",
      "tensor(177.8200, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.81997680664062\n",
      "tensor(196.3266, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "196.3266143798828\n",
      "tensor(180.9711, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.97109985351562\n",
      "tensor(182.0421, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.0421142578125\n",
      "tensor(187.5563, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.55633544921875\n",
      "tensor(181.7888, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.7887725830078\n",
      "tensor(183.6201, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.62005615234375\n",
      "tensor(183.4353, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.43533325195312\n",
      "tensor(177.7795, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.779541015625\n",
      "tensor(183.8977, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.8976593017578\n",
      "tensor(201.0189, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "201.01893615722656\n",
      "tensor(201.4161, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "201.4161376953125\n",
      "tensor(183.8278, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.82778930664062\n",
      "tensor(192.0934, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.09344482421875\n",
      "tensor(185.3080, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.3079833984375\n",
      "tensor(176.6716, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.67160034179688\n",
      "tensor(192.9934, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.99339294433594\n",
      "tensor(182.7480, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.748046875\n",
      "tensor(182.7189, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.71893310546875\n",
      "tensor(183.0915, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.09146118164062\n",
      "tensor(186.9877, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.9877471923828\n",
      "tensor(181.9426, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.94264221191406\n",
      "tensor(188.2700, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.26995849609375\n",
      "tensor(180.3452, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.3451690673828\n",
      "tensor(189.4086, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.4086151123047\n",
      "tensor(178.6467, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.64669799804688\n",
      "tensor(184.6797, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.6797332763672\n",
      "tensor(183.2849, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.28485107421875\n",
      "tensor(183.4965, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.49649047851562\n",
      "tensor(179.2348, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.23477172851562\n",
      "tensor(184.6618, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.66180419921875\n",
      "tensor(193.1656, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.16561889648438\n",
      "tensor(179.2328, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.23275756835938\n",
      "tensor(185.4110, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.41104125976562\n",
      "tensor(185.9908, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.99081420898438\n",
      "tensor(183.1481, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.14810180664062\n",
      "tensor(183.9536, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.95361328125\n",
      "tensor(190.9123, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.91233825683594\n",
      "tensor(188.1440, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.14402770996094\n",
      "tensor(185.4890, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.4889678955078\n",
      "tensor(183.3303, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.33030700683594\n",
      "tensor(178.9805, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.98052978515625\n",
      "tensor(183.2777, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.27772521972656\n",
      "tensor(182.8667, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.86669921875\n",
      "tensor(192.6748, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.6748046875\n",
      "tensor(186.6142, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.61422729492188\n",
      "tensor(180.3068, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.30682373046875\n",
      "tensor(182.5496, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.549560546875\n",
      "tensor(184.4266, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.4266357421875\n",
      "tensor(184.6004, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.60043334960938\n",
      "tensor(184.7256, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.7255859375\n",
      "tensor(176.6819, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.68194580078125\n",
      "tensor(183.6899, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.68991088867188\n",
      "tensor(186.6994, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.69944763183594\n",
      "tensor(178.0666, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.06658935546875\n",
      "tensor(186.3604, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.36041259765625\n",
      "tensor(178.0962, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.09617614746094\n",
      "tensor(197.1232, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "197.12322998046875\n",
      "tensor(186.2998, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.29983520507812\n",
      "tensor(193.4160, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.41595458984375\n",
      "tensor(177.5202, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.52017211914062\n",
      "tensor(181.0313, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.03134155273438\n",
      "tensor(187.7399, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.73989868164062\n",
      "tensor(181.5200, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.52000427246094\n",
      "tensor(182.4008, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.4007568359375\n",
      "tensor(188.3087, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.30865478515625\n",
      "tensor(172.9563, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.956298828125\n",
      "tensor(176.3818, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.38177490234375\n",
      "tensor(182.9659, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.96592712402344\n",
      "tensor(182.5158, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.5157928466797\n",
      "tensor(190.3788, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.3787841796875\n",
      "tensor(185.3819, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.38186645507812\n",
      "tensor(178.5744, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.57440185546875\n",
      "tensor(183.6756, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.67559814453125\n",
      "tensor(194.7485, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.74847412109375\n",
      "tensor(178.2036, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.20364379882812\n",
      "tensor(182.4297, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.42971801757812\n",
      "tensor(173.6344, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.6343994140625\n",
      "tensor(186.8683, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.86831665039062\n",
      "tensor(176.6442, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.6442413330078\n",
      "tensor(186.9814, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.9814453125\n",
      "tensor(174.7154, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.71536254882812\n",
      "tensor(184.0683, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.06832885742188\n",
      "tensor(180.9915, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.99148559570312\n",
      "tensor(179.1502, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.15017700195312\n",
      "tensor(176.9369, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.93690490722656\n",
      "tensor(176.3952, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.39520263671875\n",
      "tensor(186.0696, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.06961059570312\n",
      "tensor(189.3931, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.39308166503906\n",
      "tensor(181.8004, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.80043029785156\n",
      "tensor(183.1613, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.1612548828125\n",
      "tensor(187.9415, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.9414520263672\n",
      "tensor(178.2716, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.27157592773438\n",
      "tensor(173.7653, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.76531982421875\n",
      "tensor(182.1325, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.13253784179688\n",
      "tensor(179.1777, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.17771911621094\n",
      "tensor(171.7292, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.72921752929688\n",
      "tensor(176.7021, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.70205688476562\n",
      "tensor(190.6303, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.63034057617188\n",
      "tensor(171.2410, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.24102783203125\n",
      "tensor(183.7813, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.78131103515625\n",
      "tensor(178.1837, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.18368530273438\n",
      "tensor(190.3709, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.37094116210938\n",
      "tensor(175.7886, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.78863525390625\n",
      "tensor(178.8736, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.87356567382812\n",
      "tensor(185.5727, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.57272338867188\n",
      "tensor(174.9201, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.92007446289062\n",
      "tensor(189.4722, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.47216796875\n",
      "tensor(171.6102, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.61021423339844\n",
      "tensor(201.1703, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "201.17031860351562\n",
      "tensor(194.4646, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.4645538330078\n",
      "tensor(189.7798, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.77978515625\n",
      "tensor(180.7737, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.773681640625\n",
      "tensor(177.8807, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.8806610107422\n",
      "tensor(179.2519, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.2519073486328\n",
      "tensor(175.5014, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.5014190673828\n",
      "tensor(178.2649, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.264892578125\n",
      "tensor(177.9936, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.99362182617188\n",
      "tensor(192.3495, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.34954833984375\n",
      "tensor(172.7662, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.7662353515625\n",
      "tensor(185.0679, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.06790161132812\n",
      "tensor(183.3226, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.32257080078125\n",
      "tensor(185.5329, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.53286743164062\n",
      "tensor(178.3477, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.34765625\n",
      "tensor(208.0510, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "208.051025390625\n",
      "tensor(211.0063, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "211.00625610351562\n",
      "tensor(179.2840, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.2839813232422\n",
      "tensor(181.0586, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.05859375\n",
      "tensor(183.9272, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.92718505859375\n",
      "tensor(185.3322, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.33218383789062\n",
      "tensor(191.5083, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.50828552246094\n",
      "tensor(183.3539, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.3538818359375\n",
      "tensor(192.8497, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.84967041015625\n",
      "tensor(191.5605, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.56045532226562\n",
      "tensor(191.3443, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.34432983398438\n",
      "tensor(189.4397, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.43972778320312\n",
      "tensor(184.9667, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.96673583984375\n",
      "tensor(183.8145, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.81454467773438\n",
      "tensor(184.7552, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.7552490234375\n",
      "tensor(182.8231, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.82310485839844\n",
      "tensor(179.0403, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.04025268554688\n",
      "tensor(178.9079, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.90792846679688\n",
      "tensor(183.6355, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.635498046875\n",
      "tensor(174.1633, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.163330078125\n",
      "tensor(184.8080, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.80804443359375\n",
      "tensor(195.5012, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "195.501220703125\n",
      "tensor(183.9670, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.967041015625\n",
      "tensor(179.4510, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.4510498046875\n",
      "tensor(177.8303, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.83026123046875\n",
      "tensor(180.6881, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.68809509277344\n",
      "tensor(181.0119, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.01187133789062\n",
      "tensor(176.0003, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.00027465820312\n",
      "tensor(173.7350, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.73495483398438\n",
      "tensor(181.6807, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.68069458007812\n",
      "tensor(180.2359, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.23590087890625\n",
      "tensor(176.5849, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.58494567871094\n",
      "tensor(177.1395, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.13946533203125\n",
      "tensor(191.1591, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.1590576171875\n",
      "tensor(177.9224, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.92236328125\n",
      "tensor(176.7560, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.75595092773438\n",
      "tensor(183.8767, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.87672424316406\n",
      "tensor(178.9083, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.90830993652344\n",
      "tensor(182.5152, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.51516723632812\n",
      "tensor(185.0254, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.025390625\n",
      "tensor(184.2885, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.28848266601562\n",
      "tensor(198.3928, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "198.392822265625\n",
      "tensor(180.8400, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.8400421142578\n",
      "tensor(184.5839, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.5838623046875\n",
      "tensor(186.5860, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.58596801757812\n",
      "tensor(174.7912, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.79116821289062\n",
      "tensor(176.1675, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.16754150390625\n",
      "tensor(186.8623, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.86231994628906\n",
      "tensor(177.2456, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.24557495117188\n",
      "tensor(181.6910, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.69097900390625\n",
      "tensor(189.1679, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.16790771484375\n",
      "tensor(179.3777, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.3777313232422\n",
      "tensor(177.8857, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.88565063476562\n",
      "tensor(185.0512, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.05123901367188\n",
      "tensor(179.9974, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.99740600585938\n",
      "tensor(178.4510, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.45101928710938\n",
      "tensor(176.4748, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.47482299804688\n",
      "tensor(177.7109, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.71090698242188\n",
      "tensor(187.5158, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.5157928466797\n",
      "tensor(192.6893, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.68930053710938\n",
      "tensor(186.5580, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.5579833984375\n",
      "tensor(183.9724, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.97244262695312\n",
      "tensor(175.9937, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.99374389648438\n",
      "tensor(185.0915, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.09152221679688\n",
      "tensor(180.5604, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.56039428710938\n",
      "tensor(176.8500, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.84999084472656\n",
      "tensor(182.9653, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.96534729003906\n",
      "tensor(176.6307, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.63070678710938\n",
      "tensor(185.1764, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.17637634277344\n",
      "tensor(190.8691, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.869140625\n",
      "tensor(179.5580, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.5580291748047\n",
      "tensor(177.5090, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.50897216796875\n",
      "tensor(179.5650, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.56500244140625\n",
      "tensor(192.9316, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.9315948486328\n",
      "tensor(182.7737, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.7737274169922\n",
      "tensor(190.6030, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.6029815673828\n",
      "tensor(184.1293, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.1293182373047\n",
      "tensor(170.3642, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.36422729492188\n",
      "tensor(189.3337, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.333740234375\n",
      "tensor(182.7684, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.7684326171875\n",
      "tensor(180.1870, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.18704223632812\n",
      "tensor(177.7050, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.70498657226562\n",
      "tensor(177.5059, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.50588989257812\n",
      "tensor(181.5287, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.52874755859375\n",
      "tensor(175.2126, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.21261596679688\n",
      "tensor(189.2939, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.2939453125\n",
      "tensor(179.2820, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.28201293945312\n",
      "tensor(191.5689, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.5689239501953\n",
      "tensor(174.7420, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.74203491210938\n",
      "tensor(174.0033, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.00328063964844\n",
      "tensor(177.3685, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.36846923828125\n",
      "tensor(176.6674, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.66741943359375\n",
      "tensor(180.7750, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.7750244140625\n",
      "tensor(178.1480, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.14804077148438\n",
      "tensor(173.3079, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.30789184570312\n",
      "tensor(178.1213, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.121337890625\n",
      "tensor(192.7751, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.775146484375\n",
      "tensor(183.0428, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.04278564453125\n",
      "tensor(192.1846, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.18463134765625\n",
      "tensor(183.2841, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.2841033935547\n",
      "tensor(176.0098, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.009765625\n",
      "tensor(175.4413, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.4412841796875\n",
      "tensor(186.3793, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.3792724609375\n",
      "tensor(181.5813, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.58126831054688\n",
      "tensor(181.1984, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.19842529296875\n",
      "tensor(178.4005, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.4005126953125\n",
      "tensor(177.1513, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.15133666992188\n",
      "tensor(170.0145, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.0144805908203\n",
      "tensor(183.7251, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.7251434326172\n",
      "tensor(188.5608, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.56076049804688\n",
      "tensor(179.4736, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.47357177734375\n",
      "tensor(171.2281, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.22805786132812\n",
      "tensor(175.4914, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.49143981933594\n",
      "tensor(182.3904, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.39036560058594\n",
      "tensor(179.6573, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.6573028564453\n",
      "tensor(170.6917, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.691650390625\n",
      "tensor(193.4506, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.4505615234375\n",
      "tensor(175.7134, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.71337890625\n",
      "tensor(178.4713, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.47132873535156\n",
      "tensor(193.2759, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.27593994140625\n",
      "tensor(183.4738, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.4738006591797\n",
      "tensor(177.6909, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.69085693359375\n",
      "tensor(181.4673, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.46730041503906\n",
      "tensor(173.5431, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.5430908203125\n",
      "tensor(188.5585, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.55850219726562\n",
      "tensor(181.5842, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.5841827392578\n",
      "tensor(172.0619, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.0618896484375\n",
      "tensor(190.6989, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.69894409179688\n",
      "tensor(182.4771, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.47705078125\n",
      "tensor(185.6791, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.6790771484375\n",
      "tensor(190.3268, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.32684326171875\n",
      "tensor(178.8606, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.860595703125\n",
      "tensor(175.6126, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.61256408691406\n",
      "tensor(182.3688, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.36880493164062\n",
      "tensor(183.5161, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.51608276367188\n",
      "tensor(186.1110, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.11099243164062\n",
      "tensor(191.4398, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.43975830078125\n",
      "tensor(186.7261, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.72610473632812\n",
      "tensor(187.6422, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.64218139648438\n",
      "tensor(181.9510, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.95098876953125\n",
      "tensor(184.1801, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.1800537109375\n",
      "tensor(182.8644, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.86444091796875\n",
      "tensor(184.8141, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.8140869140625\n",
      "tensor(171.6612, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.66116333007812\n",
      "tensor(178.3624, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.3623809814453\n",
      "tensor(174.0078, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.00784301757812\n",
      "tensor(183.2437, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.24365234375\n",
      "tensor(173.5696, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.56961059570312\n",
      "tensor(169.4913, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.4913330078125\n",
      "tensor(179.1479, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.14794921875\n",
      "tensor(170.6364, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.63641357421875\n",
      "tensor(179.2629, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.26290893554688\n",
      "tensor(177.3568, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.35684204101562\n",
      "tensor(174.5215, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.521484375\n",
      "tensor(188.4603, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.46031188964844\n",
      "tensor(179.7887, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.78866577148438\n",
      "tensor(182.5833, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.58334350585938\n",
      "tensor(181.2280, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.22804260253906\n",
      "tensor(199.4959, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "199.49594116210938\n",
      "tensor(179.4073, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.40733337402344\n",
      "tensor(171.3091, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.30908203125\n",
      "tensor(185.5898, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.58984375\n",
      "tensor(187.7458, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.7458038330078\n",
      "tensor(174.7110, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.71099853515625\n",
      "tensor(177.4298, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.4298095703125\n",
      "tensor(183.1718, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.17178344726562\n",
      "tensor(177.6488, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.6487579345703\n",
      "tensor(181.4728, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.47280883789062\n",
      "tensor(191.9256, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.9255828857422\n",
      "tensor(185.3690, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.3690185546875\n",
      "tensor(177.4598, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.45977783203125\n",
      "tensor(183.1471, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.14712524414062\n",
      "tensor(182.5933, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.59327697753906\n",
      "tensor(175.3537, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.35366821289062\n",
      "tensor(182.2592, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.25918579101562\n",
      "tensor(179.6816, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.68157958984375\n",
      "tensor(178.1104, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.11044311523438\n",
      "tensor(191.3722, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.37216186523438\n",
      "tensor(181.2226, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.22264099121094\n",
      "tensor(177.1506, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.15057373046875\n",
      "tensor(185.6289, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.62893676757812\n",
      "tensor(173.3873, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.38729858398438\n",
      "tensor(174.9962, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.99620056152344\n",
      "tensor(175.6599, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.65988159179688\n",
      "tensor(182.1633, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.163330078125\n",
      "tensor(184.2301, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.23013305664062\n",
      "tensor(178.5576, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.55764770507812\n",
      "tensor(186.4384, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.4384307861328\n",
      "tensor(169.5288, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.52883911132812\n",
      "tensor(184.4786, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.47857666015625\n",
      "tensor(178.7590, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.7589874267578\n",
      "tensor(173.6795, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.67947387695312\n",
      "tensor(176.8146, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.81463623046875\n",
      "tensor(183.8252, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.8251953125\n",
      "tensor(173.2246, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.2245635986328\n",
      "tensor(174.4092, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.40921020507812\n",
      "tensor(176.1744, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.17440795898438\n",
      "tensor(179.3556, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.3555908203125\n",
      "tensor(176.2566, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.256591796875\n",
      "tensor(180.8553, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.85528564453125\n",
      "tensor(177.0442, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.04415893554688\n",
      "tensor(182.2623, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.26229858398438\n",
      "tensor(179.5271, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.527099609375\n",
      "tensor(174.2094, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.20938110351562\n",
      "tensor(175.2723, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.2722625732422\n",
      "tensor(171.7146, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.71463012695312\n",
      "tensor(177.5785, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.5784912109375\n",
      "tensor(179.2517, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.2516632080078\n",
      "tensor(187.9353, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.93531799316406\n",
      "tensor(199.1204, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "199.1204376220703\n",
      "tensor(184.9810, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.98098754882812\n",
      "tensor(172.1398, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.13975524902344\n",
      "tensor(187.5374, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.5373992919922\n",
      "tensor(186.7767, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.77670288085938\n",
      "tensor(176.7332, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.733154296875\n",
      "tensor(179.3224, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.32240295410156\n",
      "tensor(177.6228, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.62281799316406\n",
      "tensor(176.7605, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.7605438232422\n",
      "tensor(171.4672, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.46719360351562\n",
      "tensor(176.3020, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.302001953125\n",
      "tensor(177.9753, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.97528076171875\n",
      "tensor(182.4651, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.46514892578125\n",
      "tensor(177.0387, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.03868103027344\n",
      "tensor(193.0651, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.0651092529297\n",
      "tensor(187.7990, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.7989501953125\n",
      "tensor(200.5221, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "200.5220947265625\n",
      "tensor(183.0443, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.04434204101562\n",
      "tensor(188.3439, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.34393310546875\n",
      "tensor(173.9727, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.97271728515625\n",
      "tensor(176.2036, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.20359802246094\n",
      "tensor(178.3279, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.32791137695312\n",
      "tensor(182.2417, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.24169921875\n",
      "tensor(186.0211, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.02113342285156\n",
      "tensor(185.9050, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.90499877929688\n",
      "tensor(178.7981, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.79808044433594\n",
      "tensor(180.9754, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.97544860839844\n",
      "tensor(181.9227, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.92266845703125\n",
      "tensor(180.7264, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.7264404296875\n",
      "tensor(176.0994, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.09942626953125\n",
      "tensor(178.2001, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.20010375976562\n",
      "tensor(181.7312, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.731201171875\n",
      "tensor(188.4414, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.4413604736328\n",
      "tensor(171.8330, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.83299255371094\n",
      "tensor(192.7768, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.77679443359375\n",
      "tensor(169.1861, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.18609619140625\n",
      "tensor(176.5148, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.51480102539062\n",
      "tensor(175.6689, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.6689453125\n",
      "tensor(181.5306, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.53057861328125\n",
      "tensor(192.5985, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.59848022460938\n",
      "tensor(180.1353, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.13525390625\n",
      "tensor(178.4480, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.44796752929688\n",
      "tensor(181.5117, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.51174926757812\n",
      "tensor(182.3675, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.3675079345703\n",
      "tensor(186.4903, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.4902801513672\n",
      "tensor(187.9037, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.9036865234375\n",
      "tensor(168.5014, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.5014190673828\n",
      "tensor(171.6168, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.61679077148438\n",
      "tensor(178.9105, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.91046142578125\n",
      "tensor(185.4462, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.44619750976562\n",
      "tensor(182.9976, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.99757385253906\n",
      "tensor(181.2026, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.20260620117188\n",
      "tensor(189.7149, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.71487426757812\n",
      "tensor(180.0868, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.08680725097656\n",
      "tensor(183.1236, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.12364196777344\n",
      "tensor(180.9545, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.95448303222656\n",
      "tensor(171.5739, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.5738983154297\n",
      "tensor(175.9071, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.90711975097656\n",
      "tensor(188.7958, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.7957763671875\n",
      "tensor(185.8640, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.86398315429688\n",
      "tensor(189.7148, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.71475219726562\n",
      "tensor(174.7243, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.72430419921875\n",
      "tensor(168.8021, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.8021240234375\n",
      "tensor(181.3432, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.34317016601562\n",
      "tensor(177.3567, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.3567352294922\n",
      "tensor(176.3778, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.37779235839844\n",
      "tensor(175.7064, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.70639038085938\n",
      "tensor(173.3122, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.31216430664062\n",
      "tensor(180.2499, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.24993896484375\n",
      "tensor(168.9872, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.98715209960938\n",
      "tensor(190.4379, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.4378662109375\n",
      "tensor(179.4586, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.4586181640625\n",
      "tensor(172.0350, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.0349578857422\n",
      "tensor(178.9997, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.99972534179688\n",
      "tensor(175.7017, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.70166015625\n",
      "tensor(174.3108, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.3108367919922\n",
      "tensor(179.4759, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.47589111328125\n",
      "tensor(176.7700, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.77001953125\n",
      "tensor(180.5459, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.5459442138672\n",
      "tensor(177.8974, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.89743041992188\n",
      "tensor(185.1703, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.17025756835938\n",
      "tensor(169.6882, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.68817138671875\n",
      "tensor(183.0610, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.06100463867188\n",
      "tensor(179.2817, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.28172302246094\n",
      "tensor(170.9649, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.96493530273438\n",
      "tensor(187.5668, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.5668487548828\n",
      "tensor(173.7923, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.79226684570312\n",
      "tensor(182.3847, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.38470458984375\n",
      "tensor(174.2693, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.26931762695312\n",
      "tensor(180.9315, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.9314727783203\n",
      "tensor(179.8132, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.813232421875\n",
      "tensor(180.8670, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.8670196533203\n",
      "tensor(193.2063, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "193.20626831054688\n",
      "tensor(178.6082, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.6082000732422\n",
      "tensor(180.1741, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.17410278320312\n",
      "tensor(173.2769, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.27687072753906\n",
      "tensor(177.7624, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.76242065429688\n",
      "tensor(175.6536, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.653564453125\n",
      "tensor(181.3630, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.36297607421875\n",
      "tensor(177.1049, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.10488891601562\n",
      "tensor(170.3866, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.38662719726562\n",
      "tensor(176.6454, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.64544677734375\n",
      "tensor(171.2776, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.27755737304688\n",
      "tensor(173.9055, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.90548706054688\n",
      "tensor(175.0307, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.03070068359375\n",
      "tensor(188.8082, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.8081512451172\n",
      "tensor(172.9307, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.93072509765625\n",
      "tensor(168.3802, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.38015747070312\n",
      "tensor(175.6545, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.65447998046875\n",
      "tensor(164.5390, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.53900146484375\n",
      "tensor(178.6704, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.67041015625\n",
      "tensor(168.0601, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.06011962890625\n",
      "tensor(174.6193, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.61932373046875\n",
      "tensor(176.3426, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.34262084960938\n",
      "tensor(183.0683, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.06826782226562\n",
      "tensor(180.2777, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.27774047851562\n",
      "tensor(177.7741, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.7741241455078\n",
      "tensor(166.1261, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.12606811523438\n",
      "tensor(178.7359, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.7359161376953\n",
      "tensor(194.8471, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "194.84713745117188\n",
      "tensor(184.7681, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.76806640625\n",
      "tensor(178.3227, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.3226776123047\n",
      "tensor(185.0540, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.053955078125\n",
      "tensor(179.3415, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.34153747558594\n",
      "tensor(182.2585, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.258544921875\n",
      "tensor(173.3978, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.39779663085938\n",
      "tensor(169.1719, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.17190551757812\n",
      "tensor(201.3603, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "201.36026000976562\n",
      "tensor(167.0543, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.0543212890625\n",
      "tensor(170.0428, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.0428466796875\n",
      "tensor(172.8799, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.87991333007812\n",
      "tensor(172.4499, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.44992065429688\n",
      "tensor(180.4214, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.4214324951172\n",
      "tensor(168.0860, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.08599853515625\n",
      "tensor(181.0824, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.0823974609375\n",
      "tensor(177.6258, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.62583923339844\n",
      "tensor(172.5568, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.55679321289062\n",
      "tensor(188.7005, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.70050048828125\n",
      "tensor(179.9935, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.9934844970703\n",
      "tensor(170.2419, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.24188232421875\n",
      "tensor(186.6494, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.64938354492188\n",
      "tensor(181.1552, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.1552276611328\n",
      "tensor(178.9428, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.94277954101562\n",
      "tensor(175.6386, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.63864135742188\n",
      "tensor(172.1879, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.18792724609375\n",
      "tensor(163.9871, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.9871063232422\n",
      "tensor(239.7447, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "239.74465942382812\n",
      "tensor(169.0286, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.0286407470703\n",
      "tensor(175.4135, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.4134521484375\n",
      "tensor(183.5179, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.51785278320312\n",
      "tensor(188.0329, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.0328826904297\n",
      "tensor(181.3651, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.36505126953125\n",
      "tensor(184.5470, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.5470428466797\n",
      "tensor(173.6679, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.66793823242188\n",
      "tensor(175.3286, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.32862854003906\n",
      "tensor(173.6688, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.6688232421875\n",
      "tensor(176.3242, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.32415771484375\n",
      "tensor(173.7124, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.71243286132812\n",
      "tensor(170.5363, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.53631591796875\n",
      "tensor(185.6805, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.68045043945312\n",
      "tensor(169.2162, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.21615600585938\n",
      "tensor(176.5161, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.51605224609375\n",
      "tensor(175.7248, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.72479248046875\n",
      "tensor(169.6650, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.6649932861328\n",
      "tensor(179.4900, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.48995971679688\n",
      "tensor(171.7933, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.7933349609375\n",
      "tensor(183.4525, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.45254516601562\n",
      "tensor(175.2279, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.2278594970703\n",
      "tensor(191.8624, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.8623809814453\n",
      "tensor(170.1230, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.12295532226562\n",
      "tensor(182.3007, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.3007354736328\n",
      "tensor(178.5173, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.517333984375\n",
      "tensor(169.7207, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.72067260742188\n",
      "tensor(182.5702, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.57020568847656\n",
      "tensor(174.1715, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.17149353027344\n",
      "tensor(191.3804, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.3804473876953\n",
      "tensor(173.9779, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.9779052734375\n",
      "tensor(179.9452, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.9451904296875\n",
      "tensor(173.9240, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.92398071289062\n",
      "tensor(181.7267, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.72674560546875\n",
      "tensor(175.3211, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.32107543945312\n",
      "tensor(173.7787, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.77874755859375\n",
      "tensor(176.8623, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.86227416992188\n",
      "tensor(189.1081, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.1081085205078\n",
      "tensor(173.8197, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.81967163085938\n",
      "tensor(169.1932, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.19322204589844\n",
      "tensor(174.2780, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.27804565429688\n",
      "tensor(172.5096, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.50958251953125\n",
      "tensor(171.7317, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.73171997070312\n",
      "tensor(173.8876, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.88755798339844\n",
      "tensor(179.3044, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.30438232421875\n",
      "tensor(166.8822, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.88217163085938\n",
      "tensor(184.6600, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.6599578857422\n",
      "tensor(178.0906, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.09060668945312\n",
      "tensor(177.2533, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.2532501220703\n",
      "tensor(178.3686, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.36856079101562\n",
      "tensor(169.8181, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.818115234375\n",
      "tensor(169.8140, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.81399536132812\n",
      "tensor(183.4774, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.4774169921875\n",
      "tensor(178.0048, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.00479125976562\n",
      "tensor(169.1749, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.17489624023438\n",
      "tensor(180.3519, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.35191345214844\n",
      "tensor(172.0049, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.0048828125\n",
      "tensor(173.9999, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.99990844726562\n",
      "tensor(170.0645, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.064453125\n",
      "tensor(180.4767, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.47674560546875\n",
      "tensor(177.4281, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.42813110351562\n",
      "tensor(165.1111, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.11109924316406\n",
      "tensor(172.0544, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.05441284179688\n",
      "tensor(173.0715, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.07150268554688\n",
      "tensor(176.9534, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.953369140625\n",
      "tensor(185.5357, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.53567504882812\n",
      "tensor(188.6360, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.63601684570312\n",
      "tensor(184.4324, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.43235778808594\n",
      "tensor(176.9325, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.93247985839844\n",
      "tensor(182.3660, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.36599731445312\n",
      "tensor(191.8388, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.83877563476562\n",
      "tensor(173.8606, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.86056518554688\n",
      "tensor(179.8875, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.88754272460938\n",
      "tensor(167.3829, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.38290405273438\n",
      "tensor(176.3997, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.399658203125\n",
      "tensor(184.8895, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.8894500732422\n",
      "tensor(175.5041, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.50408935546875\n",
      "tensor(187.0861, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.0860595703125\n",
      "tensor(176.6197, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.61968994140625\n",
      "tensor(165.3274, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.32737731933594\n",
      "tensor(165.2048, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.20480346679688\n",
      "tensor(171.1799, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.179931640625\n",
      "tensor(178.4018, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.40182495117188\n",
      "tensor(169.8125, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.8125\n",
      "tensor(173.2894, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.28936767578125\n",
      "tensor(188.0990, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.0989532470703\n",
      "tensor(174.1496, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.14959716796875\n",
      "tensor(176.8562, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.85617065429688\n",
      "tensor(175.4873, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.48727416992188\n",
      "tensor(176.3271, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.32708740234375\n",
      "tensor(175.9845, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.98451232910156\n",
      "tensor(171.3957, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.39573669433594\n",
      "tensor(172.7644, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.764404296875\n",
      "tensor(180.4070, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.40696716308594\n",
      "tensor(165.3129, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.3128662109375\n",
      "tensor(166.4966, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.49658203125\n",
      "tensor(167.3028, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.30276489257812\n",
      "tensor(181.1388, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.13882446289062\n",
      "tensor(176.8979, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.89788818359375\n",
      "tensor(165.5881, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.58807373046875\n",
      "tensor(176.2812, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.28121948242188\n",
      "tensor(168.5371, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.53707885742188\n",
      "tensor(203.2248, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "203.2247772216797\n",
      "tensor(172.7225, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.72251892089844\n",
      "tensor(172.6841, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.68408203125\n",
      "tensor(172.1633, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.16329956054688\n",
      "tensor(182.5757, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.57569885253906\n",
      "tensor(182.3780, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.3779754638672\n",
      "tensor(181.9853, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.98529052734375\n",
      "tensor(203.9965, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "203.99652099609375\n",
      "tensor(178.5691, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.569091796875\n",
      "tensor(184.5439, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.54388427734375\n",
      "tensor(177.9023, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.90234375\n",
      "tensor(167.2236, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.22360229492188\n",
      "tensor(183.4581, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.4581298828125\n",
      "tensor(176.7746, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.77455139160156\n",
      "tensor(171.3045, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.30453491210938\n",
      "tensor(175.2244, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.22439575195312\n",
      "tensor(172.3147, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.31472778320312\n",
      "tensor(170.5119, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.5119171142578\n",
      "tensor(170.6206, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.62057495117188\n",
      "tensor(188.7520, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.7519989013672\n",
      "tensor(182.9634, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.9634246826172\n",
      "tensor(173.9904, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.99041748046875\n",
      "tensor(179.6501, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.65008544921875\n",
      "tensor(189.8669, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.866943359375\n",
      "tensor(176.5387, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.53871154785156\n",
      "tensor(178.2846, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.28463745117188\n",
      "tensor(169.7887, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.78872680664062\n",
      "tensor(169.9111, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.91107177734375\n",
      "tensor(177.5112, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.5111846923828\n",
      "tensor(179.6901, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.69009399414062\n",
      "tensor(173.8533, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.85325622558594\n",
      "tensor(172.4095, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.4095001220703\n",
      "tensor(168.7550, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.75497436523438\n",
      "tensor(170.9387, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.93869018554688\n",
      "tensor(173.7663, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.7662811279297\n",
      "tensor(170.5164, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.5164031982422\n",
      "tensor(189.4886, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.4886016845703\n",
      "tensor(170.6028, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.60275268554688\n",
      "tensor(175.2025, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.2025146484375\n",
      "tensor(168.0907, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.09068298339844\n",
      "tensor(180.0143, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.01429748535156\n",
      "tensor(174.6563, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.65634155273438\n",
      "tensor(173.9237, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.92367553710938\n",
      "tensor(178.2928, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.2928009033203\n",
      "tensor(184.0244, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.02438354492188\n",
      "tensor(179.1106, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.11062622070312\n",
      "tensor(173.5984, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.5984344482422\n",
      "tensor(190.7639, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.76388549804688\n",
      "tensor(181.3310, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.33096313476562\n",
      "tensor(177.6818, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.68182373046875\n",
      "tensor(167.6906, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.6905975341797\n",
      "tensor(176.3864, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.38644409179688\n",
      "tensor(175.3971, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.39712524414062\n",
      "tensor(180.0049, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.00491333007812\n",
      "tensor(178.7904, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.79037475585938\n",
      "tensor(177.8608, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.8607635498047\n",
      "tensor(174.4465, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.44651794433594\n",
      "tensor(196.0172, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "196.01724243164062\n",
      "tensor(174.4647, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.46470642089844\n",
      "tensor(178.2405, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.24050903320312\n",
      "tensor(173.1109, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.11090087890625\n",
      "tensor(172.0637, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.063720703125\n",
      "tensor(184.2517, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.251708984375\n",
      "tensor(171.8114, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.8114013671875\n",
      "tensor(180.7683, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.768310546875\n",
      "tensor(175.1690, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.1690216064453\n",
      "tensor(174.1862, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.1861572265625\n",
      "tensor(178.2235, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.22348022460938\n",
      "tensor(174.8270, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.8269805908203\n",
      "tensor(174.5426, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.54258728027344\n",
      "tensor(183.0158, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.01580810546875\n",
      "tensor(190.0397, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.03973388671875\n",
      "tensor(172.1983, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.1982879638672\n",
      "tensor(168.0404, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.0404052734375\n",
      "tensor(180.7825, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.78253173828125\n",
      "tensor(184.4533, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.45333862304688\n",
      "tensor(175.6461, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.64614868164062\n",
      "tensor(169.1310, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.13095092773438\n",
      "tensor(170.8335, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.8334503173828\n",
      "tensor(174.6345, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.63453674316406\n",
      "tensor(171.3325, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.33251953125\n",
      "tensor(172.3438, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.3438262939453\n",
      "tensor(170.1780, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.177978515625\n",
      "tensor(175.5356, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.5355987548828\n",
      "tensor(172.0611, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.06106567382812\n",
      "tensor(165.3230, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.3229522705078\n",
      "tensor(165.7894, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.78939819335938\n",
      "tensor(171.4249, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.4249267578125\n",
      "tensor(165.4028, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.40283203125\n",
      "tensor(173.2825, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.28245544433594\n",
      "tensor(180.7855, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.78546142578125\n",
      "tensor(191.9257, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.9256591796875\n",
      "tensor(170.7955, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.79550170898438\n",
      "tensor(176.6866, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.68658447265625\n",
      "tensor(189.4052, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.40524291992188\n",
      "tensor(176.0354, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.0354461669922\n",
      "tensor(172.4088, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.40884399414062\n",
      "tensor(170.9403, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.94032287597656\n",
      "tensor(176.2359, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.23590087890625\n",
      "tensor(185.4671, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.4670867919922\n",
      "tensor(180.5989, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.5989227294922\n",
      "tensor(176.9774, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.97744750976562\n",
      "tensor(175.8806, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.88063049316406\n",
      "tensor(174.2978, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.29776000976562\n",
      "tensor(176.1799, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.1798858642578\n",
      "tensor(182.5138, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.51380920410156\n",
      "tensor(167.7517, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.75173950195312\n",
      "tensor(165.7678, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.76780700683594\n",
      "tensor(177.3964, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.3963623046875\n",
      "tensor(179.0523, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.0522918701172\n",
      "tensor(177.0002, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.000244140625\n",
      "tensor(161.3106, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.31060791015625\n",
      "tensor(179.0090, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.009033203125\n",
      "tensor(172.5612, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.56124877929688\n",
      "tensor(170.6372, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.63722229003906\n",
      "tensor(185.3813, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.38134765625\n",
      "tensor(171.6250, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.6250457763672\n",
      "tensor(174.6799, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.6798858642578\n",
      "tensor(167.6138, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.61383056640625\n",
      "tensor(170.9924, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.99237060546875\n",
      "tensor(182.8282, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.82815551757812\n",
      "tensor(168.7549, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.75491333007812\n",
      "tensor(173.5907, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.59066772460938\n",
      "tensor(180.3618, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.36184692382812\n",
      "tensor(173.4746, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.47462463378906\n",
      "tensor(172.8542, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.85421752929688\n",
      "tensor(169.5888, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.58880615234375\n",
      "tensor(180.4821, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.48211669921875\n",
      "tensor(169.2099, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.20994567871094\n",
      "tensor(172.7933, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.7932586669922\n",
      "tensor(177.5712, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.5712432861328\n",
      "tensor(173.7993, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.79934692382812\n",
      "tensor(164.6970, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.69700622558594\n",
      "tensor(173.0216, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.02157592773438\n",
      "tensor(180.0167, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.01666259765625\n",
      "tensor(164.4163, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.41627502441406\n",
      "tensor(171.5292, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.5291748046875\n",
      "tensor(175.8390, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.8390350341797\n",
      "tensor(187.3433, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.3433380126953\n",
      "tensor(175.6005, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.60049438476562\n",
      "tensor(185.4963, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.49627685546875\n",
      "tensor(163.4187, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.41873168945312\n",
      "tensor(177.4163, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.41632080078125\n",
      "tensor(171.3397, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.33969116210938\n",
      "tensor(173.4348, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.43478393554688\n",
      "tensor(170.1650, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.16497802734375\n",
      "tensor(174.0908, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.09075927734375\n",
      "tensor(177.9740, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.97402954101562\n",
      "tensor(179.5577, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.55767822265625\n",
      "tensor(183.6165, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.61648559570312\n",
      "tensor(178.6267, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.62669372558594\n",
      "tensor(169.4537, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.45367431640625\n",
      "tensor(171.1416, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.14158630371094\n",
      "tensor(174.6744, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.67437744140625\n",
      "tensor(171.2879, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.28790283203125\n",
      "tensor(170.9636, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.96356201171875\n",
      "tensor(168.7679, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.76792907714844\n",
      "tensor(179.9904, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.99041748046875\n",
      "tensor(170.3266, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.32662963867188\n",
      "tensor(179.4527, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.4526824951172\n",
      "tensor(169.4399, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.43994140625\n",
      "tensor(171.0116, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.0115509033203\n",
      "tensor(180.9725, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.97250366210938\n",
      "tensor(172.7426, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.7425537109375\n",
      "tensor(176.4220, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.42202758789062\n",
      "tensor(169.0973, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.09732055664062\n",
      "tensor(169.2717, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.27166748046875\n",
      "tensor(167.7085, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.70846557617188\n",
      "tensor(174.9655, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.9654541015625\n",
      "tensor(163.2836, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.28363037109375\n",
      "tensor(163.3362, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.33624267578125\n",
      "tensor(170.5735, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.57345581054688\n",
      "tensor(173.1125, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.11248779296875\n",
      "tensor(167.8909, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.89089965820312\n",
      "tensor(177.1959, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.19586181640625\n",
      "tensor(176.5088, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.5087890625\n",
      "tensor(175.9940, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.99404907226562\n",
      "tensor(172.7540, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.75396728515625\n",
      "tensor(169.0930, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.09303283691406\n",
      "tensor(166.3603, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.3603057861328\n",
      "tensor(175.5983, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.5982666015625\n",
      "tensor(165.5835, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.5835418701172\n",
      "tensor(174.4231, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.42312622070312\n",
      "tensor(169.2655, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.2655029296875\n",
      "tensor(165.1946, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.19461059570312\n",
      "tensor(174.4079, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.40789794921875\n",
      "tensor(173.0203, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.020263671875\n",
      "tensor(176.2397, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.23973083496094\n",
      "tensor(167.7129, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.71286010742188\n",
      "tensor(161.8286, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.82862854003906\n",
      "tensor(241.6932, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "241.6932373046875\n",
      "tensor(172.8435, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.84347534179688\n",
      "tensor(168.5805, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.58053588867188\n",
      "tensor(173.7702, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.77017211914062\n",
      "tensor(176.9181, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.91806030273438\n",
      "tensor(180.8547, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.8546905517578\n",
      "tensor(181.6701, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.67013549804688\n",
      "tensor(183.1238, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.12380981445312\n",
      "tensor(179.8281, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.82809448242188\n",
      "tensor(179.1486, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.14862060546875\n",
      "tensor(170.0372, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.03721618652344\n",
      "tensor(176.1245, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.12451171875\n",
      "tensor(181.4679, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.46786499023438\n",
      "tensor(177.9402, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.94015502929688\n",
      "tensor(181.0094, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.0093994140625\n",
      "tensor(178.8133, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.81332397460938\n",
      "tensor(188.2964, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.29638671875\n",
      "tensor(171.6407, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.64065551757812\n",
      "tensor(171.0337, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.03367614746094\n",
      "tensor(176.4980, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.498046875\n",
      "tensor(179.6480, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.64804077148438\n",
      "tensor(174.8359, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.83590698242188\n",
      "tensor(179.6066, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.60658264160156\n",
      "tensor(173.3524, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.35235595703125\n",
      "tensor(177.2379, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.23788452148438\n",
      "tensor(166.7488, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.74880981445312\n",
      "tensor(175.5262, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.52621459960938\n",
      "tensor(179.4886, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.4886016845703\n",
      "tensor(163.5406, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.54055786132812\n",
      "tensor(172.4966, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.49661254882812\n",
      "tensor(186.5825, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.58253479003906\n",
      "tensor(179.6860, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.68597412109375\n",
      "tensor(172.8863, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.8863067626953\n",
      "tensor(169.5756, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.5756378173828\n",
      "tensor(179.8978, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.8978271484375\n",
      "tensor(177.8261, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.82606506347656\n",
      "tensor(188.0308, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.03077697753906\n",
      "tensor(176.5818, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.58175659179688\n",
      "tensor(165.0547, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.0546875\n",
      "tensor(177.1596, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.15956115722656\n",
      "tensor(179.4508, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.45083618164062\n",
      "tensor(174.2029, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.20286560058594\n",
      "tensor(170.4268, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.42684936523438\n",
      "tensor(170.3187, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.31866455078125\n",
      "tensor(178.2622, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.26223754882812\n",
      "tensor(164.5714, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.57135009765625\n",
      "tensor(170.8521, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.85208129882812\n",
      "tensor(180.8072, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.80715942382812\n",
      "tensor(167.1777, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.177734375\n",
      "tensor(164.2461, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.24606323242188\n",
      "tensor(162.7628, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.76278686523438\n",
      "tensor(163.4902, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.490234375\n",
      "tensor(177.1949, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.1948699951172\n",
      "tensor(170.6822, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.68215942382812\n",
      "tensor(171.3371, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.33706665039062\n",
      "tensor(164.4163, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.41632080078125\n",
      "tensor(162.3150, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.3150177001953\n",
      "tensor(178.1447, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.14474487304688\n",
      "tensor(172.7955, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.79551696777344\n",
      "tensor(170.8641, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.8640594482422\n",
      "tensor(167.2517, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.2516632080078\n",
      "tensor(176.9313, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.93133544921875\n",
      "tensor(185.7743, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.7742919921875\n",
      "tensor(165.7053, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.70533752441406\n",
      "tensor(169.0280, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.0279541015625\n",
      "tensor(168.0590, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.0589599609375\n",
      "tensor(162.0680, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.06796264648438\n",
      "tensor(185.5958, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.5958251953125\n",
      "tensor(165.9190, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.91897583007812\n",
      "tensor(167.5303, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.5303497314453\n",
      "tensor(171.5458, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.54579162597656\n",
      "tensor(172.6641, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.66412353515625\n",
      "tensor(172.2776, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.27757263183594\n",
      "tensor(163.4113, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.41127014160156\n",
      "tensor(175.1474, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.1473846435547\n",
      "tensor(169.9891, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.98910522460938\n",
      "tensor(172.8302, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.8302001953125\n",
      "tensor(160.9795, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.9795379638672\n",
      "tensor(171.8371, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.83705139160156\n",
      "tensor(183.4572, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.45718383789062\n",
      "tensor(167.8035, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.80352783203125\n",
      "tensor(184.1126, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.11260986328125\n",
      "tensor(166.1104, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.11038208007812\n",
      "tensor(168.9717, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.9716796875\n",
      "tensor(169.6658, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.6658477783203\n",
      "tensor(175.3219, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.32188415527344\n",
      "tensor(173.6138, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.61376953125\n",
      "tensor(166.0296, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.02955627441406\n",
      "tensor(169.6968, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.69677734375\n",
      "tensor(164.6401, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.64013671875\n",
      "tensor(163.6248, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.62484741210938\n",
      "tensor(174.8246, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.82456970214844\n",
      "tensor(164.2185, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.21853637695312\n",
      "tensor(162.3362, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.33615112304688\n",
      "tensor(164.3532, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.35321044921875\n",
      "tensor(176.5706, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.570556640625\n",
      "tensor(165.9609, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.96090698242188\n",
      "tensor(162.6029, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.60289001464844\n",
      "tensor(170.4634, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.46336364746094\n",
      "tensor(175.4103, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.41033935546875\n",
      "tensor(174.2209, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.22085571289062\n",
      "tensor(175.2313, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.23133850097656\n",
      "tensor(172.7444, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.74435424804688\n",
      "tensor(171.4890, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.48904418945312\n",
      "tensor(168.0292, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.02923583984375\n",
      "tensor(169.3797, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.37969970703125\n",
      "tensor(170.3621, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.36212158203125\n",
      "tensor(163.3853, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.3853302001953\n",
      "tensor(174.9352, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.93521118164062\n",
      "tensor(172.6238, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.62384033203125\n",
      "tensor(167.8528, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.85284423828125\n",
      "tensor(166.9219, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.92185974121094\n",
      "tensor(172.0602, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.06015014648438\n",
      "tensor(165.9626, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.96255493164062\n",
      "tensor(162.5251, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.52511596679688\n",
      "tensor(186.7697, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.76974487304688\n",
      "tensor(165.7589, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.7589111328125\n",
      "tensor(167.2244, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.22438049316406\n",
      "tensor(170.3037, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.30374145507812\n",
      "tensor(176.4962, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.4962158203125\n",
      "tensor(172.3971, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.3971405029297\n",
      "tensor(172.7852, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.78517150878906\n",
      "tensor(172.5946, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.5946044921875\n",
      "tensor(161.9328, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.93284606933594\n",
      "tensor(166.4744, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.47438049316406\n",
      "tensor(159.6629, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.6628875732422\n",
      "tensor(184.2131, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.21310424804688\n",
      "tensor(177.6429, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.6429443359375\n",
      "tensor(166.8600, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.85995483398438\n",
      "tensor(159.8320, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.83200073242188\n",
      "tensor(165.5948, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.59475708007812\n",
      "tensor(166.9295, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.92950439453125\n",
      "tensor(192.0213, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "192.02134704589844\n",
      "tensor(172.6086, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.60858154296875\n",
      "tensor(163.8314, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.8314208984375\n",
      "tensor(174.3748, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.37484741210938\n",
      "tensor(168.4012, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.40115356445312\n",
      "tensor(168.0045, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.00454711914062\n",
      "tensor(184.5963, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.59634399414062\n",
      "tensor(170.8216, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.82156372070312\n",
      "tensor(168.8284, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.8284454345703\n",
      "tensor(183.5649, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.56494140625\n",
      "tensor(173.4094, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.4093780517578\n",
      "tensor(156.2812, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.28125\n",
      "tensor(174.1330, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.13296508789062\n",
      "tensor(170.3740, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.37400817871094\n",
      "tensor(161.1593, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.1592559814453\n",
      "tensor(178.2293, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.2293243408203\n",
      "tensor(169.5213, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.52127075195312\n",
      "tensor(169.1373, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.1372833251953\n",
      "tensor(171.1730, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.1730194091797\n",
      "tensor(166.4102, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.41018676757812\n",
      "tensor(159.9779, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.97787475585938\n",
      "tensor(177.2810, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.28097534179688\n",
      "tensor(165.4742, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.47418212890625\n",
      "tensor(170.6688, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.66876220703125\n",
      "tensor(172.5743, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.57431030273438\n",
      "tensor(174.2926, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.2926025390625\n",
      "tensor(190.0351, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "190.03509521484375\n",
      "tensor(164.4028, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.4027862548828\n",
      "tensor(174.4266, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.42657470703125\n",
      "tensor(163.5775, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.57749938964844\n",
      "tensor(167.2204, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.22036743164062\n",
      "tensor(166.3110, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.31097412109375\n",
      "tensor(176.2248, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.2247772216797\n",
      "tensor(167.8628, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.86279296875\n",
      "tensor(174.3387, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.3387451171875\n",
      "tensor(169.2657, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.26565551757812\n",
      "tensor(169.8350, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.8349609375\n",
      "tensor(176.8297, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.82965087890625\n",
      "tensor(165.4246, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.42459106445312\n",
      "tensor(172.9366, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.93663024902344\n",
      "tensor(155.9506, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "155.95059204101562\n",
      "tensor(173.7017, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.70167541503906\n",
      "tensor(164.8864, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.88644409179688\n",
      "tensor(191.0832, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "191.0832061767578\n",
      "tensor(182.2225, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.22254943847656\n",
      "tensor(171.4323, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.4322967529297\n",
      "tensor(179.9856, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.98561096191406\n",
      "tensor(173.6304, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.63040161132812\n",
      "tensor(161.6409, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.640869140625\n",
      "tensor(174.9948, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.99484252929688\n",
      "tensor(178.3467, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.3467254638672\n",
      "tensor(170.8664, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.86639404296875\n",
      "tensor(175.4654, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.46542358398438\n",
      "tensor(177.1256, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.12559509277344\n",
      "tensor(169.5650, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.56503295898438\n",
      "tensor(172.3004, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.30038452148438\n",
      "tensor(170.4617, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.46173095703125\n",
      "tensor(169.0341, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.03411865234375\n",
      "tensor(186.0190, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "186.01898193359375\n",
      "tensor(168.1090, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.1090087890625\n",
      "tensor(170.3607, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.36074829101562\n",
      "tensor(165.5420, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.5419921875\n",
      "tensor(165.1196, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.1195831298828\n",
      "tensor(174.6796, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.6795654296875\n",
      "tensor(180.6948, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.69476318359375\n",
      "tensor(180.7517, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.75167846679688\n",
      "tensor(175.9899, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.98985290527344\n",
      "tensor(159.5031, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.5030975341797\n",
      "tensor(164.7111, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.71109008789062\n",
      "tensor(173.2459, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.24591064453125\n",
      "tensor(174.9226, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.9225616455078\n",
      "tensor(166.6224, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.62237548828125\n",
      "tensor(167.5212, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.52117919921875\n",
      "tensor(166.5582, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.5582275390625\n",
      "tensor(168.3532, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.35316467285156\n",
      "tensor(178.8215, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.8214569091797\n",
      "tensor(167.9150, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.91497802734375\n",
      "tensor(176.1900, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.19000244140625\n",
      "tensor(184.6378, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.6377716064453\n",
      "tensor(171.8632, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.8632354736328\n",
      "tensor(169.9098, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.90982055664062\n",
      "tensor(173.6615, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.66146850585938\n",
      "tensor(169.1551, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.15505981445312\n",
      "tensor(184.8296, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.82962036132812\n",
      "tensor(168.8246, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.82461547851562\n",
      "tensor(170.5288, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.52882385253906\n",
      "tensor(179.4391, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.4391326904297\n",
      "tensor(174.9339, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.93392944335938\n",
      "tensor(188.5238, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "188.52378845214844\n",
      "tensor(171.2998, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.2998046875\n",
      "tensor(171.0120, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.01199340820312\n",
      "tensor(171.2755, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.27548217773438\n",
      "tensor(178.2442, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.24423217773438\n",
      "tensor(179.0747, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.07469177246094\n",
      "tensor(156.5734, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.57337951660156\n",
      "tensor(166.6736, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.67359924316406\n",
      "tensor(174.6291, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.62911987304688\n",
      "tensor(166.0961, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.0960693359375\n",
      "tensor(178.9772, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.97718811035156\n",
      "tensor(175.8644, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.86444091796875\n",
      "tensor(171.9711, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.9711456298828\n",
      "tensor(170.0341, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.03414916992188\n",
      "tensor(173.2577, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.25765991210938\n",
      "tensor(169.2005, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.20046997070312\n",
      "tensor(182.6230, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.62295532226562\n",
      "tensor(161.3191, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.31912231445312\n",
      "tensor(179.3283, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.32833862304688\n",
      "tensor(155.9575, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "155.95745849609375\n",
      "tensor(177.5237, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.52365112304688\n",
      "tensor(170.5003, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.5002899169922\n",
      "tensor(169.6258, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.62576293945312\n",
      "tensor(164.0335, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.03346252441406\n",
      "tensor(171.0266, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.026611328125\n",
      "tensor(167.4229, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.4229278564453\n",
      "tensor(165.2159, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.21585083007812\n",
      "tensor(176.8039, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.80392456054688\n",
      "tensor(189.8982, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.898193359375\n",
      "tensor(159.8958, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.89584350585938\n",
      "tensor(171.2346, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.23463439941406\n",
      "tensor(173.0677, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.06768798828125\n",
      "tensor(170.6488, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.6487579345703\n",
      "tensor(173.5784, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.57843017578125\n",
      "tensor(176.3051, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.30514526367188\n",
      "tensor(169.0436, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.04360961914062\n",
      "tensor(172.3150, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.31503295898438\n",
      "tensor(164.1263, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.1262969970703\n",
      "tensor(164.0058, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.00575256347656\n",
      "tensor(168.9832, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.98321533203125\n",
      "tensor(157.3964, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.39642333984375\n",
      "tensor(160.7324, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.73239135742188\n",
      "tensor(183.9602, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.96022033691406\n",
      "tensor(169.5828, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.58279418945312\n",
      "tensor(164.1616, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.16160583496094\n",
      "tensor(185.7405, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "185.74053955078125\n",
      "tensor(175.3106, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.31063842773438\n",
      "tensor(172.9646, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.96463012695312\n",
      "tensor(166.6331, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.63308715820312\n",
      "tensor(162.3170, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.31703186035156\n",
      "tensor(166.9357, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.9357452392578\n",
      "tensor(169.8510, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.85104370117188\n",
      "tensor(157.1678, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.16783142089844\n",
      "tensor(180.5885, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.58853149414062\n",
      "tensor(168.5516, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.55157470703125\n",
      "tensor(163.0430, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.0430145263672\n",
      "tensor(160.5408, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.54083251953125\n",
      "tensor(170.5953, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.59532165527344\n",
      "tensor(165.8469, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.84686279296875\n",
      "tensor(170.4389, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.43885803222656\n",
      "tensor(172.2946, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.2945556640625\n",
      "tensor(169.4773, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.4773406982422\n",
      "tensor(160.1610, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.16098022460938\n",
      "tensor(161.0438, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.04379272460938\n",
      "tensor(164.4915, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.49154663085938\n",
      "tensor(177.3520, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.35198974609375\n",
      "tensor(172.0248, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.02479553222656\n",
      "tensor(182.4592, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.45919799804688\n",
      "tensor(174.2519, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.25189208984375\n",
      "tensor(164.1576, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.15757751464844\n",
      "tensor(166.6592, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.6592254638672\n",
      "tensor(166.5454, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.5453643798828\n",
      "tensor(162.5440, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.54400634765625\n",
      "tensor(168.0739, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.07388305664062\n",
      "tensor(165.7966, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.79656982421875\n",
      "tensor(162.7896, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.78955078125\n",
      "tensor(167.5500, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.550048828125\n",
      "tensor(164.7506, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.75062561035156\n",
      "tensor(165.4195, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.41949462890625\n",
      "tensor(156.7706, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.7706298828125\n",
      "tensor(171.8196, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.81961059570312\n",
      "tensor(159.9752, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.97518920898438\n",
      "tensor(164.9567, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.95669555664062\n",
      "tensor(167.0737, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.07373046875\n",
      "tensor(177.1629, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.16293334960938\n",
      "tensor(170.4072, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.4072265625\n",
      "tensor(162.6835, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.68350219726562\n",
      "tensor(170.4318, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.4317626953125\n",
      "tensor(165.4204, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.42039489746094\n",
      "tensor(170.3264, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.32640075683594\n",
      "tensor(167.2445, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.2445068359375\n",
      "tensor(163.1382, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.13821411132812\n",
      "tensor(169.0817, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.0816650390625\n",
      "tensor(167.9760, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.97601318359375\n",
      "tensor(160.3600, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.35997009277344\n",
      "tensor(179.0547, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.05471801757812\n",
      "tensor(180.8463, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.8463134765625\n",
      "tensor(171.0295, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.0294952392578\n",
      "tensor(163.5680, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.5680389404297\n",
      "tensor(182.4246, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.4246063232422\n",
      "tensor(159.0759, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.0758514404297\n",
      "tensor(169.6178, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.6177520751953\n",
      "tensor(154.5555, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "154.5555419921875\n",
      "tensor(166.6423, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.64227294921875\n",
      "tensor(169.0702, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.0701904296875\n",
      "tensor(168.2584, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.25836181640625\n",
      "tensor(178.8305, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.8305206298828\n",
      "tensor(165.9583, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.95826721191406\n",
      "tensor(168.0700, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.07000732421875\n",
      "tensor(171.7279, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.72793579101562\n",
      "tensor(165.0784, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.0784149169922\n",
      "tensor(173.8712, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.87118530273438\n",
      "tensor(182.6763, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.67625427246094\n",
      "tensor(187.4050, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.405029296875\n",
      "tensor(169.3635, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.36349487304688\n",
      "tensor(174.0144, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.01437377929688\n",
      "tensor(165.9274, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.92735290527344\n",
      "tensor(166.6455, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.64547729492188\n",
      "tensor(167.4871, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.487060546875\n",
      "tensor(164.0862, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.0862274169922\n",
      "tensor(165.4116, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.41163635253906\n",
      "tensor(166.7131, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.7130889892578\n",
      "tensor(167.1479, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.1478729248047\n",
      "tensor(161.6751, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.67510986328125\n",
      "tensor(172.4277, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.42767333984375\n",
      "tensor(169.1392, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.13916015625\n",
      "tensor(167.4461, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.44607543945312\n",
      "tensor(181.1994, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.19940185546875\n",
      "tensor(169.1194, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.11944580078125\n",
      "tensor(168.7945, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.79454040527344\n",
      "tensor(169.8376, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.83755493164062\n",
      "tensor(167.6788, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.6788330078125\n",
      "tensor(172.3859, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.3859100341797\n",
      "tensor(163.3826, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.38259887695312\n",
      "tensor(165.4579, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.4579315185547\n",
      "tensor(161.3329, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.33291625976562\n",
      "tensor(169.9261, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.9261474609375\n",
      "tensor(160.6718, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.67181396484375\n",
      "tensor(173.2453, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.24526977539062\n",
      "tensor(167.4570, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.45704650878906\n",
      "tensor(183.3726, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.37255859375\n",
      "tensor(164.2973, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.29734802246094\n",
      "tensor(163.7866, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.78662109375\n",
      "tensor(167.3047, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.30465698242188\n",
      "tensor(176.1027, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.10272216796875\n",
      "tensor(175.3444, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.34442138671875\n",
      "tensor(164.7088, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.70875549316406\n",
      "tensor(170.4598, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.4597625732422\n",
      "tensor(170.9943, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.99429321289062\n",
      "tensor(167.0796, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.07962036132812\n",
      "tensor(165.4322, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.43218994140625\n",
      "tensor(161.6746, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.674560546875\n",
      "tensor(165.5291, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.529052734375\n",
      "tensor(165.5494, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.5494384765625\n",
      "tensor(174.6469, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.6468963623047\n",
      "tensor(160.6346, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.63455200195312\n",
      "tensor(174.0433, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.04330444335938\n",
      "tensor(156.1665, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.16650390625\n",
      "tensor(183.3003, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "183.30032348632812\n",
      "tensor(162.8091, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.80911254882812\n",
      "tensor(159.9379, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.93792724609375\n",
      "tensor(171.3982, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.39817810058594\n",
      "tensor(165.7163, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.71632385253906\n",
      "tensor(160.3104, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.3104248046875\n",
      "tensor(167.3012, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.30123901367188\n",
      "tensor(155.3369, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "155.33685302734375\n",
      "tensor(168.6114, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.61141967773438\n",
      "tensor(176.5923, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.59228515625\n",
      "tensor(170.9685, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.96853637695312\n",
      "tensor(165.0135, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.0135498046875\n",
      "tensor(162.2881, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.28814697265625\n",
      "tensor(166.1654, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.1654052734375\n",
      "tensor(177.3444, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.34439086914062\n",
      "tensor(181.1233, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.12326049804688\n",
      "tensor(175.5618, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.561767578125\n",
      "tensor(165.5417, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.541748046875\n",
      "tensor(164.6691, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.66905212402344\n",
      "tensor(155.8866, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "155.8865966796875\n",
      "tensor(168.1460, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.14598083496094\n",
      "tensor(182.4715, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "182.47146606445312\n",
      "tensor(158.2980, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "158.29798889160156\n",
      "tensor(174.5946, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.5946044921875\n",
      "tensor(162.6666, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.6666259765625\n",
      "tensor(172.1020, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.10198974609375\n",
      "tensor(169.1261, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.1260986328125\n",
      "tensor(176.7395, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.73948669433594\n",
      "tensor(174.7614, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.7614288330078\n",
      "tensor(164.0743, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.0742950439453\n",
      "tensor(164.4824, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.4823760986328\n",
      "tensor(161.5582, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.5581817626953\n",
      "tensor(170.2579, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.2579345703125\n",
      "tensor(175.8345, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.83453369140625\n",
      "tensor(163.2074, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.20738220214844\n",
      "tensor(163.2723, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.27227783203125\n",
      "tensor(169.2500, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.24998474121094\n",
      "tensor(161.6674, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.66741943359375\n",
      "tensor(166.0563, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.05633544921875\n",
      "tensor(168.9156, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.91555786132812\n",
      "tensor(169.5328, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.5327911376953\n",
      "tensor(180.7643, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "180.76431274414062\n",
      "tensor(160.9786, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.97857666015625\n",
      "tensor(171.6277, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.62765502929688\n",
      "tensor(158.5081, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "158.50808715820312\n",
      "tensor(161.8608, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.86082458496094\n",
      "tensor(172.1853, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.18533325195312\n",
      "tensor(163.2438, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.24383544921875\n",
      "tensor(160.0016, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.00164794921875\n",
      "tensor(164.0625, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.06251525878906\n",
      "tensor(174.3366, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.33657836914062\n",
      "tensor(160.4003, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.4003143310547\n",
      "tensor(163.4693, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.46926879882812\n",
      "tensor(159.5234, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.5233612060547\n",
      "tensor(157.5525, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.55247497558594\n",
      "tensor(181.0573, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "181.05726623535156\n",
      "tensor(160.5445, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.54446411132812\n",
      "tensor(159.3984, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.39840698242188\n",
      "tensor(167.6269, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.6269073486328\n",
      "tensor(175.3502, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "175.35020446777344\n",
      "tensor(171.8452, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.8451690673828\n",
      "tensor(168.9683, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.96827697753906\n",
      "tensor(178.2290, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.22897338867188\n",
      "tensor(166.3255, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.32553100585938\n",
      "tensor(161.4825, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.48245239257812\n",
      "tensor(174.9830, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.98297119140625\n",
      "tensor(163.6393, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.63934326171875\n",
      "tensor(176.7967, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.79666137695312\n",
      "tensor(170.8890, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.88900756835938\n",
      "tensor(168.9025, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.90252685546875\n",
      "tensor(168.2256, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.22561645507812\n",
      "tensor(170.8472, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.84716796875\n",
      "tensor(162.1963, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.19631958007812\n",
      "tensor(167.5021, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.50209045410156\n",
      "tensor(165.8228, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.82284545898438\n",
      "tensor(164.9131, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.9131317138672\n",
      "tensor(170.3060, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.3059844970703\n",
      "tensor(162.1213, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.1212615966797\n",
      "tensor(166.6160, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.6160125732422\n",
      "tensor(168.3904, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.3904266357422\n",
      "tensor(163.8156, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.81564331054688\n",
      "tensor(168.2453, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.24525451660156\n",
      "tensor(163.6009, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.60092163085938\n",
      "tensor(163.0485, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.04847717285156\n",
      "tensor(164.2126, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.212646484375\n",
      "tensor(158.1489, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "158.14894104003906\n",
      "tensor(162.0226, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.02264404296875\n",
      "tensor(174.5142, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.51422119140625\n",
      "tensor(156.2030, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.2030029296875\n",
      "tensor(178.6726, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.672607421875\n",
      "tensor(156.5885, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.5885009765625\n",
      "tensor(176.6851, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.68508911132812\n",
      "tensor(173.0565, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.05645751953125\n",
      "tensor(167.8919, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.89187622070312\n",
      "tensor(155.0706, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "155.0706329345703\n",
      "tensor(152.7733, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "152.7733154296875\n",
      "tensor(166.6978, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.69784545898438\n",
      "tensor(165.0688, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.06884765625\n",
      "tensor(157.5329, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.53289794921875\n",
      "tensor(166.2522, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.25221252441406\n",
      "tensor(170.2612, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.26116943359375\n",
      "tensor(150.9794, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "150.9793701171875\n",
      "tensor(163.3631, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.3631134033203\n",
      "tensor(163.1413, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.14129638671875\n",
      "tensor(167.7444, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.74444580078125\n",
      "tensor(157.0411, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.04110717773438\n",
      "tensor(168.7823, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.78231811523438\n",
      "tensor(156.7360, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.7359619140625\n",
      "tensor(163.9440, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.94400024414062\n",
      "tensor(157.7696, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.76963806152344\n",
      "tensor(166.9767, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.97674560546875\n",
      "tensor(163.3899, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.38986206054688\n",
      "tensor(167.9670, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.9669647216797\n",
      "tensor(165.0833, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.0833282470703\n",
      "tensor(162.3611, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.36114501953125\n",
      "tensor(170.6138, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "170.61380004882812\n",
      "tensor(168.1107, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.11065673828125\n",
      "tensor(164.0844, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.08444213867188\n",
      "tensor(176.4060, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.406005859375\n",
      "tensor(168.4146, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.4146270751953\n",
      "tensor(161.8632, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.8631591796875\n",
      "tensor(168.0645, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.06448364257812\n",
      "tensor(160.1989, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.19888305664062\n",
      "tensor(171.5398, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.53976440429688\n",
      "tensor(167.2525, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.25245666503906\n",
      "tensor(165.3643, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.36431884765625\n",
      "tensor(161.6542, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.65423583984375\n",
      "tensor(177.5536, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "177.5535888671875\n",
      "tensor(159.1849, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.18487548828125\n",
      "tensor(166.2454, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.2454376220703\n",
      "tensor(164.5399, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.5399169921875\n",
      "tensor(187.7451, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "187.74514770507812\n",
      "tensor(156.8832, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "156.88323974609375\n",
      "tensor(152.1594, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "152.15940856933594\n",
      "tensor(162.9917, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.99172973632812\n",
      "tensor(160.0950, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.09500122070312\n",
      "tensor(166.0571, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.05714416503906\n",
      "tensor(157.9536, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.95361328125\n",
      "tensor(159.4027, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "159.40272521972656\n",
      "tensor(176.4446, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "176.4446258544922\n",
      "tensor(173.1865, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "173.1864776611328\n",
      "tensor(168.8658, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.8658447265625\n",
      "tensor(166.4500, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.44998168945312\n",
      "tensor(165.2291, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.2290802001953\n",
      "tensor(160.9288, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "160.9288330078125\n",
      "tensor(157.9987, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.99868774414062\n",
      "tensor(189.8585, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "189.8585205078125\n",
      "tensor(171.6164, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.61636352539062\n",
      "tensor(157.0786, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.07858276367188\n",
      "tensor(157.4276, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.42764282226562\n",
      "tensor(157.6846, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "157.68460083007812\n",
      "tensor(171.4301, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.43011474609375\n",
      "tensor(168.9893, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.98931884765625\n",
      "tensor(164.0626, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.06259155273438\n",
      "tensor(158.7480, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "158.7480010986328\n",
      "tensor(154.7266, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "154.72659301757812\n",
      "tensor(184.3431, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "184.34312438964844\n",
      "tensor(165.5471, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "165.54713439941406\n",
      "tensor(162.9448, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.9447784423828\n",
      "tensor(169.3174, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "169.31744384765625\n",
      "tensor(167.5643, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.5643310546875\n",
      "tensor(162.9129, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.91294860839844\n",
      "tensor(158.0354, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "158.03536987304688\n",
      "tensor(163.0498, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "163.04983520507812\n",
      "tensor(161.4093, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "161.40933227539062\n",
      "tensor(174.9831, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "174.98309326171875\n",
      "tensor(167.4616, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "167.46157836914062\n",
      "tensor(150.8357, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "150.83572387695312\n",
      "tensor(172.1754, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.1754150390625\n",
      "tensor(164.0537, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "164.0536651611328\n",
      "tensor(178.4332, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "178.43319702148438\n",
      "tensor(166.5627, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "166.56272888183594\n",
      "tensor(179.2025, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "179.20254516601562\n",
      "tensor(172.3149, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "172.31492614746094\n",
      "tensor(171.7124, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "171.71240234375\n",
      "tensor(162.6718, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "162.67178344726562\n",
      "tensor(168.6509, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "168.65093994140625\n",
      "tensor(158.9624, device='cuda:6', grad_fn=<SumBackward0>)\n",
      "158.96241760253906\n"
     ]
    }
   ],
   "source": [
    "clsmodel = clsmodel.to(6)\n",
    "embeds = embeds.to(6)\n",
    "cri = models.ListMLE()\n",
    "# clsmodel.eval()\n",
    "# for i in data:\n",
    "#     i[\"seq_t\"] = i[\"seq_t\"].to(2)\n",
    "q = torch.optim.Adam(filter(lambda x: x.requires_grad, clsmodel.parameters()), lr=0.0001)\n",
    "for i in range(1500):\n",
    "    q.zero_grad()\n",
    "    loss = cri(clsmodel(embeds.unsqueeze(1)).squeeze(1))\n",
    "    # loss = clsmodel._list_training_step(data)\n",
    "    print(loss)\n",
    "    # loss = loss * 0.0\n",
    "    loss.backward()\n",
    "    q.step()\n",
    "    print(loss.item())\n",
    "    # print(model(data).item())\n",
    "    # print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.l1.weight Parameter containing:\n",
      "tensor([[ 5.6183e-03, -7.2583e-04,  9.2370e-04,  ...,  2.3575e-03,\n",
      "         -6.6478e-03, -2.2759e-03],\n",
      "        [-4.2368e-03,  3.8491e-03, -3.8733e-03,  ..., -3.8524e-03,\n",
      "         -4.0215e-03,  3.9197e-03],\n",
      "        [ 1.6638e-03,  8.2855e-04,  1.1220e-03,  ..., -5.4048e-04,\n",
      "         -3.3162e-05, -1.0610e-03],\n",
      "        ...,\n",
      "        [-4.1821e-03,  2.9903e-03, -2.4395e-03,  ..., -1.5212e-03,\n",
      "         -2.8255e-03,  3.6214e-03],\n",
      "        [ 1.8198e-03, -1.1366e-03,  1.9718e-03,  ...,  9.9019e-05,\n",
      "          7.1877e-04, -1.6034e-03],\n",
      "        [-1.1829e-03,  5.6399e-03, -4.5914e-03,  ...,  3.6545e-03,\n",
      "         -1.4379e-04,  5.3215e-03]], device='cuda:2', requires_grad=True)\n",
      "clf.l1.bias Parameter containing:\n",
      "tensor([ 2.9316e-02, -6.9022e-03, -6.3497e-04,  2.0163e-02,  1.9693e-03,\n",
      "         4.1604e-03,  7.1277e-03,  2.1045e-02,  1.6818e-02,  1.6161e-03,\n",
      "         1.4598e-02, -1.0070e-02, -5.4684e-04, -4.7448e-03, -1.8326e-03,\n",
      "         6.9637e-03,  6.9514e-03,  2.1132e-03, -3.2294e-03, -7.6070e-03,\n",
      "         1.1627e-02,  2.2202e-02,  1.3819e-03,  1.1194e-03,  1.7981e-02,\n",
      "         8.4880e-03,  1.7360e-02, -2.1212e-04,  5.2687e-03, -7.0888e-03,\n",
      "         1.3577e-02,  7.2277e-04,  1.3848e-02,  4.1628e-03,  1.5805e-03,\n",
      "         1.4541e-02,  2.3371e-02,  5.3903e-05,  4.1947e-03,  1.0670e-02,\n",
      "         1.5079e-02, -1.1911e-02,  1.1945e-02, -3.8656e-03,  2.1448e-02,\n",
      "         1.4740e-02,  1.4879e-02, -4.2359e-03,  1.7297e-02,  7.6483e-03,\n",
      "         2.9752e-03,  1.0214e-02,  8.8905e-04,  1.3978e-03, -4.4592e-04,\n",
      "         9.9586e-03,  1.7862e-02,  2.0642e-02,  4.9035e-03,  1.1962e-02,\n",
      "        -7.2790e-03,  2.7525e-04,  7.4641e-04,  4.4860e-03,  1.2593e-02,\n",
      "         1.7946e-02,  9.8589e-03,  1.5900e-02,  4.0260e-03,  4.0096e-03,\n",
      "        -6.1903e-03, -2.9462e-03,  2.3569e-02,  2.6050e-02,  3.7106e-03,\n",
      "         2.6087e-03,  1.2064e-02, -8.6739e-05,  8.5181e-03,  2.0361e-04,\n",
      "        -2.9976e-03,  2.4627e-03,  5.2186e-03,  4.9930e-03,  1.0881e-02,\n",
      "         1.1715e-02,  3.3346e-03, -7.3297e-03,  4.6348e-03,  6.0813e-04,\n",
      "        -2.5928e-04,  6.9294e-03,  4.4036e-03, -5.0533e-03, -4.0425e-04,\n",
      "         1.3600e-02, -1.3423e-03,  9.2342e-03,  1.0444e-02,  1.9833e-02,\n",
      "         1.7261e-02,  2.7908e-02,  3.6758e-03,  1.1956e-02,  1.5728e-02,\n",
      "        -3.4458e-03, -4.1225e-03,  5.1880e-04, -1.5313e-04, -2.7326e-03,\n",
      "         6.9649e-03,  7.5553e-04,  1.7948e-03, -1.1249e-02, -4.9315e-03,\n",
      "         4.4669e-03, -3.0221e-03,  7.3532e-03,  8.6020e-03, -3.0623e-03,\n",
      "         8.8783e-03,  2.5940e-03,  1.3334e-02,  1.9238e-02,  4.7789e-03,\n",
      "         2.0239e-03,  7.6495e-03,  6.2369e-04,  3.1390e-03, -5.6758e-03,\n",
      "         1.6585e-02,  1.0855e-02, -1.5788e-03,  2.8579e-02,  7.1673e-03,\n",
      "        -2.0624e-03,  1.1089e-03,  1.1957e-02,  9.6818e-03,  1.5314e-02,\n",
      "        -4.8753e-03, -2.0971e-03, -4.7343e-03, -1.4695e-03, -6.0603e-04,\n",
      "         1.3654e-03,  2.5255e-03,  5.5901e-03,  3.7474e-03,  5.8833e-03,\n",
      "         1.5084e-02,  7.1234e-03,  6.0208e-03,  4.9580e-03, -3.3189e-03,\n",
      "        -4.0573e-03, -5.3487e-03,  2.4141e-03,  7.9498e-03,  1.2017e-02,\n",
      "        -5.2094e-03,  5.4687e-03,  6.2032e-03,  1.6029e-02,  1.1218e-02,\n",
      "         4.4153e-03,  2.8527e-02, -5.5969e-03, -6.2284e-04, -9.7085e-04,\n",
      "         2.8682e-03,  2.2677e-03,  2.9059e-03, -4.5419e-03,  3.0941e-02,\n",
      "         1.3289e-03,  2.4669e-03, -6.6660e-03, -6.1192e-03,  6.8022e-03,\n",
      "         1.4670e-03, -6.1687e-03,  2.9753e-02,  8.4967e-04,  1.1984e-02,\n",
      "         1.1585e-02, -2.2384e-03,  1.3972e-02,  9.7625e-04, -7.5939e-03,\n",
      "        -3.5041e-06,  5.6072e-03,  2.4469e-02,  1.1861e-02, -3.4935e-03,\n",
      "         1.4935e-02,  2.5431e-04,  2.5519e-03, -8.9539e-03, -1.5088e-03,\n",
      "        -1.3453e-03,  8.7516e-03,  1.4115e-03,  2.3824e-02,  2.5549e-02,\n",
      "         9.2784e-03, -1.2691e-02,  4.0351e-03,  1.8892e-03,  5.0049e-03,\n",
      "        -1.5777e-03,  1.0276e-02, -1.0264e-02,  2.4648e-03,  1.6148e-02,\n",
      "         1.8465e-02, -8.5780e-03,  7.9008e-03, -2.6921e-03,  2.3246e-03,\n",
      "         7.4658e-03,  6.1668e-03, -5.9395e-03,  1.4024e-02,  2.8172e-04,\n",
      "         7.7910e-03,  1.7675e-03, -2.8120e-03,  9.9277e-03,  1.1494e-02,\n",
      "         1.2200e-02,  4.1551e-04,  8.5567e-03,  1.5739e-03,  2.3703e-03,\n",
      "         6.5206e-03,  3.3215e-03, -5.6454e-03,  2.0893e-02,  9.3659e-03,\n",
      "        -4.4474e-03,  1.3512e-02, -7.6348e-04, -4.9629e-03, -5.2683e-04,\n",
      "         2.6304e-03,  2.5258e-03,  6.1397e-03, -6.7724e-04,  7.9233e-03,\n",
      "         1.5253e-02,  5.5759e-03,  9.7872e-04, -9.6808e-03,  4.4343e-03,\n",
      "        -1.3686e-02, -2.0651e-03, -3.0731e-03,  1.4955e-02, -2.9253e-04,\n",
      "        -1.0091e-02,  8.6937e-03,  5.9277e-03,  2.8592e-03,  1.1561e-03,\n",
      "         6.4625e-03,  4.2199e-03,  1.1651e-02,  2.4569e-03,  1.3970e-03,\n",
      "         1.4554e-03,  6.3237e-03,  2.1123e-02, -3.6490e-04, -3.9099e-03,\n",
      "         7.7017e-03,  3.8866e-03,  4.4428e-04,  9.6337e-03, -7.5204e-03,\n",
      "         7.1176e-03, -7.3598e-03,  4.1036e-03,  2.5905e-02,  2.6718e-05,\n",
      "         3.6008e-03,  4.4432e-03, -2.2302e-03,  1.1753e-02,  1.1404e-02,\n",
      "         1.2019e-02,  2.8390e-03,  7.2568e-03,  1.5399e-02,  5.7405e-03,\n",
      "         3.5565e-03,  1.2405e-02,  5.8824e-03, -4.7023e-03,  1.4737e-04,\n",
      "         5.8616e-04,  1.2182e-02,  1.5356e-02,  8.7733e-03,  6.8594e-03,\n",
      "        -7.1624e-03,  1.0725e-02, -5.2411e-03,  1.9329e-02, -3.6959e-03,\n",
      "        -2.5093e-03,  2.5536e-02,  1.0697e-03,  4.3422e-04,  3.2651e-03,\n",
      "         1.8039e-02,  6.0013e-03,  2.0646e-03,  3.0085e-03, -1.5119e-03,\n",
      "        -4.0721e-03,  5.8524e-03,  4.5170e-03,  3.0610e-02,  1.1355e-03,\n",
      "         8.0239e-03,  6.4897e-03,  1.5216e-02,  1.8676e-02, -7.8732e-03,\n",
      "        -3.1069e-03,  2.3816e-02, -3.0422e-03,  4.7021e-03, -1.5728e-02,\n",
      "        -4.2533e-04,  6.0497e-03, -2.6682e-03,  9.0384e-03, -1.7775e-04,\n",
      "         8.9290e-03,  5.5556e-03, -3.1449e-03, -7.5259e-03,  2.4541e-02,\n",
      "         1.6526e-03,  3.8601e-03,  1.0936e-02,  1.1048e-02,  2.6398e-02,\n",
      "        -5.5842e-03,  3.4133e-04, -1.7368e-03,  7.7660e-04,  1.3734e-03,\n",
      "         9.4519e-04,  8.2423e-04,  1.5443e-02,  1.9696e-02, -4.6283e-03,\n",
      "         9.3884e-03,  2.5341e-03,  1.0199e-02,  2.1147e-03,  2.7360e-03,\n",
      "         8.9003e-03,  1.4718e-02,  8.3167e-04,  1.4644e-02,  1.3827e-02,\n",
      "        -4.4238e-03,  1.8611e-03,  6.7546e-03,  7.3624e-04,  4.0510e-03,\n",
      "         8.8614e-03, -9.3987e-04, -4.5683e-04,  1.0306e-02, -1.0082e-03,\n",
      "         1.1301e-03, -5.1635e-03,  7.0008e-04,  1.0838e-02, -4.9538e-03,\n",
      "         1.6529e-02, -9.5349e-03, -8.1158e-04,  8.4450e-04,  2.2744e-03,\n",
      "         7.9906e-03, -1.0237e-03, -3.6544e-03,  7.9897e-04, -5.0357e-03,\n",
      "         1.2617e-02,  2.9553e-02, -5.6488e-04,  1.2867e-02, -1.3715e-03,\n",
      "         1.7846e-03,  1.7102e-02, -6.7703e-03,  8.2637e-03, -2.6049e-03,\n",
      "         8.1133e-03,  1.3280e-02,  5.1652e-04, -3.8213e-03,  6.9380e-03,\n",
      "        -9.9944e-06, -2.8532e-03,  1.0096e-02,  2.7325e-03,  3.1989e-03,\n",
      "         8.8716e-03, -1.2342e-02, -7.7108e-03, -7.6089e-05,  2.7764e-03,\n",
      "         2.5117e-03, -1.0036e-03,  9.0982e-03,  1.0625e-02,  2.9129e-02,\n",
      "         1.3713e-03,  7.6257e-03,  6.2738e-03,  1.8151e-03, -4.1802e-03,\n",
      "        -1.1506e-04,  2.3757e-03,  1.4782e-02,  1.1659e-02,  5.2484e-03,\n",
      "         7.6126e-03,  1.3734e-03,  1.0359e-02, -2.6740e-03,  1.3178e-03,\n",
      "         2.7269e-03, -1.1555e-02, -5.1569e-03,  4.3623e-03, -3.3022e-03,\n",
      "         3.0880e-03, -2.2619e-03, -8.8545e-03,  4.9827e-03,  1.9580e-03,\n",
      "        -3.9624e-03,  4.5348e-03,  7.6566e-03, -3.1407e-03,  4.0202e-03,\n",
      "         9.4930e-04,  2.6968e-02,  3.4018e-03,  1.2464e-02,  4.0089e-03,\n",
      "         1.4676e-02,  2.1449e-03,  1.8898e-02,  6.7609e-03, -1.2319e-03,\n",
      "        -1.8980e-03, -1.7640e-03,  2.2843e-04,  2.2552e-02, -5.4070e-03,\n",
      "        -1.0727e-04, -1.0343e-02,  1.2843e-03,  2.1454e-03,  1.5111e-02,\n",
      "         1.4971e-04,  7.0931e-03,  2.9511e-02,  5.3099e-03,  8.6640e-03,\n",
      "         4.2189e-03, -1.1322e-03,  4.9722e-03,  7.5171e-03, -1.8843e-03,\n",
      "        -4.0812e-04, -7.6176e-03, -5.2873e-03,  1.0018e-02, -6.4204e-03,\n",
      "         1.1764e-03,  7.6672e-03,  5.3810e-03,  3.0914e-03,  9.9772e-03,\n",
      "         5.0530e-03,  1.5327e-02, -7.4796e-03,  4.9013e-03,  2.6197e-02,\n",
      "        -3.2452e-03, -4.3659e-03, -2.0394e-03,  1.1334e-02, -7.4645e-04,\n",
      "         1.0585e-03, -6.2541e-03,  1.7839e-03, -1.0725e-03, -3.0155e-03,\n",
      "        -1.0248e-03, -1.0620e-02,  2.7396e-02, -3.6524e-03,  1.0203e-02,\n",
      "         1.2113e-02,  1.3590e-03, -5.0465e-03,  4.8773e-03,  2.0148e-02,\n",
      "        -1.9414e-03,  1.1700e-02, -1.1250e-02,  2.1542e-02,  1.7705e-02,\n",
      "        -2.4761e-03,  6.1369e-03, -1.0477e-02,  2.8691e-02, -1.0644e-02,\n",
      "         1.0329e-03,  1.3230e-02,  1.0969e-02, -2.2881e-03,  1.3080e-03,\n",
      "         1.1045e-03,  8.8854e-03,  1.9716e-04,  8.8035e-03,  2.9197e-04,\n",
      "         1.3495e-02,  1.4609e-02,  2.8087e-03,  8.8138e-04, -2.2266e-03,\n",
      "         1.1557e-04, -2.3870e-03,  8.4098e-03,  6.7591e-03,  3.8046e-03,\n",
      "         4.0739e-03,  3.7035e-04, -7.8080e-04,  1.2396e-02,  1.6661e-02,\n",
      "         1.8045e-04,  2.9213e-03,  4.1457e-03,  2.5601e-03,  9.1011e-03,\n",
      "         3.0431e-03,  2.1980e-02,  5.8035e-04,  7.7631e-03,  6.1293e-03,\n",
      "         3.3073e-03,  1.1145e-02,  4.3380e-03,  1.6358e-02, -1.2421e-02,\n",
      "         1.1271e-02,  1.0328e-03,  2.4292e-02,  1.5753e-02, -4.0981e-03,\n",
      "         2.3135e-02], device='cuda:2', requires_grad=True)\n",
      "clf.l2.weight Parameter containing:\n",
      "tensor([[ 4.2965e-03,  6.0798e-04, -9.0565e-04,  ..., -1.6891e-03,\n",
      "          7.8209e-05, -1.1572e-03],\n",
      "        [ 2.3009e-03, -9.4394e-04, -3.3502e-03,  ...,  3.2168e-03,\n",
      "         -4.7184e-03, -8.1374e-04],\n",
      "        [-4.0656e-03,  1.1829e-03,  9.5954e-04,  ..., -5.5230e-03,\n",
      "          4.7459e-04, -1.4069e-02],\n",
      "        ...,\n",
      "        [ 6.9185e-04, -8.0920e-03,  6.9711e-03,  ..., -5.9359e-03,\n",
      "          8.5139e-03, -7.9627e-03],\n",
      "        [-3.2872e-03, -4.7071e-03,  2.6649e-04,  ...,  7.3735e-04,\n",
      "          9.9917e-04,  2.2702e-03],\n",
      "        [ 2.7850e-02,  4.6591e-03, -4.3217e-03,  ...,  2.7488e-03,\n",
      "         -3.9468e-03, -1.0860e-02]], device='cuda:2', requires_grad=True)\n",
      "clf.l2.bias Parameter containing:\n",
      "tensor([-5.6135e-03, -2.4775e-03,  3.7666e-03,  3.5573e-03,  5.4907e-03,\n",
      "        -1.3027e-02, -5.7855e-03,  4.6155e-03, -9.0300e-03, -1.4956e-03,\n",
      "         1.8492e-03,  3.7777e-03,  8.8117e-03,  7.1679e-03, -2.4149e-03,\n",
      "        -1.4188e-03,  1.4512e-03, -3.8477e-03, -4.2684e-03, -4.9121e-03,\n",
      "        -7.7667e-03, -4.1025e-04,  1.7253e-03, -3.6733e-04,  6.3470e-03,\n",
      "        -1.9642e-02, -1.1641e-02,  3.3866e-03, -5.3215e-03,  1.3078e-02,\n",
      "        -2.7836e-03,  8.6718e-03, -4.7459e-03, -2.0679e-03, -1.6767e-03,\n",
      "         7.5403e-04,  4.1529e-03,  2.0647e-02,  4.4639e-04, -4.0388e-03,\n",
      "         1.7097e-02,  3.6974e-03,  5.2687e-03,  1.0887e-03,  2.3914e-03,\n",
      "        -7.6586e-03,  1.9702e-03, -2.0376e-03, -8.0484e-04,  5.0489e-03,\n",
      "         1.0950e-02,  1.3123e-02,  4.3572e-03,  1.2318e-02,  1.7302e-03,\n",
      "         9.6022e-03, -3.7654e-03, -2.1963e-04, -1.4956e-03,  8.3731e-03,\n",
      "         1.1438e-02, -8.2606e-04, -4.7375e-03,  4.1318e-03,  4.9683e-03,\n",
      "         6.6949e-03, -8.8367e-03,  1.5283e-02,  1.4356e-02,  1.1738e-02,\n",
      "        -1.1132e-02,  5.1700e-03, -3.8331e-03, -1.0764e-02,  5.5070e-03,\n",
      "         6.6328e-03,  6.5925e-03,  3.1266e-03,  1.4305e-03,  1.8518e-03,\n",
      "        -2.6139e-04,  1.3282e-02,  1.5849e-02,  5.2935e-03,  2.0419e-02,\n",
      "         8.3022e-04, -3.6799e-03, -3.3788e-03, -5.7941e-03,  3.8340e-03,\n",
      "         1.0748e-03,  1.7984e-02,  4.4706e-03,  1.1208e-02,  5.1381e-03,\n",
      "         3.7638e-03,  2.1524e-03,  5.3900e-03,  4.7189e-03, -1.0364e-02,\n",
      "        -5.1352e-03,  1.1553e-03, -8.9020e-03, -4.3286e-03, -1.0750e-02,\n",
      "        -6.1165e-03, -6.3132e-04,  8.2280e-03,  8.9764e-04, -1.1828e-02,\n",
      "         3.4651e-03,  1.2769e-03,  7.8395e-03,  3.5046e-03, -4.7333e-05,\n",
      "         1.0277e-03, -2.0866e-03,  1.3915e-02,  3.6790e-03,  5.2276e-03,\n",
      "        -4.6844e-04, -4.5381e-03,  1.4719e-02,  6.1945e-03,  6.1808e-04,\n",
      "        -1.5436e-02, -1.2705e-03, -5.1604e-04,  1.2219e-02, -3.0737e-03,\n",
      "         1.8482e-03, -9.3404e-03,  3.9903e-04,  9.3169e-03, -9.4593e-03,\n",
      "         4.9698e-03, -4.2307e-03, -2.7258e-03, -4.8582e-03,  1.1713e-02,\n",
      "        -1.6599e-03,  4.8008e-03, -1.9132e-03, -1.7272e-03, -6.6121e-03,\n",
      "        -1.3410e-03, -6.9821e-03, -4.3211e-03,  1.2039e-03, -1.6229e-03,\n",
      "        -5.5397e-04, -1.1735e-02,  2.0487e-03,  7.8113e-04,  1.0272e-02,\n",
      "         2.5434e-03, -2.3989e-03,  2.3333e-03,  3.5300e-03,  2.3377e-03,\n",
      "         1.7199e-02,  5.9835e-03,  4.0740e-03, -2.6631e-03, -1.6027e-04,\n",
      "        -1.0263e-03, -1.8095e-03, -4.9312e-04, -8.5947e-03, -6.7667e-03,\n",
      "        -2.9420e-03,  7.7879e-03,  8.5732e-05,  1.6346e-02, -6.9649e-03,\n",
      "        -6.4080e-03,  1.8751e-02,  6.4051e-03, -4.5746e-03,  3.5270e-03,\n",
      "        -2.2649e-03, -5.1700e-03, -6.3740e-03, -1.2751e-02, -5.1785e-03,\n",
      "        -4.8238e-03,  1.3823e-02,  1.9819e-02,  1.5037e-04,  1.5723e-02,\n",
      "        -3.2161e-03,  5.4303e-04, -1.9661e-03,  1.1709e-02,  9.8101e-04,\n",
      "         1.3116e-02,  8.5598e-03, -3.9223e-03,  2.3416e-03,  2.5561e-03,\n",
      "        -3.6545e-03, -4.9902e-03, -6.1405e-03,  3.3153e-03,  1.2204e-04,\n",
      "        -2.2799e-02, -1.6014e-02,  1.2011e-03, -6.5678e-03,  4.2516e-03,\n",
      "         5.1835e-03, -1.2623e-03, -1.2527e-03,  1.0055e-02,  2.4802e-03,\n",
      "        -1.4226e-04, -2.6428e-03,  8.6848e-04, -6.7983e-03,  4.5448e-03,\n",
      "        -1.5350e-03,  5.0959e-03, -1.8893e-03,  5.6460e-04, -4.1387e-03,\n",
      "        -9.2047e-04,  3.7695e-03,  2.3089e-03,  6.6326e-03, -2.0978e-03,\n",
      "        -2.8362e-03,  7.2234e-04,  1.6468e-03,  5.3604e-03,  3.1334e-03,\n",
      "         3.6527e-03, -5.8334e-03,  1.0622e-03,  6.7267e-03,  7.2582e-03,\n",
      "         7.5924e-03,  1.7697e-03,  9.5563e-03, -1.0625e-02, -1.5179e-03,\n",
      "        -1.8451e-03,  4.7181e-03, -6.1290e-03, -6.2610e-04, -2.3846e-03,\n",
      "         1.5672e-02,  1.1465e-03, -7.9196e-03,  2.0755e-02, -6.4004e-03,\n",
      "         3.8673e-03,  2.8850e-03, -3.6548e-03, -3.2545e-04,  5.0090e-03,\n",
      "        -1.0788e-03, -5.2878e-03,  7.8120e-03, -3.4543e-03, -3.8834e-03,\n",
      "        -6.4013e-03, -1.0864e-03,  1.9908e-03, -4.6335e-03, -4.4871e-03,\n",
      "        -1.0841e-02,  5.5851e-04,  3.7319e-03,  1.1776e-02,  1.0010e-02,\n",
      "         1.5748e-03,  1.9064e-02,  3.4534e-03,  2.5870e-03, -7.1518e-03,\n",
      "         1.2667e-03, -3.7195e-03,  2.2341e-03,  3.6125e-03,  1.0512e-03,\n",
      "        -2.8095e-03,  4.8018e-03,  4.0257e-03], device='cuda:2',\n",
      "       requires_grad=True)\n",
      "clf.l3.weight Parameter containing:\n",
      "tensor([[-0.0627, -0.0278, -0.0696,  0.0298,  0.0528,  0.0566,  0.0507,  0.0372,\n",
      "          0.0250, -0.0320,  0.0139,  0.0604,  0.0700,  0.0583,  0.0376, -0.0932,\n",
      "          0.0007, -0.0014, -0.0069, -0.0064, -0.0260, -0.0476, -0.0098,  0.0053,\n",
      "          0.0219,  0.0589,  0.0551, -0.0574, -0.0817, -0.0656, -0.0056, -0.0582,\n",
      "          0.0425, -0.0179,  0.0628,  0.0233,  0.0176,  0.0570,  0.0037, -0.0598,\n",
      "          0.0456,  0.0475,  0.0161, -0.0108,  0.0125, -0.0886,  0.0089, -0.0945,\n",
      "         -0.0592, -0.0174, -0.0824, -0.0589, -0.0694, -0.0642, -0.0892,  0.0497,\n",
      "         -0.0219, -0.0109,  0.0117,  0.0529,  0.0406, -0.0131,  0.0026,  0.0134,\n",
      "          0.0470, -0.0753,  0.0478, -0.0788, -0.0890, -0.0542,  0.0399,  0.0132,\n",
      "         -0.0030,  0.0722,  0.0588, -0.0163,  0.0532, -0.0076,  0.0042,  0.0011,\n",
      "          0.0357,  0.0596,  0.0663, -0.0543,  0.0534,  0.0078, -0.0721,  0.0489,\n",
      "          0.0479, -0.0794, -0.0033, -0.0759,  0.0321, -0.0554, -0.0845, -0.0887,\n",
      "         -0.0158, -0.0149,  0.0092, -0.0733, -0.0931,  0.0138, -0.0710, -0.0012,\n",
      "         -0.0357, -0.0791,  0.0138,  0.0304, -0.0026, -0.0200,  0.0274,  0.0488,\n",
      "          0.0506,  0.0379, -0.0280,  0.0670, -0.0045,  0.0418,  0.0113, -0.0678,\n",
      "          0.0073, -0.0593,  0.0359,  0.0551, -0.0102, -0.0717,  0.0151,  0.0519,\n",
      "          0.0516, -0.0673, -0.0068,  0.0613,  0.0002, -0.0844, -0.0843,  0.0559,\n",
      "         -0.0729, -0.0240, -0.0037,  0.0527,  0.0288,  0.0188, -0.0063,  0.0471,\n",
      "          0.0475, -0.0083,  0.0213,  0.0526, -0.0089,  0.0550,  0.0015,  0.0543,\n",
      "          0.0239, -0.0012, -0.0911,  0.0050, -0.0816, -0.0153,  0.0611, -0.0077,\n",
      "         -0.0712, -0.0908,  0.0340,  0.0592,  0.0359,  0.0262,  0.0732,  0.0136,\n",
      "          0.0618,  0.0602,  0.0149,  0.0562,  0.0117, -0.0449, -0.0779, -0.0622,\n",
      "         -0.0885,  0.0279,  0.0037, -0.0118, -0.0054, -0.0421,  0.0438,  0.0614,\n",
      "         -0.0541, -0.0057,  0.0547, -0.0840,  0.0262, -0.0888,  0.0782, -0.0079,\n",
      "         -0.0126,  0.0463, -0.0121, -0.0852,  0.0452, -0.0110,  0.0024,  0.0307,\n",
      "         -0.0542, -0.0035, -0.0206, -0.0890,  0.0068, -0.0894, -0.0935, -0.0295,\n",
      "         -0.0914, -0.0150,  0.0515, -0.0207, -0.0016,  0.0593,  0.0169,  0.0102,\n",
      "          0.0703,  0.0206, -0.0330, -0.0517,  0.0610,  0.0254,  0.0141, -0.0213,\n",
      "         -0.0092,  0.0270,  0.0526,  0.0032,  0.0343, -0.0471,  0.0555, -0.0002,\n",
      "         -0.0031,  0.0594,  0.0546,  0.0623, -0.0065,  0.0036,  0.0613,  0.0518,\n",
      "          0.0599,  0.0153,  0.0564, -0.0739, -0.0036,  0.0334,  0.0584, -0.0891,\n",
      "         -0.0159,  0.0116, -0.0775,  0.0636, -0.0740, -0.0705, -0.0149,  0.0347,\n",
      "          0.0165, -0.0011,  0.0352,  0.0199, -0.0072, -0.0056,  0.0395,  0.0091,\n",
      "         -0.0817,  0.0667, -0.0975, -0.0035,  0.0051, -0.0129, -0.0364,  0.0085,\n",
      "         -0.0412,  0.0596,  0.0652,  0.0120,  0.0598,  0.0217, -0.0210, -0.0527,\n",
      "         -0.0455,  0.0662, -0.0084, -0.0717,  0.0084, -0.0382,  0.0344,  0.0454]],\n",
      "       device='cuda:2', requires_grad=True)\n",
      "clf.l3.bias Parameter containing:\n",
      "tensor([0.0506], device='cuda:2', requires_grad=True)\n",
      "clf.ln1.weight Parameter containing:\n",
      "tensor([0.4885, 0.0835, 0.0859, 0.2820, 0.1061, 0.0835, 0.2024, 0.2524, 0.1146,\n",
      "        0.0812, 0.2565, 0.0899, 0.0825, 0.0805, 0.0811, 0.0840, 0.0981, 0.1420,\n",
      "        0.0797, 0.1321, 0.1270, 0.3389, 0.1401, 0.0959, 0.1774, 0.2897, 0.1166,\n",
      "        0.1235, 0.2321, 0.0846, 0.2981, 0.0818, 0.1197, 0.0964, 0.1281, 0.2771,\n",
      "        0.2012, 0.0800, 0.1461, 0.1307, 0.2378, 0.0920, 0.1190, 0.0816, 0.2791,\n",
      "        0.1497, 0.3030, 0.1746, 0.1575, 0.1856, 0.0817, 0.1599, 0.1068, 0.0989,\n",
      "        0.0794, 0.1607, 0.2145, 0.2388, 0.2545, 0.1696, 0.0802, 0.0871, 0.1859,\n",
      "        0.1270, 0.1925, 0.1115, 0.2466, 0.1691, 0.0902, 0.0829, 0.2260, 0.1087,\n",
      "        0.1775, 0.2441, 0.1597, 0.0915, 0.1878, 0.0921, 0.0905, 0.0882, 0.0814,\n",
      "        0.1388, 0.1787, 0.0947, 0.1447, 0.1543, 0.2707, 0.1649, 0.1111, 0.0920,\n",
      "        0.0788, 0.0842, 0.0874, 0.0861, 0.1212, 0.2106, 0.0988, 0.2612, 0.2612,\n",
      "        0.2086, 0.1909, 0.3320, 0.2325, 0.1554, 0.1861, 0.0799, 0.1725, 0.0875,\n",
      "        0.0840, 0.0878, 0.1345, 0.0940, 0.0841, 0.1240, 0.1603, 0.1386, 0.0839,\n",
      "        0.1733, 0.2036, 0.0792, 0.1299, 0.0831, 0.1425, 0.3366, 0.0863, 0.1048,\n",
      "        0.1164, 0.0771, 0.1345, 0.0871, 0.2311, 0.1958, 0.0866, 0.3882, 0.3141,\n",
      "        0.0875, 0.0883, 0.1254, 0.1898, 0.1271, 0.1099, 0.0875, 0.0809, 0.1037,\n",
      "        0.0991, 0.1801, 0.0784, 0.1029, 0.0788, 0.0881, 0.1750, 0.1324, 0.1192,\n",
      "        0.0874, 0.0881, 0.1137, 0.0960, 0.0914, 0.1067, 0.1271, 0.0864, 0.0876,\n",
      "        0.1013, 0.3188, 0.2228, 0.0790, 0.2633, 0.1009, 0.1029, 0.0861, 0.0783,\n",
      "        0.0793, 0.0820, 0.1146, 0.2524, 0.0888, 0.0784, 0.0881, 0.0837, 0.2262,\n",
      "        0.0793, 0.1045, 0.3463, 0.0964, 0.1598, 0.3154, 0.0855, 0.2653, 0.0813,\n",
      "        0.1014, 0.0781, 0.1718, 0.3025, 0.2582, 0.0840, 0.1313, 0.2034, 0.0878,\n",
      "        0.0966, 0.0788, 0.2897, 0.1963, 0.0915, 0.2661, 0.2896, 0.2141, 0.1275,\n",
      "        0.2404, 0.1575, 0.0979, 0.1638, 0.0971, 0.1030, 0.0930, 0.2070, 0.2338,\n",
      "        0.1253, 0.1047, 0.0942, 0.0804, 0.1341, 0.1672, 0.0978, 0.2950, 0.1147,\n",
      "        0.1047, 0.0877, 0.0934, 0.2081, 0.1991, 0.3000, 0.1254, 0.2078, 0.0876,\n",
      "        0.1085, 0.1331, 0.0871, 0.2876, 0.3114, 0.1462, 0.0875, 0.1195, 0.0796,\n",
      "        0.0973, 0.0950, 0.1446, 0.0809, 0.1116, 0.1443, 0.1731, 0.2296, 0.0900,\n",
      "        0.0795, 0.1201, 0.1576, 0.1874, 0.1139, 0.1327, 0.2275, 0.0779, 0.0913,\n",
      "        0.2138, 0.1169, 0.0921, 0.0782, 0.3003, 0.0811, 0.2680, 0.0944, 0.0970,\n",
      "        0.0780, 0.1527, 0.2985, 0.0786, 0.1055, 0.2094, 0.0843, 0.0810, 0.2442,\n",
      "        0.0841, 0.1255, 0.1263, 0.0857, 0.2833, 0.1908, 0.0934, 0.2520, 0.1283,\n",
      "        0.3391, 0.0921, 0.0974, 0.1347, 0.0830, 0.1407, 0.1134, 0.1197, 0.1272,\n",
      "        0.1362, 0.0848, 0.0860, 0.2066, 0.2552, 0.1605, 0.1260, 0.0809, 0.0944,\n",
      "        0.1047, 0.2904, 0.2560, 0.1792, 0.2761, 0.2392, 0.0858, 0.0797, 0.0770,\n",
      "        0.2035, 0.1221, 0.0790, 0.1513, 0.0946, 0.0803, 0.1438, 0.1395, 0.3091,\n",
      "        0.0989, 0.1595, 0.1305, 0.2161, 0.2100, 0.0978, 0.0899, 0.1651, 0.0899,\n",
      "        0.2415, 0.1065, 0.0925, 0.0837, 0.0931, 0.1292, 0.0846, 0.1962, 0.0827,\n",
      "        0.1165, 0.0865, 0.2262, 0.0988, 0.1424, 0.1966, 0.1159, 0.3116, 0.0875,\n",
      "        0.0930, 0.0955, 0.0839, 0.0867, 0.1056, 0.1104, 0.2004, 0.2334, 0.1136,\n",
      "        0.1145, 0.0790, 0.1810, 0.1659, 0.0792, 0.0902, 0.1512, 0.2910, 0.1186,\n",
      "        0.2201, 0.2140, 0.0825, 0.0850, 0.0859, 0.1559, 0.2586, 0.0840, 0.0953,\n",
      "        0.1070, 0.1121, 0.0783, 0.0860, 0.0793, 0.2392, 0.1377, 0.2244, 0.0879,\n",
      "        0.0872, 0.0919, 0.0927, 0.0955, 0.0786, 0.0829, 0.0908, 0.0897, 0.1843,\n",
      "        0.3394, 0.0876, 0.2028, 0.0904, 0.1068, 0.1631, 0.1771, 0.2902, 0.1314,\n",
      "        0.1037, 0.2122, 0.0779, 0.0844, 0.1314, 0.1213, 0.2795, 0.3319, 0.0862,\n",
      "        0.0791, 0.1068, 0.1024, 0.1236, 0.0898, 0.0802, 0.0797, 0.0849, 0.1327,\n",
      "        0.1551, 0.3144, 0.1591, 0.1251, 0.0896, 0.0851, 0.0785, 0.0881, 0.0784,\n",
      "        0.2773, 0.1060, 0.1022, 0.1032, 0.0790, 0.1090, 0.0849, 0.0789, 0.0786,\n",
      "        0.1041, 0.0853, 0.1369, 0.1136, 0.0931, 0.0871, 0.2722, 0.1050, 0.0811,\n",
      "        0.0918, 0.0844, 0.1665, 0.0859, 0.2054, 0.0786, 0.2612, 0.0897, 0.1444,\n",
      "        0.0972, 0.1138, 0.0827, 0.2220, 0.0838, 0.0803, 0.0807, 0.0776, 0.3147,\n",
      "        0.2241, 0.1376, 0.0814, 0.0904, 0.0779, 0.0803, 0.1854, 0.0939, 0.2806,\n",
      "        0.2829, 0.1623, 0.1253, 0.0960, 0.0831, 0.1577, 0.1044, 0.1254, 0.0960,\n",
      "        0.0914, 0.0922, 0.1847, 0.0921, 0.0832, 0.0858, 0.1152, 0.2426, 0.1834,\n",
      "        0.2030, 0.1081, 0.0968, 0.1086, 0.1794, 0.0991, 0.1084, 0.0801, 0.3531,\n",
      "        0.0905, 0.0815, 0.1108, 0.0830, 0.0796, 0.0779, 0.0921, 0.0958, 0.3026,\n",
      "        0.0930, 0.2608, 0.1508, 0.1359, 0.1100, 0.1678, 0.2025, 0.0781, 0.2237,\n",
      "        0.1296, 0.2896, 0.2599, 0.0989, 0.0796, 0.1009, 0.3806, 0.1277, 0.1996,\n",
      "        0.1456, 0.2750, 0.0805, 0.0845, 0.0879, 0.0916, 0.0795, 0.1023, 0.1277,\n",
      "        0.2293, 0.3937, 0.0781, 0.0793, 0.1006, 0.1167, 0.0800, 0.2538, 0.0868,\n",
      "        0.0954, 0.0869, 0.0785, 0.0793, 0.1468, 0.2246, 0.0780, 0.0910, 0.1482,\n",
      "        0.3032, 0.1668, 0.0886, 0.2910, 0.1035, 0.3042, 0.0785, 0.1085, 0.1987,\n",
      "        0.0890, 0.2320, 0.1380, 0.2244, 0.0838, 0.2342, 0.1342, 0.0814, 0.3489],\n",
      "       device='cuda:2', requires_grad=True)\n",
      "clf.ln1.bias Parameter containing:\n",
      "tensor([-1.7169e-03, -6.4024e-03, -4.0817e-03, -2.8322e-03,  3.0609e-03,\n",
      "         3.2516e-03, -1.0613e-03,  3.8960e-03,  2.5319e-03,  2.4460e-03,\n",
      "         6.5901e-03, -5.4361e-03,  2.6698e-03, -6.1033e-03, -1.8769e-03,\n",
      "         6.0750e-03,  3.7635e-03, -3.1617e-03, -3.6937e-03, -7.2553e-03,\n",
      "         4.7570e-03, -4.8233e-05, -6.2589e-03, -9.6564e-04,  1.5505e-03,\n",
      "        -2.5191e-03,  5.9887e-03, -2.7519e-03,  7.1440e-03, -3.0155e-03,\n",
      "         6.6367e-03, -6.6881e-05,  4.1761e-03, -1.8205e-03,  4.6875e-03,\n",
      "        -1.1505e-03,  4.3006e-03,  1.8032e-03,  2.5827e-03,  1.3467e-03,\n",
      "         6.5443e-03, -3.5968e-03,  5.2514e-03, -4.7872e-03,  4.7710e-03,\n",
      "         8.7511e-03, -3.8376e-03, -1.6151e-03,  2.5726e-03,  9.3679e-03,\n",
      "         4.2489e-03, -2.5855e-03, -1.4595e-03,  2.9561e-03, -1.0671e-03,\n",
      "         6.2592e-03,  1.6030e-03,  7.1540e-03,  1.1825e-03,  2.3114e-03,\n",
      "        -6.8138e-03, -1.2981e-03, -5.2673e-03,  5.1305e-04,  3.7510e-03,\n",
      "         5.6093e-03,  1.9092e-04, -6.9248e-04,  3.6319e-03,  3.5979e-03,\n",
      "        -2.5885e-03, -7.3299e-03,  3.2451e-03,  5.5232e-03,  7.3445e-03,\n",
      "         3.6901e-03,  2.4866e-03,  3.8887e-03,  3.3705e-03, -1.6095e-03,\n",
      "        -2.1066e-03,  6.6740e-03,  3.2104e-03, -7.2744e-04,  3.6151e-03,\n",
      "         3.0407e-03, -4.9732e-03, -5.1959e-03,  3.5279e-03,  2.6547e-03,\n",
      "        -4.0316e-03,  2.9055e-03,  1.8915e-03, -7.1677e-03, -8.4716e-05,\n",
      "         9.9647e-04, -2.2603e-03,  7.2356e-03, -2.5841e-03,  3.7609e-03,\n",
      "         1.2996e-03, -6.0478e-06,  7.6986e-03,  3.9677e-03,  8.7882e-03,\n",
      "        -5.2402e-03, -5.1996e-03,  9.0843e-04, -1.0661e-05, -1.2774e-03,\n",
      "         6.6963e-03, -1.1735e-03,  4.5238e-03, -5.4784e-03, -3.6891e-03,\n",
      "        -9.8194e-04, -4.9928e-04,  1.7495e-03,  7.2891e-03, -2.9155e-03,\n",
      "         2.5630e-03,  9.1983e-04,  3.2941e-03, -1.3372e-03,  1.6226e-03,\n",
      "         3.4274e-03,  5.3412e-04, -2.1217e-03, -1.0208e-03, -5.1439e-03,\n",
      "         2.1468e-03,  2.4617e-03, -4.6370e-03,  1.8262e-03, -1.2361e-03,\n",
      "        -5.6781e-04,  2.2630e-03, -4.1486e-03,  9.3931e-03,  3.4902e-03,\n",
      "        -4.2583e-03, -3.9366e-03, -6.9751e-03,  4.2000e-03,  4.2952e-03,\n",
      "         3.1982e-03,  2.1038e-03,  4.2516e-03,  4.2748e-03,  2.2211e-03,\n",
      "         2.5939e-03, -5.1954e-03,  2.6547e-03, -8.8790e-05, -9.6481e-04,\n",
      "        -6.7785e-03,  5.6276e-04, -4.6442e-03,  4.2841e-03,  3.3749e-04,\n",
      "        -4.4963e-03,  5.3245e-05, -2.6936e-04,  2.6724e-03,  4.5386e-03,\n",
      "         4.3848e-03,  8.9149e-03, -6.7733e-03,  1.3215e-03,  3.2201e-03,\n",
      "         3.4018e-03,  3.5517e-03,  2.5417e-03, -4.6048e-03,  3.2198e-03,\n",
      "        -8.8734e-04,  1.6324e-03, -6.0494e-03, -3.6494e-03,  2.8981e-03,\n",
      "         6.7651e-05, -3.2245e-03,  5.2721e-03, -1.7140e-03,  3.1385e-03,\n",
      "         4.8572e-03, -4.7048e-03,  4.1077e-03, -9.0880e-04, -4.4186e-03,\n",
      "        -1.5650e-03,  5.2427e-03,  4.9873e-03,  7.5581e-03, -4.4017e-03,\n",
      "         2.2968e-03, -4.5359e-03,  1.0032e-03, -4.9931e-03, -2.4281e-03,\n",
      "         2.7529e-03,  3.0492e-03,  3.0366e-03,  8.0719e-03,  3.5544e-03,\n",
      "         7.7903e-03, -3.7275e-03,  3.7900e-03,  2.5442e-03,  5.3166e-03,\n",
      "        -1.9836e-03,  4.8077e-03, -5.5137e-03,  3.6882e-03,  3.8475e-03,\n",
      "         1.6942e-03, -3.0567e-03, -2.7887e-03, -1.7471e-03,  4.2166e-03,\n",
      "        -4.0746e-03,  7.6173e-04,  5.3100e-05,  4.2503e-03, -8.4897e-04,\n",
      "         6.5718e-03,  2.5249e-03, -5.9585e-03, -1.0726e-03, -1.1319e-03,\n",
      "         9.4659e-04, -3.7647e-03,  6.9439e-03, -4.0191e-03,  6.1331e-03,\n",
      "         4.7025e-03,  2.0306e-03,  4.5050e-03,  4.4499e-03,  3.9740e-03,\n",
      "        -3.4693e-03,  5.5257e-03, -5.3991e-04, -5.7186e-03,  7.0201e-05,\n",
      "         4.2586e-03,  4.3470e-03, -1.2227e-03, -4.8707e-03,  6.4680e-03,\n",
      "         6.6786e-03,  2.4905e-03,  3.8261e-04, -5.3732e-03, -4.0731e-03,\n",
      "        -1.8837e-04,  3.2447e-05, -1.2291e-03,  4.1231e-03, -2.1154e-03,\n",
      "        -6.5319e-03,  3.5192e-03,  3.3003e-03,  1.6539e-03, -1.0008e-03,\n",
      "         2.0766e-03,  3.9354e-03,  1.9069e-03,  2.7128e-03,  2.8479e-03,\n",
      "        -1.2695e-03,  4.1526e-03,  1.0440e-03,  6.8113e-04, -3.1886e-03,\n",
      "        -5.3649e-04,  1.1478e-03,  3.9880e-05,  8.8157e-03, -4.4572e-03,\n",
      "        -6.9641e-04, -3.9386e-03,  5.4461e-03,  5.8342e-03,  1.0130e-03,\n",
      "         7.6428e-04,  4.4386e-03,  2.5995e-03,  1.2913e-03,  5.0453e-03,\n",
      "         4.9594e-03, -4.4249e-03,  3.9027e-03,  4.8459e-03,  5.4668e-04,\n",
      "         3.1372e-03,  7.1809e-03,  3.7587e-03, -5.8207e-04, -1.0494e-03,\n",
      "         6.5072e-03,  3.1304e-03,  5.1495e-03, -2.6987e-03,  4.6983e-03,\n",
      "        -5.7414e-03,  3.5624e-03, -1.8211e-03,  2.1574e-03,  5.0386e-03,\n",
      "         4.2335e-03,  6.3784e-03,  4.8858e-03, -8.8545e-04,  2.4251e-03,\n",
      "         5.6907e-03,  4.5334e-03, -3.8192e-03, -9.6119e-04, -3.0815e-03,\n",
      "        -2.6393e-03,  2.9800e-03,  2.5510e-03,  7.3937e-03,  2.3042e-03,\n",
      "         3.8165e-03, -1.4908e-03, -1.6872e-04,  2.9918e-03, -2.7639e-03,\n",
      "        -1.6390e-03,  7.7472e-03,  1.4382e-04,  4.4371e-04, -3.2413e-03,\n",
      "        -1.3548e-03,  3.5655e-03, -3.4944e-03, -6.6127e-04, -1.3404e-03,\n",
      "         4.1042e-03,  5.9579e-03, -4.2710e-03, -3.1435e-03,  3.0984e-03,\n",
      "         3.1176e-03,  3.3186e-03,  1.9791e-03, -4.8484e-04,  3.0187e-03,\n",
      "        -5.8445e-03, -3.8346e-03, -2.4358e-03, -1.5640e-03, -4.3997e-03,\n",
      "         1.8110e-03,  1.2584e-03, -3.4799e-03,  2.3630e-03, -3.4408e-03,\n",
      "         1.3044e-02,  3.4573e-03, -1.0708e-03,  5.2649e-03, -4.3005e-04,\n",
      "         2.7071e-03,  9.0830e-04,  1.0327e-03,  9.0725e-06,  1.0959e-03,\n",
      "         2.4268e-03, -1.5082e-03,  3.5193e-03, -7.7678e-04,  7.0963e-03,\n",
      "        -9.5247e-04, -2.7094e-03, -5.6585e-03,  2.4441e-03,  1.9781e-04,\n",
      "        -1.5814e-03, -7.4467e-04, -3.4565e-03,  1.2991e-02,  2.8941e-03,\n",
      "         4.8483e-04, -6.5297e-03,  2.9045e-03,  1.1490e-05, -1.5250e-03,\n",
      "         5.3740e-03, -4.5160e-03, -3.1390e-03,  2.5321e-03, -4.8296e-03,\n",
      "         3.1942e-03,  3.2589e-03, -4.5805e-03,  1.0853e-03, -6.0256e-04,\n",
      "         9.3623e-04, -5.7330e-04,  3.1469e-03,  1.1394e-03, -1.0681e-02,\n",
      "         5.1593e-03,  5.1299e-03,  4.1923e-04, -5.0436e-03, -1.2714e-03,\n",
      "        -4.5662e-03,  3.7948e-03,  5.1192e-03,  4.3405e-03,  1.5547e-03,\n",
      "         8.0495e-04, -5.4770e-03, -4.3682e-03,  1.2135e-04,  3.1228e-03,\n",
      "         4.3030e-03, -3.1620e-03,  2.2579e-03, -5.1392e-03,  7.3365e-03,\n",
      "         3.1072e-03,  1.6420e-03,  3.2801e-03, -3.6090e-03, -3.8053e-03,\n",
      "        -1.0366e-03,  3.6422e-03,  7.0699e-03,  6.3525e-03,  4.7306e-03,\n",
      "         6.0049e-03, -6.4592e-04,  4.0021e-03, -3.1342e-03, -2.0982e-03,\n",
      "         4.4884e-03, -1.2862e-03, -5.0746e-03,  7.1182e-03, -4.2060e-03,\n",
      "         3.5041e-03,  2.7183e-03,  4.4076e-03,  1.3522e-03,  2.4042e-03,\n",
      "        -3.8380e-03,  2.8803e-03, -3.7142e-04, -4.4956e-03, -5.9256e-04,\n",
      "        -1.0589e-03,  6.0239e-03,  5.6488e-04,  2.3930e-03,  4.0433e-03,\n",
      "         4.2737e-03,  1.3197e-03,  2.2189e-03,  6.3790e-03, -5.9200e-03,\n",
      "        -4.8676e-03, -6.4263e-03,  3.3313e-03,  6.9326e-04, -1.1633e-03,\n",
      "        -5.4137e-04, -8.2302e-03,  2.3488e-03,  1.7001e-04,  5.1193e-03,\n",
      "        -2.3022e-03, -7.6225e-05,  4.5208e-03,  9.5531e-04,  2.5367e-03,\n",
      "        -1.0990e-03, -4.8384e-03,  5.1225e-04,  3.1121e-03, -7.0463e-03,\n",
      "        -2.8990e-03, -3.7989e-03, -5.1725e-03,  3.5360e-03, -2.7052e-03,\n",
      "         4.9766e-03,  4.2025e-03,  4.0796e-03,  5.3245e-03,  2.5167e-03,\n",
      "         3.8535e-03,  7.1950e-03, -5.4333e-03,  2.0426e-03,  3.9677e-03,\n",
      "        -2.2897e-03, -4.9444e-03, -2.2634e-03,  6.0194e-03,  2.8302e-03,\n",
      "         6.3532e-04, -1.2417e-03,  4.9555e-04, -4.2937e-03, -5.2242e-03,\n",
      "        -1.8925e-03, -4.3446e-03,  3.9666e-04, -3.2465e-03,  1.5074e-03,\n",
      "         1.9837e-03, -2.7517e-03, -2.3451e-03, -4.3494e-03,  6.8762e-03,\n",
      "        -4.5205e-03,  6.0533e-03, -6.8286e-03,  8.1998e-03, -1.4872e-03,\n",
      "        -5.4506e-03,  3.6701e-03, -6.1596e-03,  1.5351e-03, -4.6115e-03,\n",
      "         2.8454e-03,  2.8644e-03,  9.9247e-04, -6.3254e-03,  1.4656e-03,\n",
      "        -1.6860e-03,  3.7626e-03, -4.1496e-04, -8.4843e-05, -3.2065e-03,\n",
      "         7.8444e-03, -3.4443e-04,  2.8216e-03,  3.0704e-04,  2.1233e-03,\n",
      "        -3.8907e-03, -3.5005e-03, -2.9644e-03,  2.5672e-03,  1.0615e-03,\n",
      "         3.7628e-03, -5.2772e-04, -1.9884e-03,  8.6778e-05,  4.4170e-03,\n",
      "        -1.9148e-03,  4.6962e-03, -3.5299e-03,  3.6498e-03, -2.0560e-03,\n",
      "         2.1988e-03, -1.7963e-03, -3.8985e-03,  6.3822e-03,  5.5111e-03,\n",
      "         8.5996e-04,  7.6556e-04,  3.1649e-03,  3.7718e-03, -5.1426e-03,\n",
      "        -2.0823e-03,  1.9926e-03,  3.6202e-03, -3.0058e-03, -2.9444e-03,\n",
      "         4.6843e-03], device='cuda:2', requires_grad=True)\n",
      "clf.ln2.weight Parameter containing:\n",
      "tensor([0.2796, 0.0954, 0.4265, 0.1609, 0.3313, 0.4390, 0.2942, 0.2715, 0.1213,\n",
      "        0.1891, 0.1178, 0.3941, 0.5056, 0.3772, 0.1263, 0.3743, 0.0734, 0.0717,\n",
      "        0.0783, 0.0749, 0.0964, 0.2928, 0.1136, 0.0768, 0.1447, 0.3933, 0.3830,\n",
      "        0.3155, 0.3445, 0.3710, 0.0796, 0.3368, 0.2142, 0.1085, 0.3591, 0.1859,\n",
      "        0.1571, 0.5300, 0.0776, 0.3970, 0.3845, 0.4430, 0.1633, 0.0978, 0.1248,\n",
      "        0.3566, 0.0932, 0.3729, 0.2815, 0.1733, 0.4386, 0.3913, 0.2762, 0.3380,\n",
      "        0.4702, 0.3958, 0.0947, 0.1041, 0.1019, 0.3219, 0.3020, 0.1112, 0.0717,\n",
      "        0.1189, 0.3092, 0.4183, 0.3527, 0.5100, 0.3606, 0.3081, 0.2157, 0.1388,\n",
      "        0.0749, 0.4343, 0.4745, 0.1865, 0.3904, 0.1216, 0.0828, 0.0791, 0.2571,\n",
      "        0.3819, 0.5330, 0.2543, 0.4532, 0.0803, 0.3566, 0.2743, 0.3738, 0.3410,\n",
      "        0.0810, 0.5131, 0.1525, 0.3166, 0.4280, 0.3783, 0.1495, 0.1600, 0.0946,\n",
      "        0.4698, 0.4513, 0.0926, 0.3524, 0.0723, 0.1333, 0.3168, 0.0842, 0.1972,\n",
      "        0.0896, 0.0841, 0.1233, 0.2914, 0.5222, 0.2194, 0.1807, 0.4916, 0.0872,\n",
      "        0.2586, 0.1082, 0.3350, 0.0832, 0.3584, 0.3740, 0.4248, 0.0868, 0.3411,\n",
      "        0.0967, 0.3199, 0.4523, 0.3608, 0.1061, 0.5352, 0.0742, 0.4125, 0.4094,\n",
      "        0.4598, 0.3994, 0.1154, 0.0752, 0.3332, 0.1283, 0.1459, 0.0968, 0.2157,\n",
      "        0.3495, 0.1055, 0.1095, 0.3881, 0.1229, 0.4325, 0.0712, 0.4474, 0.1243,\n",
      "        0.0710, 0.4843, 0.0800, 0.4143, 0.1442, 0.3430, 0.0994, 0.4646, 0.4237,\n",
      "        0.2148, 0.3885, 0.1482, 0.1796, 0.4566, 0.0874, 0.4727, 0.4663, 0.0759,\n",
      "        0.4996, 0.0917, 0.3147, 0.4274, 0.4060, 0.3569, 0.2084, 0.0804, 0.1217,\n",
      "        0.0790, 0.1466, 0.2699, 0.4170, 0.3039, 0.0777, 0.3582, 0.4439, 0.1322,\n",
      "        0.3979, 0.4504, 0.1163, 0.0853, 0.2506, 0.1390, 0.4727, 0.3209, 0.0801,\n",
      "        0.0748, 0.1290, 0.2817, 0.0735, 0.1027, 0.4545, 0.0828, 0.4346, 0.3820,\n",
      "        0.1778, 0.4665, 0.1596, 0.2425, 0.0981, 0.0735, 0.3606, 0.1045, 0.0822,\n",
      "        0.3573, 0.1226, 0.1432, 0.2540, 0.3489, 0.1628, 0.0968, 0.1333, 0.0766,\n",
      "        0.1218, 0.4390, 0.0801, 0.1938, 0.1913, 0.3241, 0.0703, 0.0825, 0.3507,\n",
      "        0.3407, 0.2676, 0.0745, 0.0838, 0.2710, 0.2050, 0.4977, 0.1206, 0.4114,\n",
      "        0.5316, 0.0782, 0.1519, 0.3468, 0.4257, 0.1165, 0.0803, 0.4601, 0.4003,\n",
      "        0.4361, 0.5037, 0.0938, 0.2549, 0.1053, 0.0733, 0.2155, 0.1454, 0.0845,\n",
      "        0.0813, 0.2084, 0.0767, 0.3273, 0.4133, 0.3741, 0.0809, 0.0724, 0.0776,\n",
      "        0.1290, 0.0898, 0.2151, 0.4919, 0.3290, 0.1039, 0.4182, 0.1239, 0.1496,\n",
      "        0.2020, 0.1776, 0.4464, 0.0955, 0.4811, 0.0907, 0.1728, 0.1848, 0.3779],\n",
      "       device='cuda:2', requires_grad=True)\n",
      "clf.ln2.bias Parameter containing:\n",
      "tensor([ 8.6169e-03,  2.9158e-04,  1.3521e-02,  6.4053e-03,  1.0579e-02,\n",
      "         5.8642e-03,  4.4049e-03,  6.8116e-03, -2.4615e-03,  2.7833e-03,\n",
      "         3.0098e-03,  1.4066e-02,  7.0938e-03,  7.3184e-03,  5.2483e-03,\n",
      "         2.3079e-02, -1.6376e-03, -1.3197e-04,  3.2767e-03, -4.6463e-04,\n",
      "         3.5050e-03,  7.5110e-03, -7.2622e-05,  2.4044e-03,  1.2425e-02,\n",
      "         2.9051e-03,  6.0351e-03,  8.7719e-03,  1.8175e-02,  1.4225e-02,\n",
      "        -1.1901e-03,  1.6341e-02,  3.8712e-03,  1.8694e-04,  4.7292e-03,\n",
      "         2.9701e-03,  2.2112e-03,  8.7568e-03, -1.1613e-04,  1.2618e-02,\n",
      "         1.0165e-02,  5.5450e-03, -8.9889e-04, -9.3869e-04,  1.1939e-03,\n",
      "         1.5697e-02, -1.7809e-03,  2.6342e-02,  8.7329e-03,  1.5549e-03,\n",
      "         1.6926e-02,  1.4787e-02,  1.8020e-02,  2.1513e-02,  1.9330e-02,\n",
      "         6.0448e-03,  3.8052e-04,  2.2868e-05,  3.0641e-03,  1.2943e-02,\n",
      "         8.8392e-03,  7.1072e-04,  7.1078e-04,  9.8074e-04,  8.2485e-03,\n",
      "         1.7356e-02,  7.7785e-03,  1.0131e-02,  2.8876e-02,  9.2524e-03,\n",
      "         3.3684e-04,  5.9845e-03,  2.2527e-03,  9.7945e-03,  7.9807e-03,\n",
      "         4.6242e-04,  1.2643e-02, -7.1027e-04,  1.5094e-03, -2.5191e-03,\n",
      "         6.5175e-03,  1.8476e-02,  9.0412e-03,  1.4186e-02,  7.4216e-03,\n",
      "        -2.7297e-03,  1.0920e-02,  1.0229e-02,  8.2256e-04,  1.7135e-02,\n",
      "         6.2551e-05,  6.9766e-03,  7.7284e-03,  1.8904e-02,  2.2586e-02,\n",
      "         2.7122e-02, -2.0892e-04, -2.3299e-03, -1.4796e-04,  1.0291e-02,\n",
      "         1.7326e-02,  1.2612e-03,  1.9452e-02, -7.7772e-04, -5.8827e-04,\n",
      "         1.4405e-02, -8.3631e-04,  1.2823e-02, -2.4092e-03, -6.1532e-03,\n",
      "         5.0125e-03,  8.3509e-03,  3.9538e-03,  7.5956e-03,  3.5327e-03,\n",
      "         9.2519e-03,  2.0568e-04,  1.1161e-02,  1.6302e-03,  1.0405e-02,\n",
      "         9.7594e-05,  1.0746e-02,  2.9203e-03,  7.6575e-03,  3.4200e-04,\n",
      "         1.1090e-02,  7.7512e-04,  3.5358e-03,  1.1309e-02,  1.4584e-02,\n",
      "        -8.5288e-04,  6.5821e-03, -5.9907e-04,  2.5361e-02,  1.7141e-02,\n",
      "         9.5432e-03,  1.4009e-02, -1.0661e-03,  3.0729e-03,  1.2107e-02,\n",
      "         4.0797e-03,  2.9660e-03, -2.0776e-04,  9.7255e-03,  1.0893e-02,\n",
      "        -3.6015e-03, -1.6041e-03,  1.7171e-03,  1.0903e-04,  1.1590e-02,\n",
      "        -1.1382e-03,  4.8394e-03,  1.3899e-03, -1.5115e-03,  1.2071e-02,\n",
      "        -2.4485e-03,  2.1414e-02,  3.6034e-03,  9.2700e-03,  1.0866e-03,\n",
      "         9.8533e-03,  1.4794e-02,  9.0513e-03,  1.1358e-02,  5.4163e-03,\n",
      "         8.0557e-04,  1.2088e-02,  3.3856e-04,  2.3726e-03,  4.6063e-03,\n",
      "         2.3272e-03,  8.8741e-03, -3.0413e-03,  1.2576e-02,  1.0176e-02,\n",
      "         8.5202e-03,  3.0240e-02,  7.2848e-03, -1.4583e-03,  5.6529e-04,\n",
      "         1.3586e-03,  6.2066e-03,  3.4451e-03,  6.4599e-03,  1.3158e-02,\n",
      "         3.8068e-03,  1.4265e-02,  2.0656e-02,  1.8939e-03,  1.9455e-02,\n",
      "         1.5566e-02, -5.0603e-04, -3.1872e-04,  1.1708e-02,  3.2021e-04,\n",
      "         1.4296e-02,  1.2144e-02,  2.5228e-04, -1.0568e-03,  3.8168e-03,\n",
      "         8.5489e-03, -1.6990e-03, -1.0559e-03,  1.3639e-02, -8.5688e-04,\n",
      "         9.2560e-03,  1.7919e-02,  6.0917e-03,  1.7188e-02, -2.4522e-03,\n",
      "         1.1667e-02, -5.1120e-03, -3.2488e-04,  9.0676e-03,  2.4016e-03,\n",
      "        -4.6788e-04,  1.2370e-02,  3.5706e-03,  2.3372e-03,  1.1298e-02,\n",
      "         1.3010e-02,  4.1725e-03, -1.0658e-03,  6.4803e-04,  2.0543e-03,\n",
      "         6.9118e-04,  5.4502e-03, -9.5041e-04, -5.7196e-03,  7.5998e-03,\n",
      "         1.0888e-02, -1.9110e-03,  2.8987e-03,  5.6721e-03,  8.4877e-03,\n",
      "         1.1040e-02,  1.7315e-04,  2.7616e-04,  1.1907e-02,  1.5554e-02,\n",
      "         3.8336e-03,  5.1500e-04,  1.3497e-02,  9.1580e-03,  3.4520e-03,\n",
      "         4.0590e-03,  1.6146e-02,  1.8940e-02, -1.0306e-03,  1.0368e-03,\n",
      "         1.6371e-02,  7.7471e-03,  1.2469e-02,  1.2390e-02,  3.4519e-03,\n",
      "         2.1271e-03,  1.8929e-03, -3.8818e-05,  8.1168e-03,  4.8030e-03,\n",
      "         1.6059e-03,  1.2390e-03,  9.9383e-03,  9.3789e-05,  2.1970e-02,\n",
      "         1.2487e-02,  1.8115e-02,  1.4796e-03, -1.0992e-03, -8.6139e-04,\n",
      "        -3.5361e-03,  1.1325e-03,  1.2623e-02,  9.9336e-03,  1.2327e-02,\n",
      "         3.6923e-03,  1.2138e-02,  4.3567e-03, -5.2354e-05,  5.5097e-03,\n",
      "         5.2499e-03,  7.1194e-03, -3.0409e-03,  1.6723e-02, -4.5372e-04,\n",
      "        -1.2470e-03,  1.0979e-02,  1.0922e-02], device='cuda:2',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i, j in model.named_parameters():\n",
    "    if j.requires_grad:\n",
    "        print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def listMLE(y_pred, y_true, eps=1e-8, padded_value_indicator=0):\n",
    "    \"\"\"\n",
    "    ListMLE loss introduced in \"Listwise Approach to Learning to Rank - Theory and Algorithm\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param eps: epsilon value, used for numerical stability\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    # shuffle for randomised tie resolution\n",
    "    random_indices = torch.randperm(y_pred.shape[-1])\n",
    "    y_pred_shuffled = y_pred[:, random_indices]\n",
    "    y_true_shuffled = y_true[:, random_indices]\n",
    "\n",
    "    y_true_sorted, indices = y_true_shuffled.sort(descending=True, dim=-1)\n",
    "\n",
    "    mask = y_true_sorted == padded_value_indicator\n",
    "\n",
    "    preds_sorted_by_true = torch.gather(y_pred_shuffled, dim=1, index=indices)\n",
    "    preds_sorted_by_true[mask] = float(\"-inf\")\n",
    "\n",
    "    max_pred_values, _ = preds_sorted_by_true.max(dim=1, keepdim=True)\n",
    "\n",
    "    preds_sorted_by_true_minus_max = preds_sorted_by_true - max_pred_values\n",
    "\n",
    "    cumsums = torch.cumsum(preds_sorted_by_true_minus_max.exp().flip(dims=[1]), dim=1).flip(dims=[1])\n",
    "\n",
    "    observation_loss = torch.log(cumsums + eps) - preds_sorted_by_true_minus_max\n",
    "\n",
    "    observation_loss[mask] = 0.0\n",
    "\n",
    "    return torch.mean(torch.sum(observation_loss, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6818,  1.1132,  0.5710,  1.1020, -1.1299]) tensor([ 1.6818,  1.1132,  0.5710,  1.1020, -1.1299])\n"
     ]
    }
   ],
   "source": [
    "gelu = nn.Identity()\n",
    "ln = nn.LayerNorm(5)\n",
    "b = gelu(a)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0510,  0.4618, -0.1001,  0.4501, -1.8627],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln(a*10+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.softmax(out)\n",
    "        return out\n",
    "model = NN(50, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0780], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.train()\n",
    "model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Linearcls(256, p0=0.2, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8889)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1]).unsqueeze(0)\n",
    "b = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5]).unsqueeze(0)\n",
    "listMLE(b, torch.tensor(range(5)).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(85, 50) \n",
    "b = torch.tensor(range(85)).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "         14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
       "         28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
       "         42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
       "         56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
       "         70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.,\n",
       "         84.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cri = models.ListMLE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(291.3251), tensor(295.7650))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([ 3.5803e-04,  2.1979e-03,  3.8311e-04,  5.2792e-04,  4.8840e-04,\n",
    "         1.0956e-03,  4.0375e-04,  2.1282e-03, -1.0719e-03,  2.1690e-03,\n",
    "         3.5637e-04,  7.8660e-04, -1.7349e-04,  1.1136e-03,  1.4135e-03,\n",
    "        -5.7234e-05, -1.9069e-04,  1.5373e-03, -1.5336e-04,  5.0368e-04,\n",
    "         1.0354e-03,  9.4463e-04,  1.5163e-03,  1.9483e-03,  6.8653e-04,\n",
    "         1.1449e-03,  1.4879e-04,  3.3806e-04,  1.5077e-03,  9.2047e-04,\n",
    "         2.3447e-03,  2.2386e-03,  1.4788e-03,  1.7258e-03, -2.6664e-04,\n",
    "        -1.5059e-04,  6.0289e-04, -6.1682e-04,  7.1805e-04,  1.0268e-03,\n",
    "         2.7502e-03, -7.3034e-04, -8.2444e-05, -7.1462e-04,  1.1572e-03,\n",
    "         7.2149e-04,  6.2846e-04,  1.6673e-03,  1.2135e-03,  1.4576e-03,\n",
    "         1.4565e-03,  1.9022e-03, -4.4323e-04,  8.4509e-04,  8.5283e-04,\n",
    "         3.7751e-04,  1.6441e-03,  7.9547e-04, -1.8599e-04,  2.2232e-04,\n",
    "         9.0810e-04,  1.7102e-03,  5.1223e-04,  7.4762e-05, -5.0921e-04,\n",
    "         2.4580e-03,  1.3834e-03,  1.8816e-04,  1.7089e-03,  1.1783e-03,\n",
    "         1.0902e-03,  9.6748e-06,  7.7385e-04,  2.1562e-03,  8.9369e-04,\n",
    "         1.0644e-03,  4.2991e-05,  1.5249e-03,  2.2387e-03, -1.2712e-03,\n",
    "         3.6258e-04, -3.7134e-04,  2.8287e-04,  2.0146e-03,  2.1470e-03])\n",
    "listMLE(t.unsqueeze(0), torch.tensor(range(len(t))).unsqueeze(0)), cri(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(298.6251, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cri(model(a).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1719],\n",
       "        [ 0.0232],\n",
       "        [ 0.1478],\n",
       "        [-0.3223],\n",
       "        [ 0.0409],\n",
       "        [-0.2715],\n",
       "        [-0.0988],\n",
       "        [-0.1593],\n",
       "        [ 0.0015],\n",
       "        [-0.4033],\n",
       "        [-0.4174],\n",
       "        [-0.4033],\n",
       "        [-0.2422],\n",
       "        [-0.1940],\n",
       "        [ 0.3468],\n",
       "        [-0.2545],\n",
       "        [-0.1316],\n",
       "        [-0.1732],\n",
       "        [-0.2200],\n",
       "        [-0.3019],\n",
       "        [-0.0202],\n",
       "        [-0.2633],\n",
       "        [-0.2919],\n",
       "        [-0.0913],\n",
       "        [-0.1816],\n",
       "        [ 0.0142],\n",
       "        [ 0.0456],\n",
       "        [-0.0702],\n",
       "        [-0.4243],\n",
       "        [-0.2370],\n",
       "        [-0.1419],\n",
       "        [-0.3999],\n",
       "        [-0.2288],\n",
       "        [-0.2057],\n",
       "        [-0.1580],\n",
       "        [-0.1026],\n",
       "        [-0.0859],\n",
       "        [-0.1071],\n",
       "        [-0.1817],\n",
       "        [-0.0507],\n",
       "        [-0.0473],\n",
       "        [-0.0468],\n",
       "        [-0.3241],\n",
       "        [ 0.0374],\n",
       "        [ 0.0046],\n",
       "        [-0.1215],\n",
       "        [-0.2482],\n",
       "        [-0.2893],\n",
       "        [-0.1320],\n",
       "        [ 0.1539],\n",
       "        [-0.1502],\n",
       "        [ 0.0855],\n",
       "        [ 0.0929],\n",
       "        [-0.1974],\n",
       "        [-0.4818],\n",
       "        [ 0.1231],\n",
       "        [-0.2769],\n",
       "        [-0.2654],\n",
       "        [-0.2120],\n",
       "        [-0.0330],\n",
       "        [-0.2568],\n",
       "        [ 0.1294],\n",
       "        [ 0.0472],\n",
       "        [-0.3249],\n",
       "        [-0.1694],\n",
       "        [ 0.0586],\n",
       "        [-0.2214],\n",
       "        [ 0.0786],\n",
       "        [-0.4206],\n",
       "        [-0.2201],\n",
       "        [-0.1833],\n",
       "        [-0.4648],\n",
       "        [-0.2233],\n",
       "        [-0.4330],\n",
       "        [-0.0648],\n",
       "        [-0.1225],\n",
       "        [ 0.0961],\n",
       "        [ 0.1179],\n",
       "        [ 0.0008],\n",
       "        [ 0.4616],\n",
       "        [ 0.3030],\n",
       "        [ 0.0882],\n",
       "        [-0.0654],\n",
       "        [-0.0729],\n",
       "        [-0.0837]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = embeds.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameter containing:\n",
      "tensor([[-0.0279,  0.0043,  0.1825,  ..., -0.1534, -0.1215,  0.2763],\n",
      "        [ 0.0121, -0.1111,  0.1724,  ..., -0.3070,  0.0582, -0.0786],\n",
      "        [-0.0142, -0.1625,  0.2000,  ..., -0.0525,  0.0381, -0.1630],\n",
      "        ...,\n",
      "        [-0.3378,  0.1492,  0.2998,  ...,  0.2824, -0.3519, -0.1671],\n",
      "        [-0.2589, -0.0412,  0.0214,  ..., -0.1667,  0.1242,  0.2417],\n",
      "        [ 0.3284, -0.1897, -0.0264,  ..., -0.1723,  0.2315,  0.2736]],\n",
      "       requires_grad=True)\n",
      "fc1.bias Parameter containing:\n",
      "tensor([ 0.1124,  0.0783,  0.1414,  0.0805,  0.1332,  0.1182,  0.2013,  0.2869,\n",
      "         0.1947,  0.0760, -0.1375,  0.2667,  0.0584,  0.1275,  0.1456,  0.2045,\n",
      "         0.1409,  0.1069,  0.3160,  0.1832,  0.2036,  0.0835,  0.2350,  0.1788,\n",
      "        -0.0908,  0.0475,  0.2332,  0.0854,  0.1001,  0.0647, -0.0127,  0.0817,\n",
      "         0.2769,  0.1208,  0.2200,  0.2132,  0.1891,  0.0272,  0.1092,  0.1077,\n",
      "         0.2005,  0.0364,  0.1362,  0.1097,  0.1636,  0.1569,  0.2000,  0.1451,\n",
      "         0.0858,  0.2234], requires_grad=True)\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[ 0.2545,  0.2311,  0.2603, -0.3232,  0.2515, -0.3314, -0.3267, -0.2937,\n",
      "         -0.2908,  0.2782, -0.2810, -0.2548,  0.2348, -0.3202, -0.2792, -0.2576,\n",
      "          0.2600, -0.3757,  0.2349, -0.2800, -0.2465, -0.2680,  0.2302,  0.2947,\n",
      "         -0.2286,  0.3445, -0.2728,  0.3110, -0.2567,  0.2673,  0.2449,  0.3360,\n",
      "         -0.2211,  0.2972, -0.2319, -0.2303,  0.2409, -0.3065, -0.2692,  0.2429,\n",
      "          0.2623, -0.2791, -0.2888,  0.2004, -0.3673,  0.3290,  0.2782, -0.2588,\n",
      "          0.3149, -0.2296]], requires_grad=True)\n",
      "fc2.bias Parameter containing:\n",
      "tensor([-0.0389], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i, j in model.named_parameters():\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291.46185302734375\n",
      "284.89581298828125\n",
      "278.4154968261719\n",
      "271.0216064453125\n",
      "262.43304443359375\n",
      "252.792724609375\n",
      "242.41966247558594\n",
      "231.54661560058594\n",
      "220.30198669433594\n",
      "208.88369750976562\n",
      "197.58184814453125\n",
      "186.66175842285156\n",
      "176.27749633789062\n",
      "166.52468872070312\n",
      "157.51181030273438\n",
      "149.46783447265625\n",
      "142.9174041748047\n",
      "138.11264038085938\n",
      "134.81072998046875\n",
      "132.6311492919922\n",
      "131.20948791503906\n",
      "130.210205078125\n",
      "129.5453338623047\n",
      "129.45982360839844\n",
      "129.21636962890625\n",
      "129.2898712158203\n",
      "129.11300659179688\n",
      "129.08448791503906\n",
      "128.95175170898438\n",
      "128.96481323242188\n",
      "128.9128875732422\n",
      "128.9095458984375\n",
      "128.8632354736328\n",
      "128.71414184570312\n",
      "128.7430419921875\n",
      "128.76937866210938\n",
      "128.89527893066406\n",
      "128.8540802001953\n",
      "128.66522216796875\n",
      "128.8581085205078\n",
      "128.639404296875\n",
      "128.7723388671875\n",
      "128.77435302734375\n",
      "128.9160919189453\n",
      "128.75860595703125\n",
      "128.9180145263672\n",
      "128.73268127441406\n",
      "128.80215454101562\n",
      "128.7133026123047\n",
      "128.67440795898438\n",
      "128.78245544433594\n",
      "128.760498046875\n",
      "128.78074645996094\n",
      "128.86822509765625\n",
      "128.73895263671875\n",
      "128.859375\n",
      "128.92279052734375\n",
      "128.66078186035156\n",
      "128.7006378173828\n",
      "128.64401245117188\n",
      "128.85919189453125\n",
      "128.6962127685547\n",
      "128.80677795410156\n",
      "128.72918701171875\n",
      "128.62661743164062\n",
      "128.77085876464844\n",
      "128.74526977539062\n",
      "128.74700927734375\n",
      "128.70864868164062\n",
      "128.70159912109375\n",
      "129.0113525390625\n",
      "128.81886291503906\n",
      "128.9272003173828\n",
      "128.7576904296875\n",
      "128.71527099609375\n",
      "128.78073120117188\n",
      "128.6793975830078\n",
      "128.6883544921875\n",
      "128.7140655517578\n",
      "128.74676513671875\n",
      "128.64813232421875\n",
      "128.77279663085938\n",
      "128.8035430908203\n",
      "128.84129333496094\n",
      "128.6123046875\n",
      "128.8085174560547\n",
      "128.74533081054688\n",
      "128.7650604248047\n",
      "128.79971313476562\n",
      "128.85067749023438\n",
      "128.8571014404297\n",
      "128.74110412597656\n",
      "128.7857666015625\n",
      "128.66986083984375\n",
      "128.7969207763672\n",
      "128.72938537597656\n",
      "128.68984985351562\n",
      "128.804443359375\n",
      "128.81138610839844\n",
      "128.7781524658203\n"
     ]
    }
   ],
   "source": [
    "q = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for i in range(1000):\n",
    "    q.zero_grad()\n",
    "    # loss = cri(model(embeds).squeeze(1))\n",
    "    loss = listMLE(model(a).T, b)\n",
    "    loss.backward()\n",
    "    q.step()\n",
    "    if i % 10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 0.3624, -0.0180,  0.0869,  ...,  0.1210, -0.1889, -0.0382],\n",
       "           [ 0.3376, -0.0175,  0.0706,  ...,  0.1113, -0.1621, -0.0224],\n",
       "           [ 0.3828, -0.0033,  0.1014,  ...,  0.1104, -0.1883, -0.0070],\n",
       "           ...,\n",
       "           [-0.0309,  0.0461,  0.0257,  ..., -0.0556,  0.0204, -0.0142],\n",
       "           [ 0.4842, -0.0369,  0.1025,  ...,  0.1440, -0.2434, -0.0419],\n",
       "           [-0.2140,  0.0103, -0.0538,  ..., -0.0549,  0.1110,  0.0041]]),\n",
       "   'exp_avg_sq': tensor([[1.0088, 0.0942, 0.1916,  ..., 0.3879, 0.3529, 0.7078],\n",
       "           [0.7957, 0.0746, 0.1558,  ..., 0.2263, 0.2691, 0.4234],\n",
       "           [1.1089, 0.1546, 0.1994,  ..., 0.2568, 0.3463, 0.0579],\n",
       "           ...,\n",
       "           [0.1074, 0.0681, 0.0455,  ..., 0.2289, 0.0395, 0.0111],\n",
       "           [1.5793, 0.1695, 0.3196,  ..., 0.3848, 0.5516, 1.0559],\n",
       "           [0.2913, 0.0523, 0.0579,  ..., 0.0755, 0.1181, 0.4904]])},\n",
       "  1: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([ 0.0069,  0.0052,  0.0127, -0.1112, -0.0652, -0.0453, -0.0911, -0.1255,\n",
       "            0.0358, -0.0899,  0.0347, -0.0620, -0.0977,  0.0174,  0.0597,  0.0250,\n",
       "           -0.0276,  0.0238,  0.0248, -0.0551, -0.0652,  0.0729, -0.0017,  0.0102,\n",
       "            0.0685, -0.0421,  0.0896,  0.0148, -0.0068, -0.0157, -0.0853,  0.0101,\n",
       "            0.0832,  0.0143,  0.0782, -0.0331, -0.0130, -0.0893, -0.0206,  0.0164,\n",
       "           -0.0094,  0.0115, -0.0072,  0.0067, -0.0828,  0.0284,  0.0719,  0.0579,\n",
       "           -0.0213,  0.0159]),\n",
       "   'exp_avg_sq': tensor([0.0061, 0.0024, 0.1150, 0.1245, 0.1282, 0.1059, 0.1087, 0.1186, 0.0580,\n",
       "           0.1812, 0.0932, 0.0654, 0.1228, 0.1099, 0.0671, 0.0631, 0.1228, 0.1487,\n",
       "           0.0180, 0.0710, 0.0681, 0.0917, 0.0940, 0.1659, 0.0635, 0.1421, 0.0756,\n",
       "           0.0816, 0.1179, 0.1629, 0.1411, 0.0700, 0.0722, 0.0092, 0.0793, 0.0200,\n",
       "           0.1081, 0.0671, 0.0659, 0.0084, 0.0628, 0.0830, 0.1064, 0.0041, 0.1091,\n",
       "           0.0100, 0.1588, 0.0663, 0.0287, 0.0551])},\n",
       "  2: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 0.1256,  0.2336,  0.5462,  0.2878, -0.5607, -0.0281,  0.2868,  0.4345,\n",
       "            -0.1415, -0.3748, -0.1721,  0.1174, -0.5509,  0.0542, -0.3491, -0.1950,\n",
       "            -0.1054, -0.0547,  0.1461,  0.1179,  0.2531, -0.1737, -0.1319,  0.3002,\n",
       "            -0.1792, -0.6011, -0.1583,  0.1950,  0.1198, -0.2699, -0.6696,  0.3225,\n",
       "            -0.5121, -0.2529, -0.5007,  0.1117,  0.3253,  0.1989,  0.0963, -0.1695,\n",
       "            -0.2552,  0.0141,  0.0456, -0.1138,  0.1909, -0.6597,  0.9257, -0.0575,\n",
       "            -0.3938, -0.0968]]),\n",
       "   'exp_avg_sq': tensor([[ 9.4297, 15.6054,  8.4876,  2.3040, 17.2553,  3.1336,  2.1094,  3.5121,\n",
       "             2.3842,  9.2341,  5.0681,  2.1786, 19.3084,  4.7234,  3.1900,  4.7682,\n",
       "             3.7419,  3.0520,  7.5869,  1.0606,  3.4148,  1.8034,  7.6449, 12.3677,\n",
       "             1.2048, 10.1314,  2.7887,  5.1834,  3.6890, 11.8587,  7.3933,  7.3670,\n",
       "             5.9031,  3.6280,  6.9617,  1.5081, 10.4545,  1.5974,  2.1161,  8.3644,\n",
       "             8.1725,  3.2835,  2.8401,  9.5150,  2.1405,  5.2129, 18.6755,  5.0030,\n",
       "             8.8290,  5.2571]])},\n",
       "  3: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([-3.2321e-08]),\n",
       "   'exp_avg_sq': tensor([4.0164e-14])}},\n",
       " 'param_groups': [{'lr': 0.001,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0,\n",
       "   'amsgrad': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'capturable': False,\n",
       "   'differentiable': False,\n",
       "   'fused': None,\n",
       "   'params': [0, 1, 2, 3]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.8344],\n",
       "        [-10.8776],\n",
       "        [-10.6649],\n",
       "        [-10.3965],\n",
       "        [-10.1751],\n",
       "        [ -9.8600],\n",
       "        [ -9.6131],\n",
       "        [ -9.3776],\n",
       "        [ -9.1433],\n",
       "        [ -8.8702],\n",
       "        [ -8.6376],\n",
       "        [ -8.3426],\n",
       "        [ -8.1251],\n",
       "        [ -7.8258],\n",
       "        [ -7.6583],\n",
       "        [ -7.3539],\n",
       "        [ -7.0861],\n",
       "        [ -6.8799],\n",
       "        [ -6.5894],\n",
       "        [ -6.3686],\n",
       "        [ -6.0891],\n",
       "        [ -5.8393],\n",
       "        [ -5.5828],\n",
       "        [ -5.3361],\n",
       "        [ -5.0743],\n",
       "        [ -4.8626],\n",
       "        [ -4.5704],\n",
       "        [ -4.3510],\n",
       "        [ -3.9892],\n",
       "        [ -3.8003],\n",
       "        [ -3.5776],\n",
       "        [ -3.3267],\n",
       "        [ -3.0435],\n",
       "        [ -2.7844],\n",
       "        [ -2.5751],\n",
       "        [ -2.2815],\n",
       "        [ -2.0667],\n",
       "        [ -1.7644],\n",
       "        [ -1.5289],\n",
       "        [ -1.2880],\n",
       "        [ -1.0456],\n",
       "        [ -0.7574],\n",
       "        [ -0.4885],\n",
       "        [ -0.2374],\n",
       "        [  0.0395],\n",
       "        [  0.2724],\n",
       "        [  0.4757],\n",
       "        [  0.7451],\n",
       "        [  0.9777],\n",
       "        [  1.2863],\n",
       "        [  1.5187],\n",
       "        [  1.7922],\n",
       "        [  2.0076],\n",
       "        [  2.2806],\n",
       "        [  2.5184],\n",
       "        [  2.8293],\n",
       "        [  2.9897],\n",
       "        [  3.2998],\n",
       "        [  3.5542],\n",
       "        [  3.7831],\n",
       "        [  4.0065],\n",
       "        [  4.3110],\n",
       "        [  4.5410],\n",
       "        [  4.8380],\n",
       "        [  5.1084],\n",
       "        [  5.2647],\n",
       "        [  5.5611],\n",
       "        [  5.7984],\n",
       "        [  6.0770],\n",
       "        [  6.3487],\n",
       "        [  6.5776],\n",
       "        [  6.7896],\n",
       "        [  7.1187],\n",
       "        [  7.3275],\n",
       "        [  7.5954],\n",
       "        [  7.8266],\n",
       "        [  8.1518],\n",
       "        [  8.3724],\n",
       "        [  8.6022],\n",
       "        [  8.7622],\n",
       "        [  8.6580],\n",
       "        [  8.6188],\n",
       "        [  8.6914],\n",
       "        [  8.7918],\n",
       "        [  8.7404]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = models.ListMLE() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3083)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(b.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>average</th>\n",
       "      <th>datav2_1130_e22</th>\n",
       "      <th>datav2_1125_v2_e9</th>\n",
       "      <th>v4_1125_e7</th>\n",
       "      <th>geomean1</th>\n",
       "      <th>geomean2</th>\n",
       "      <th>DeepPLM_mCNN</th>\n",
       "      <th>domain885_e7</th>\n",
       "      <th>testdomain_e9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...</td>\n",
       "      <td>GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...</td>\n",
       "      <td>VIL1</td>\n",
       "      <td>0.764076</td>\n",
       "      <td>0.866837</td>\n",
       "      <td>0.963859</td>\n",
       "      <td>0.994411</td>\n",
       "      <td>0.940096</td>\n",
       "      <td>0.892614</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.210354</td>\n",
       "      <td>0.614913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VIL2</td>\n",
       "      <td>0.428990</td>\n",
       "      <td>0.565509</td>\n",
       "      <td>0.256785</td>\n",
       "      <td>0.528166</td>\n",
       "      <td>0.424874</td>\n",
       "      <td>0.425899</td>\n",
       "      <td>0.166094</td>\n",
       "      <td>0.316010</td>\n",
       "      <td>0.588083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...</td>\n",
       "      <td>GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...</td>\n",
       "      <td>VIL3</td>\n",
       "      <td>0.599692</td>\n",
       "      <td>0.911146</td>\n",
       "      <td>0.906326</td>\n",
       "      <td>0.996002</td>\n",
       "      <td>0.936938</td>\n",
       "      <td>0.838041</td>\n",
       "      <td>0.298704</td>\n",
       "      <td>0.834467</td>\n",
       "      <td>0.856670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...</td>\n",
       "      <td>GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...</td>\n",
       "      <td>VIL4</td>\n",
       "      <td>0.822577</td>\n",
       "      <td>0.970665</td>\n",
       "      <td>0.952568</td>\n",
       "      <td>0.935138</td>\n",
       "      <td>0.952680</td>\n",
       "      <td>0.918342</td>\n",
       "      <td>0.364926</td>\n",
       "      <td>0.775129</td>\n",
       "      <td>0.871358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VIL5</td>\n",
       "      <td>0.633763</td>\n",
       "      <td>0.995752</td>\n",
       "      <td>0.529931</td>\n",
       "      <td>0.994043</td>\n",
       "      <td>0.806477</td>\n",
       "      <td>0.759321</td>\n",
       "      <td>0.368040</td>\n",
       "      <td>0.732566</td>\n",
       "      <td>0.850245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \\\n",
       "num                                                      \n",
       "0    MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...   \n",
       "1    MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "2    MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...   \n",
       "3    MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...   \n",
       "4    MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "\n",
       "                                                     average  \\\n",
       "num                                                                      \n",
       "0    GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...  VIL1  0.764076   \n",
       "1    GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...  VIL2  0.428990   \n",
       "2    GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...  VIL3  0.599692   \n",
       "3    GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...  VIL4  0.822577   \n",
       "4    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...  VIL5  0.633763   \n",
       "\n",
       "     datav2_1130_e22  datav2_1125_v2_e9  v4_1125_e7  geomean1  geomean2  \\\n",
       "num                                                                       \n",
       "0           0.866837           0.963859    0.994411  0.940096  0.892614   \n",
       "1           0.565509           0.256785    0.528166  0.424874  0.425899   \n",
       "2           0.911146           0.906326    0.996002  0.936938  0.838041   \n",
       "3           0.970665           0.952568    0.935138  0.952680  0.918342   \n",
       "4           0.995752           0.529931    0.994043  0.806477  0.759321   \n",
       "\n",
       "     DeepPLM_mCNN  domain885_e7  testdomain_e9  \n",
       "num                                             \n",
       "0        0.100018      0.210354       0.614913  \n",
       "1        0.166094      0.316010       0.588083  \n",
       "2        0.298704      0.834467       0.856670  \n",
       "3        0.364926      0.775129       0.871358  \n",
       "4        0.368040      0.732566       0.850245  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"./temp/1204.xlsx\", sheet_name=\"Sheet1\", index_col=0)\n",
    "# df = df[~pd.isnull(df[\"\"])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...</td>\n",
       "      <td>GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...</td>\n",
       "      <td>VIL1</td>\n",
       "      <td>VILH-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VIL2</td>\n",
       "      <td>VILH-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...</td>\n",
       "      <td>GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...</td>\n",
       "      <td>VIL3</td>\n",
       "      <td>VILH-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...</td>\n",
       "      <td>GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...</td>\n",
       "      <td>VIL4</td>\n",
       "      <td>VILH-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VIL5</td>\n",
       "      <td>VILH-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \\\n",
       "num                                                      \n",
       "0    MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...   \n",
       "1    MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "2    MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...   \n",
       "3    MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...   \n",
       "4    MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "\n",
       "                                                     name2  \n",
       "num                                                                   \n",
       "0    GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...  VIL1  VILH-1  \n",
       "1    GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...  VIL2  VILH-2  \n",
       "2    GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...  VIL3  VILH-3  \n",
       "3    GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...  VIL4  VILH-4  \n",
       "4    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...  VIL5  VILH-5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"\", \"\", \"\"]]\n",
    "df[\"name2\"] = df[\"\"].apply(lambda x: x[:3]+\"H-\"+x[3:])\n",
    "df.to_csv(\"./temp/all885.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VILH-301</td>\n",
       "      <td>GSMPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSR...</td>\n",
       "      <td>MPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VILH-197</td>\n",
       "      <td>GSMRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPC...</td>\n",
       "      <td>MRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPCYF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VILH-19</td>\n",
       "      <td>GSMAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPC...</td>\n",
       "      <td>MAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPCSY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VILH-363</td>\n",
       "      <td>GSMDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALE...</td>\n",
       "      <td>MDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALEFA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VILH-426</td>\n",
       "      <td>GSMDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISAL...</td>\n",
       "      <td>MDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISALLL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      \\\n",
       "0  VILH-301  GSMPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSR...   \n",
       "1  VILH-197  GSMRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPC...   \n",
       "2   VILH-19  GSMAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPC...   \n",
       "3  VILH-363  GSMDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALE...   \n",
       "4  VILH-426  GSMDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISAL...   \n",
       "\n",
       "                                             .1  \n",
       "0  MPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSRSS...  \n",
       "1  MRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPCYF...  \n",
       "2  MAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPCSY...  \n",
       "3  MDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALEFA...  \n",
       "4  MDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISALLL...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfexp = pd.read_excel(\"./temp/1210_.xlsx\", sheet_name=\"Sheet1\")\n",
    "# dfexp[\"new\"] = dfexp[\".1\"].map(df[\"new\"])\n",
    "dfexp2 = pd.read_excel(\"./temp/1210_.xlsx\", sheet_name=\"Sheet2\")\n",
    "dfexp2 = dfexp2.sort_values(\"final_p\", ascending=False)\n",
    "dfexp2[\"rank\"] = range(len(dfexp2))\n",
    "# dfexp2[\"rank\"] = -dfexp2[\"rank\"]\n",
    "# dfexp[\"rank\"] = dfexp[\"\"].map(dfexp2.set_index(\"Name\")[\"rank\"])\n",
    "# dfexp = dfexp.sort_values(\"rank\")\n",
    "dfexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VILH-828': 0,\n",
       " 'VILH-53': 1,\n",
       " 'VILH-791': 2,\n",
       " 'VILH-295': 3,\n",
       " 'VILH-284': 4,\n",
       " 'VILH-149': 5,\n",
       " 'VILH-238': 6,\n",
       " 'VILH-537': 7,\n",
       " 'VILH-796': 8,\n",
       " 'VILH-151': 9,\n",
       " 'VILH-534': 10,\n",
       " 'VILH-736': 11,\n",
       " 'VILH-776': 12,\n",
       " 'VILH-731': 13,\n",
       " 'VILH-845': 14,\n",
       " 'VILH-511': 15,\n",
       " 'VILH-228': 16,\n",
       " 'VILH-518': 17,\n",
       " 'VILH-188': 18,\n",
       " 'VILH-802': 19,\n",
       " 'VILH-450': 20,\n",
       " 'VILH-63': 21,\n",
       " 'VILH-179': 22,\n",
       " 'VILH-787': 23,\n",
       " 'VILH-129': 24,\n",
       " 'VILH-311': 25,\n",
       " 'VILH-559': 26,\n",
       " 'VILH-741': 27,\n",
       " 'VILH-325': 28,\n",
       " 'VILH-131': 29,\n",
       " 'VILH-723': 30,\n",
       " 'VILH-713': 31,\n",
       " 'VILH-591': 32,\n",
       " 'VILH-712': 33,\n",
       " 'VILH-62': 34,\n",
       " 'VILH-827': 35,\n",
       " 'VILH-752': 36,\n",
       " 'VILH-594': 37,\n",
       " 'VILH-525': 38,\n",
       " 'VILH-282': 39,\n",
       " 'VILH-732': 40,\n",
       " 'VILH-124': 41,\n",
       " 'VILH-749': 42,\n",
       " 'VILH-786': 43,\n",
       " 'VILH-254': 44,\n",
       " 'VILH-503': 45,\n",
       " 'VILH-449': 46,\n",
       " 'VILH-453': 47,\n",
       " 'VILH-581': 48,\n",
       " 'VILH-280': 49,\n",
       " 'VILH-259': 50,\n",
       " 'VILH-150': 51,\n",
       " 'VILH-314': 52,\n",
       " 'VILH-144': 53,\n",
       " 'VILH-832': 54,\n",
       " 'VILH-194': 55,\n",
       " 'VILH-785': 56,\n",
       " 'VILH-456': 57,\n",
       " 'VILH-232': 58,\n",
       " 'VILH-267': 59,\n",
       " 'VILH-502': 60,\n",
       " 'VILH-759': 61,\n",
       " 'VILH-59': 62,\n",
       " 'VILH-801': 63,\n",
       " 'VILH-279': 64,\n",
       " 'VILH-211': 65,\n",
       " 'VILH-446': 66,\n",
       " 'VILH-366': 67,\n",
       " 'VILH-635': 68,\n",
       " 'VILH-757': 69,\n",
       " 'VILH-199': 70,\n",
       " 'VILH-293': 71,\n",
       " 'VILH-771': 72,\n",
       " 'VILH-640': 73,\n",
       " 'VILH-265': 74,\n",
       " 'VILH-2': 75,\n",
       " 'VILH-204': 76,\n",
       " 'VILH-313': 77,\n",
       " 'VILH-241': 78,\n",
       " 'VILH-166': 79,\n",
       " 'VILH-681': 80,\n",
       " 'VILH-243': 81,\n",
       " 'VILH-815': 82,\n",
       " 'VILH-157': 83,\n",
       " 'VILH-844': 84,\n",
       " 'VILH-197': 85,\n",
       " 'VILH-154': 86,\n",
       " 'VILH-346': 87,\n",
       " 'VILH-200': 88,\n",
       " 'VILH-19': 89,\n",
       " 'VILH-743': 90,\n",
       " 'VILH-735': 91,\n",
       " 'VILH-429': 92,\n",
       " 'VILH-251': 93,\n",
       " 'VILH-515': 94,\n",
       " 'VILH-363': 95,\n",
       " 'VILH-426': 96,\n",
       " 'VILH-480': 97,\n",
       " 'VILH-61': 98,\n",
       " 'VILH-231': 99,\n",
       " 'VILH-838': 100,\n",
       " 'VILH-301': 101,\n",
       " 'VILH-730': 102,\n",
       " 'VILH-48': 103,\n",
       " 'VILH-5': 104,\n",
       " 'VILH-306': 105}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfexp2.head()\n",
    "dfexp2 = dfexp2.set_index(\"Name\")\n",
    "d = dict(dfexp2[\"rank\"])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>experiment_1</th>\n",
       "      <th>experiment_2</th>\n",
       "      <th>yeast_1</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VIL2</td>\n",
       "      <td>75</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VIL5</td>\n",
       "      <td>104</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPCSY...</td>\n",
       "      <td>GSMAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPC...</td>\n",
       "      <td>VIL19</td>\n",
       "      <td>89</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>MAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNFAR...</td>\n",
       "      <td>GSMAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNF...</td>\n",
       "      <td>VIL48</td>\n",
       "      <td>103</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>MPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEYLK...</td>\n",
       "      <td>GSMPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEY...</td>\n",
       "      <td>VIL53</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>MNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFINK...</td>\n",
       "      <td>GSMNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFI...</td>\n",
       "      <td>VIL828</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>MLSLILFFPSFAFSATPVTPYYGPGHITSDWCGFGDSRSDCNNSYF...</td>\n",
       "      <td>GSMLSLILFFPSFAFSATPVTPYYGPGHITSDWCGFGDSRSDCNNS...</td>\n",
       "      <td>VIL832</td>\n",
       "      <td>54</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>MGGWSSKPRQGMGTNLSVPNPLGFFPDHQLDPAFGANSNNPDWDFN...</td>\n",
       "      <td>GSMGGWSSKPRQGMGTNLSVPNPLGFFPDHQLDPAFGANSNNPDWD...</td>\n",
       "      <td>VIL838</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>MFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHMFC...</td>\n",
       "      <td>GSMFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHM...</td>\n",
       "      <td>VIL844</td>\n",
       "      <td>84</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>MTEANITQEELVTHLKNWNFSWNIILTIFIVILQFGHYKYSRIFYG...</td>\n",
       "      <td>GSMTEANITQEELVTHLKNWNFSWNIILTIFIVILQFGHYKYSRIF...</td>\n",
       "      <td>VIL845</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \\\n",
       "num                                                      \n",
       "1    MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "4    MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "18   MAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPCSY...   \n",
       "48   MAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNFAR...   \n",
       "53   MPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEYLK...   \n",
       "..                                                 ...   \n",
       "837  MNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFINK...   \n",
       "841  MLSLILFFPSFAFSATPVTPYYGPGHITSDWCGFGDSRSDCNNSYF...   \n",
       "847  MGGWSSKPRQGMGTNLSVPNPLGFFPDHQLDPAFGANSNNPDWDFN...   \n",
       "853  MFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHMFC...   \n",
       "854  MTEANITQEELVTHLKNWNFSWNIILTIFIVILQFGHYKYSRIFYG...   \n",
       "\n",
       "                                                      experiment_1  \\\n",
       "num                                                                            \n",
       "1    GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...    VIL2            75   \n",
       "4    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...    VIL5           104   \n",
       "18   GSMAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPC...   VIL19            89   \n",
       "48   GSMAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNF...   VIL48           103   \n",
       "53   GSMPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEY...   VIL53             1   \n",
       "..                                                 ...     ...           ...   \n",
       "837  GSMNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFI...  VIL828             0   \n",
       "841  GSMLSLILFFPSFAFSATPVTPYYGPGHITSDWCGFGDSRSDCNNS...  VIL832            54   \n",
       "847  GSMGGWSSKPRQGMGTNLSVPNPLGFFPDHQLDPAFGANSNNPDWD...  VIL838           100   \n",
       "853  GSMFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHM...  VIL844            84   \n",
       "854  GSMTEANITQEELVTHLKNWNFSWNIILTIFIVILQFGHYKYSRIF...  VIL845            14   \n",
       "\n",
       "     experiment_2  yeast_1      name  \n",
       "num                                   \n",
       "1              -1       -1    VILH-2  \n",
       "4              -1       -1    VILH-5  \n",
       "18             -1       -1   VILH-19  \n",
       "48             -1       -1   VILH-48  \n",
       "53             -1       -1   VILH-53  \n",
       "..            ...      ...       ...  \n",
       "837            -1       -1  VILH-828  \n",
       "841            -1       -1  VILH-832  \n",
       "847            -1       -1  VILH-838  \n",
       "853            -1       -1  VILH-844  \n",
       "854            -1       -1  VILH-845  \n",
       "\n",
       "[106 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = pd.read_csv(\"./temp/expres.csv\", index_col=0)\n",
    "# t[\"name\"] = t[\"\"].apply(lambda x: \"-\" if not isinstance(x, str) else x[:3]+\"H-\"+x[3:])\n",
    "# t[\"experiment_1\"] = t[\"name\"].apply(lambda x: d.get(x, -1))\n",
    "# t[t[\"experiment_1\"]>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv(\"./temp/expres.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MGCDVHDPSWQCQWGVPTIIVAWITCAALGIWCLAGSSADVSSGPGIAAVVGCSVFMIFLCAYLIRYREFFKDSVIDLLTCRWVRYCSCSCKCSCKCISGPCSRCCSACYKETMIYDMVQYGHRRRPGHGDDPDRVICEIVESPPVSAPTVFVPPPSEESHQPVIPPQPPTPTSEPKPKKGRAKDKPKGRPKNKPPCEPTVSSQPPSQPTAMPGGPPDASPPAMPQMPPGVAEAVQAAVQAAMAAALQQQQQHQTGT',\n",
       " 'MISQGNGGGCRPGEPCWRCALESTRCITLMGVLVALLAACMLSVPPAASTMLLGVASLMAMLRLPMPLVDRFIPACMGLQLVGAAVFAAGWALASRDAISAGVLLWAVCALISHMYNVVCVASGPDAHYRPACLVMGVAAACGAAGALVNVRTEARLGIALGLAVTCATNNVARSLRGTCTYVASRARFLAAPADLGRGYSVENADADPTAEPERRVYEATVPHTHAYAGSIALFALVFSAASSLQWMVSQMVGRGNQLVSPTTAAAAGAAGFLDAAAVSLFVRPSTRHLSVAVKGAHTLLILAAIVLTAAGEPMGVPISLAASTGLGAARGGPRPLRHTRAYRLAAAHVTRALLVQAYVTVAMCATSIKSVS')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/0/test.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f) \n",
    "data[-1][\"ori_seq\"], data[0][\"ori_seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MGGWSSKPRQGMGTNLSVPNPLGFFPDHQLDPAFGANSNNPDWDFNPNKDHWPEANQVGAGAFGPGFTPPHGGLLGWSPQAQGILTTVPAAPPPASTNRQSGRQPTPISPPLRDSHPQAIRWNSTTFHQALLDPRVRGLYFPAGGSSSGTVNPVPTTASPISSIFSRTGDPATNMENTTSGFLGPLLVLQAGFFSLTRILTIPQSLDSWWTSLNFLGGAPTCPGQNSQSPTSNHSPTSCPPICPGYRWMCLRRFIIFLFILLLCLIFLLVLLDYQGMLPVCPLLPGTSTTSTGPCKTCTIPAQGTSMFPSCCCTKPSDGNCTCIPIPSSWAFARFLWEWASVRFSWLSLLVPFVQWFVGLSPTVWLSVIWMMWYWGPSLYNILSPFLPLLPIFFYLWVYI',\n",
       " 'MNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFINKNNCTNNVIRVHKRIKCPDCEPFCNKRDDISTPRAGVDIPSFILPGLNLSEGTPN')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/0/test_2.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f) \n",
    "data[-1][\"ori_seq\"], data[0][\"ori_seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>experiment_1</th>\n",
       "      <th>experiment_2</th>\n",
       "      <th>yeast_1</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>MNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFINK...</td>\n",
       "      <td>GSMNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFI...</td>\n",
       "      <td>VIL828</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>MPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEYLK...</td>\n",
       "      <td>GSMPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEY...</td>\n",
       "      <td>VIL53</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>MGNTSITIEFTSKFWPYFTLIHMILTLISLLIIITIMIAILNKLSE...</td>\n",
       "      <td>GSMGNTSITIEFTSKFWPYFTLIHMILTLISLLIIITIMIAILNKL...</td>\n",
       "      <td>VIL791</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>MFALVLAVVILPLWTTANKSYVTPTPATRSIGHMSALLREYSDRNM...</td>\n",
       "      <td>GSMFALVLAVVILPLWTTANKSYVTPTPATRSIGHMSALLREYSDR...</td>\n",
       "      <td>VIL295</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>MVRARVLRGLLAAWVLAAWGGVAAVETTWKHASAGDDVRFFDLPAG...</td>\n",
       "      <td>GSMVRARVLRGLLAAWVLAAWGGVAAVETTWKHASAGDDVRFFDLP...</td>\n",
       "      <td>VIL284</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>MPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSRSS...</td>\n",
       "      <td>GSMPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSR...</td>\n",
       "      <td>VIL301</td>\n",
       "      <td>101</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>SFSEPLTVVGVMLTLASGMMRHTSQEALCALAAASFLLLMLVLGTR...</td>\n",
       "      <td>GSMSFSEPLTVVGVMLTLASGMMRHTSQEALCALAAASFLLLMLVL...</td>\n",
       "      <td>VIL730</td>\n",
       "      <td>102</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>MAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNFAR...</td>\n",
       "      <td>GSMAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNF...</td>\n",
       "      <td>VIL48</td>\n",
       "      <td>103</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VIL5</td>\n",
       "      <td>104</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>MISQGNGGGCRPGEPCWRCALESTRCITLMGVLVALLAACMLSVPP...</td>\n",
       "      <td>GSMISQGNGGGCRPGEPCWRCALESTRCITLMGVLVALLAACMLSV...</td>\n",
       "      <td>VIL306</td>\n",
       "      <td>105</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>VILH-306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \\\n",
       "num                                                      \n",
       "837  MNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFINK...   \n",
       "53   MPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEYLK...   \n",
       "799  MGNTSITIEFTSKFWPYFTLIHMILTLISLLIIITIMIAILNKLSE...   \n",
       "299  MFALVLAVVILPLWTTANKSYVTPTPATRSIGHMSALLREYSDRNM...   \n",
       "288  MVRARVLRGLLAAWVLAAWGGVAAVETTWKHASAGDDVRFFDLPAG...   \n",
       "..                                                 ...   \n",
       "305  MPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSRSS...   \n",
       "737  SFSEPLTVVGVMLTLASGMMRHTSQEALCALAAASFLLLMLVLGTR...   \n",
       "48   MAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNFAR...   \n",
       "4    MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "310  MISQGNGGGCRPGEPCWRCALESTRCITLMGVLVALLAACMLSVPP...   \n",
       "\n",
       "                                                      experiment_1  \\\n",
       "num                                                                            \n",
       "837  GSMNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFI...  VIL828             0   \n",
       "53   GSMPAIPVQYVNCVGVVGLVAFGLLSYQWIERKRRQGPLPQWFKEY...   VIL53             1   \n",
       "799  GSMGNTSITIEFTSKFWPYFTLIHMILTLISLLIIITIMIAILNKL...  VIL791             2   \n",
       "299  GSMFALVLAVVILPLWTTANKSYVTPTPATRSIGHMSALLREYSDR...  VIL295             3   \n",
       "288  GSMVRARVLRGLLAAWVLAAWGGVAAVETTWKHASAGDDVRFFDLP...  VIL284             4   \n",
       "..                                                 ...     ...           ...   \n",
       "305  GSMPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSR...  VIL301           101   \n",
       "737  GSMSFSEPLTVVGVMLTLASGMMRHTSQEALCALAAASFLLLMLVL...  VIL730           102   \n",
       "48   GSMAQARPQGHISWLWKWATSATNTNTHKLATKAPLNLSSEYRLNF...   VIL48           103   \n",
       "4    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...    VIL5           104   \n",
       "310  GSMISQGNGGGCRPGEPCWRCALESTRCITLMGVLVALLAACMLSV...  VIL306           105   \n",
       "\n",
       "     experiment_2  yeast_1      name  \n",
       "num                                   \n",
       "837            -1       -1  VILH-828  \n",
       "53             -1       -1   VILH-53  \n",
       "799            -1       -1  VILH-791  \n",
       "299            -1       -1  VILH-295  \n",
       "288            -1       -1  VILH-284  \n",
       "..            ...      ...       ...  \n",
       "305            -1       -1  VILH-301  \n",
       "737            -1       -1  VILH-730  \n",
       "48             -1       -1   VILH-48  \n",
       "4              -1       -1    VILH-5  \n",
       "310            -1       -1  VILH-306  \n",
       "\n",
       "[106 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv(\"./temp/expres.csv\", index_col=0)\n",
    "# t = t.sort_values(\"experiment_2\", ascending=False)\n",
    "t = t[t[\"experiment_1\"] >= 0]\n",
    "t = t.sort_values(\"experiment_1\")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 106)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [] \n",
    "for i in t[\"\"]:\n",
    "    for j in data:\n",
    "        if i == j[\"ori_seq\"]:\n",
    "            res.append(j)\n",
    "len(res), len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/exp2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(106))\n",
    "random.seed(1509)\n",
    "random.shuffle(a)\n",
    "splits = [0, 21, 42, 63, 85, 106]\n",
    "for i in range(5):\n",
    "    test_indices = a[splits[i]:splits[i+1]] \n",
    "    train = [] \n",
    "    test = [] \n",
    "    for j in range(106):\n",
    "        if j in test_indices:\n",
    "            test.append(res[j])\n",
    "        else:\n",
    "            train.append(res[j])\n",
    "    with open(f\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/{i}/train_2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train, f)\n",
    "    with open(f\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/{i}/test_2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import trainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/FromTestDomain/1\"\n",
    "with open(os.path.join(path, \"config.json\"), \"r\") as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix model for active learning\n"
     ]
    }
   ],
   "source": [
    "pretrain_model = trainUtils.loadPretrainModel(configs)\n",
    "model = trainUtils.buildModel(configs, pretrain_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = trainUtils.loadDataset(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = ds.train_dataloader()\n",
    "for i, j in enumerate(dl):\n",
    "    break\n",
    "print(j[0].shape, j[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.l1.weight\n",
      "clf.l1.bias\n",
      "clf.l2.weight\n",
      "clf.l2.bias\n",
      "clf.l3.weight\n",
      "clf.l3.bias\n",
      "clf.ln1.weight\n",
      "clf.ln1.bias\n",
      "clf.ln2.weight\n",
      "clf.ln2.bias\n"
     ]
    }
   ],
   "source": [
    "for i, j in model.named_parameters():\n",
    "    if j.requires_grad:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = trainUtils.loadDataset(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL1</th>\n",
       "      <td>MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...</td>\n",
       "      <td>GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...</td>\n",
       "      <td>VILH-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL2</th>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VILH-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL3</th>\n",
       "      <td>MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...</td>\n",
       "      <td>GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...</td>\n",
       "      <td>VILH-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL4</th>\n",
       "      <td>MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...</td>\n",
       "      <td>GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...</td>\n",
       "      <td>VILH-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL5</th>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VILH-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      \\\n",
       "                                                      \n",
       "VIL1   MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...   \n",
       "VIL2   MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "VIL3   MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...   \n",
       "VIL4   MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...   \n",
       "VIL5   MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "\n",
       "                                                      name2  \n",
       "                                                             \n",
       "VIL1   GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...  VILH-1  \n",
       "VIL2   GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...  VILH-2  \n",
       "VIL3   GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...  VILH-3  \n",
       "VIL4   GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...  VILH-4  \n",
       "VIL5   GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...  VILH-5  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./temp/all885.csv\", index_col=0)\n",
    "df = df.set_index(\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Areatstatistic</th>\n",
       "      <th>Areapvalue</th>\n",
       "      <th>Maxtstatistic</th>\n",
       "      <th>Maxpvalue</th>\n",
       "      <th>geomean</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL100</th>\n",
       "      <td>13.051121</td>\n",
       "      <td>0.999901</td>\n",
       "      <td>10.967152</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>MNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVLIR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL455</th>\n",
       "      <td>3.114733</td>\n",
       "      <td>0.982146</td>\n",
       "      <td>11.386229</td>\n",
       "      <td>0.999830</td>\n",
       "      <td>0.990949</td>\n",
       "      <td>MALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRASHR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL750</th>\n",
       "      <td>7.659041</td>\n",
       "      <td>0.999219</td>\n",
       "      <td>2.369652</td>\n",
       "      <td>0.961575</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>MMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDSKI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL10</th>\n",
       "      <td>1.946238</td>\n",
       "      <td>0.938255</td>\n",
       "      <td>3.043280</td>\n",
       "      <td>0.980860</td>\n",
       "      <td>0.959321</td>\n",
       "      <td>MTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVICL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL835</th>\n",
       "      <td>2.592156</td>\n",
       "      <td>0.969727</td>\n",
       "      <td>2.055082</td>\n",
       "      <td>0.945470</td>\n",
       "      <td>0.957522</td>\n",
       "      <td>MPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLGNL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Areatstatistic  Areapvalue  Maxtstatistic  Maxpvalue   geomean  \\\n",
       "Name                                                                     \n",
       "VIL100       13.051121    0.999901      10.967152   0.999804  0.999852   \n",
       "VIL455        3.114733    0.982146      11.386229   0.999830  0.990949   \n",
       "VIL750        7.659041    0.999219       2.369652   0.961575  0.980216   \n",
       "VIL10         1.946238    0.938255       3.043280   0.980860  0.959321   \n",
       "VIL835        2.592156    0.969727       2.055082   0.945470  0.957522   \n",
       "\n",
       "                                                      seq  \n",
       "Name                                                       \n",
       "VIL100  MNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVLIR...  \n",
       "VIL455  MALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRASHR...  \n",
       "VIL750  MMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDSKI...  \n",
       "VIL10   MTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVICL...  \n",
       "VIL835  MPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLGNL...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfexp = pd.read_excel(\"./temp/exp2.xlsx\", sheet_name=\"Sheet1\")\n",
    "dfexp = dfexp.set_index(\"Name\")\n",
    "dfexp[\"seq\"] = df[\"\"]\n",
    "dfexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Areatstatistic</th>\n",
       "      <th>Areapvalue</th>\n",
       "      <th>Maxtstatistic</th>\n",
       "      <th>Maxpvalue</th>\n",
       "      <th>geomean</th>\n",
       "      <th>seq</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL100</th>\n",
       "      <td>13.051121</td>\n",
       "      <td>0.999901</td>\n",
       "      <td>10.967152</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>MNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVLIR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL455</th>\n",
       "      <td>3.114733</td>\n",
       "      <td>0.982146</td>\n",
       "      <td>11.386229</td>\n",
       "      <td>0.999830</td>\n",
       "      <td>0.990949</td>\n",
       "      <td>MALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRASHR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL750</th>\n",
       "      <td>7.659041</td>\n",
       "      <td>0.999219</td>\n",
       "      <td>2.369652</td>\n",
       "      <td>0.961575</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>MMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDSKI...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL10</th>\n",
       "      <td>1.946238</td>\n",
       "      <td>0.938255</td>\n",
       "      <td>3.043280</td>\n",
       "      <td>0.980860</td>\n",
       "      <td>0.959321</td>\n",
       "      <td>MTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVICL...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL835</th>\n",
       "      <td>2.592156</td>\n",
       "      <td>0.969727</td>\n",
       "      <td>2.055082</td>\n",
       "      <td>0.945470</td>\n",
       "      <td>0.957522</td>\n",
       "      <td>MPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLGNL...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Areatstatistic  Areapvalue  Maxtstatistic  Maxpvalue   geomean  \\\n",
       "Name                                                                     \n",
       "VIL100       13.051121    0.999901      10.967152   0.999804  0.999852   \n",
       "VIL455        3.114733    0.982146      11.386229   0.999830  0.990949   \n",
       "VIL750        7.659041    0.999219       2.369652   0.961575  0.980216   \n",
       "VIL10         1.946238    0.938255       3.043280   0.980860  0.959321   \n",
       "VIL835        2.592156    0.969727       2.055082   0.945470  0.957522   \n",
       "\n",
       "                                                      seq  rank  \n",
       "Name                                                             \n",
       "VIL100  MNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVLIR...     0  \n",
       "VIL455  MALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRASHR...     1  \n",
       "VIL750  MMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDSKI...     2  \n",
       "VIL10   MTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVICL...     3  \n",
       "VIL835  MPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLGNL...     4  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfexp = dfexp.sort_values(\"geomean\", ascending=False) \n",
    "dfexp[\"rank\"] = range(len(dfexp))\n",
    "dfexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 62)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [] \n",
    "for i in dfexp[\"seq\"]:\n",
    "    for j in data:\n",
    "        if i == j[\"ori_seq\"]:\n",
    "            res.append(j)\n",
    "len(res), len(dfexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/exp2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from scipy import stats\n",
    "target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest/\"\n",
    "output_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest/bayes10/\"\n",
    "dataset = \"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\"\n",
    "for i in os.listdir(target_dir):\n",
    "    if i.startswith(\"epoch\"):\n",
    "        print(i) \n",
    "        \n",
    "    ret = os.system(\"python test.py -p %s -c %s -i %s -d %d -b %d -o %s\"%(target_dir, i, dataset, 5, 10, output_dir)) \n",
    "    \n",
    "target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest2/\"\n",
    "output_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest2/bayes10/\"\n",
    "dataset = \"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\"\n",
    "for i in os.listdir(target_dir):\n",
    "    if i.startswith(\"epoch\"):\n",
    "        print(i) \n",
    "        \n",
    "    ret = os.system(\"python test.py -p %s -c %s -i %s -d %d -b %d -o %s\"%(target_dir, i, dataset, 5, 10, output_dir)) \n",
    "    \n",
    "# target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest3/\"\n",
    "# output_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest3/bayes10/\"\n",
    "# dataset = \"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\"\n",
    "# for i in os.listdir(target_dir):\n",
    "#     if i.startswith(\"epoch\"):\n",
    "#         print(i) \n",
    "        \n",
    "#     ret = os.system(\"python test.py -p %s -c %s -i %s -d %d -b %d -o %s\"%(target_dir, i, dataset, 5, 10, output_dir)) \n",
    "    # if ret == 0:\n",
    "    #     a = np.loadtxt(\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/FromTestDomain/4_2/test_last.txt\")\n",
    "    #     stats.spearmanr(a, range(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 16.90it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 17.04it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 16.68it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 17.41it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 22/22 [00:01<00:00, 17.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in range(5):\n",
    "    # target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/%d_2/\"%i\n",
    "    target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/nodomain/\"\n",
    "    dataset = \"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/%d/test.pkl\"%i\n",
    "    outputdir = \"/data2/tyfei/trainresults/ionChannels/ESMC/nodomain/%d\"%i\n",
    "    os.system(\"python test.py -p %s -c %s -i %s -d %d -o %s\"%(target_dir, target_dir+\"epoch=8-validate_acc=0.9899.ckpt\", dataset, 5, outputdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "fix model for active learning\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in range(5):\n",
    "    target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/%d_4/\"%i\n",
    "    dataset = \"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/%d/test.pkl\"%i\n",
    "    os.system(\"python test.py -p %s -c %s -i %s -d %d\"%(target_dir, target_dir+\"last.ckpt\", dataset, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.247482962061986, pvalue=0.279429612882753)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import testUtils \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "a = np.loadtxt(\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/FromTestDomain/4_2/test_last.txt\")\n",
    "stats.spearmanr(a, range(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14285714285714285,\n",
       " -0.212987012987013,\n",
       " 0.14935064935064934,\n",
       " 0.22337662337662337,\n",
       " -0.005081874647092039]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0 = []\n",
    "for i in range(5):\n",
    "    a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMC/nodomain/{i}/test_epoch=8-validate_acc=0.9899.txt\")\n",
    "    d0.append(stats.spearmanr(a, range(len(a)))[0])\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32597402597402597,\n",
       " 0.15194805194805194,\n",
       " 0.6896103896103896,\n",
       " 0.5064935064935064,\n",
       " 0.11914172783738003]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = [] \n",
    "for i in range(5):\n",
    "    if os.path.exists(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}/test_last-v1.txt\"):\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}/test_last-v1.txt\")\n",
    "    else:\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}/test_last.txt\")\n",
    "    d1.append(stats.spearmanr(a, range(len(a)))[0])\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6168831168831169,\n",
       " -0.15584415584415584,\n",
       " 0.3922077922077922,\n",
       " 0.5649350649350648,\n",
       " 0.1981931112365895]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = [] \n",
    "for i in range(5):\n",
    "    if os.path.exists(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}_3/test_last-v1.txt\"):\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}_3/test_last-v1.txt\")\n",
    "    else:\n",
    "        # print(\"train\")\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}_3/test_last.txt\")\n",
    "    d2.append(stats.spearmanr(a, range(len(a)))[0])\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m [] \n\u001b[0;32m----> 2\u001b[0m d\u001b[38;5;241m.\u001b[39mextend(\u001b[43md0\u001b[49m)\n\u001b[1;32m      3\u001b[0m d\u001b[38;5;241m.\u001b[39mextend(d1)\n\u001b[1;32m      4\u001b[0m d\u001b[38;5;241m.\u001b[39mextend(d2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd0' is not defined"
     ]
    }
   ],
   "source": [
    "d = [] \n",
    "d.extend(d0)\n",
    "d.extend(d1)\n",
    "d.extend(d2)\n",
    "f = [] \n",
    "f.extend([\"0\"]*5)\n",
    "f.extend([\"1\"]*5)\n",
    "f.extend([\"2\"]*5)\n",
    "df = pd.DataFrame({\"d\": d, \"f\": f}) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='f', ylabel='d'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=\"f\", y=\"d\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df2 = pd.read_csv(\"/data/tyfei/datasets/ion_channel/Active_test/20250302_d2_screen/test885_epoch=7-validate_list_loss=9.52-validate_acc=0.9810-validate_corr=0.6691.csv\")\n",
    "df = pd.read_csv(\"./temp/all885.csv\")\n",
    "df[\"predict\"] = df2[\"prediction\"]\n",
    "df = df.set_index(\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Area</th>\n",
       "      <th>Type</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vector</td>\n",
       "      <td>374.6225</td>\n",
       "      <td>1</td>\n",
       "      <td>1.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vector</td>\n",
       "      <td>368.1975</td>\n",
       "      <td>1</td>\n",
       "      <td>1.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vector</td>\n",
       "      <td>376.1290</td>\n",
       "      <td>1</td>\n",
       "      <td>1.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VIL5</td>\n",
       "      <td>358.8065</td>\n",
       "      <td>1</td>\n",
       "      <td>1.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIL5</td>\n",
       "      <td>347.1150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>352.1715</td>\n",
       "      <td>4</td>\n",
       "      <td>1.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>349.7370</td>\n",
       "      <td>4</td>\n",
       "      <td>1.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>355.9575</td>\n",
       "      <td>4</td>\n",
       "      <td>1.323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>363.6730</td>\n",
       "      <td>4</td>\n",
       "      <td>1.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>358.8755</td>\n",
       "      <td>4</td>\n",
       "      <td>1.353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Group      Area  Type    Max\n",
       "0    Vector  374.6225     1  1.402\n",
       "1    Vector  368.1975     1  1.368\n",
       "2    Vector  376.1290     1  1.389\n",
       "3      VIL5  358.8065     1  1.313\n",
       "4      VIL5  347.1150     1  1.261\n",
       "..      ...       ...   ...    ...\n",
       "223  VIL241  352.1715     4  1.289\n",
       "224  VIL241  349.7370     4  1.274\n",
       "225  VIL241  355.9575     4  1.323\n",
       "226  VIL241  363.6730     4  1.349\n",
       "227  VIL241  358.8755     4  1.353\n",
       "\n",
       "[228 rows x 4 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfexp3 = pd.read_excel(\"./exp_res/exp3.xlsx\", sheet_name=\"2\")\n",
    "dfexp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "for key, subdf in dfexp3.groupby(\"Type\"):\n",
    "    df2 = subdf[subdf[\"Group\"] == \"Vector\"]\n",
    "    Max = np.mean(subdf[\"Max\"])\n",
    "    Area = np.mean(subdf[\"Area\"])\n",
    "    subdf[\"Max\"] = subdf[\"Max\"] / Max\n",
    "    subdf[\"Area\"] = subdf[\"Area\"] / Area\n",
    "    dfs.append(subdf)\n",
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Area</th>\n",
       "      <th>Max</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vector</td>\n",
       "      <td>1.022249</td>\n",
       "      <td>1.049239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vector</td>\n",
       "      <td>1.029591</td>\n",
       "      <td>1.077768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vector</td>\n",
       "      <td>1.010930</td>\n",
       "      <td>1.023087</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vector</td>\n",
       "      <td>1.018065</td>\n",
       "      <td>1.046862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIL5</td>\n",
       "      <td>0.979419</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>0.994449</td>\n",
       "      <td>0.987708</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>0.987575</td>\n",
       "      <td>0.976214</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>1.005140</td>\n",
       "      <td>1.013761</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>1.026927</td>\n",
       "      <td>1.033683</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>VIL241</td>\n",
       "      <td>1.013380</td>\n",
       "      <td>1.036749</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>578 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Group      Area       Max  Type\n",
       "0    Vector  1.022249  1.049239     1\n",
       "1    Vector  1.029591  1.077768     1\n",
       "2    Vector  1.010930  1.023087     1\n",
       "3    Vector  1.018065  1.046862     1\n",
       "4      VIL5  0.979419  0.964444     1\n",
       "..      ...       ...       ...   ...\n",
       "223  VIL241  0.994449  0.987708     4\n",
       "224  VIL241  0.987575  0.976214     4\n",
       "225  VIL241  1.005140  1.013761     4\n",
       "226  VIL241  1.026927  1.033683     4\n",
       "227  VIL241  1.013380  1.036749     4\n",
       "\n",
       "[578 rows x 4 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat(dfs)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VIL1': 0.9949153894429349,\n",
       " 'VIL114': 0.9792670776159762,\n",
       " 'VIL129': 1.000439526064463,\n",
       " 'VIL135': 0.9914627934815322,\n",
       " 'VIL138': 0.9788773073122068,\n",
       " 'VIL157': 0.9616505394072011,\n",
       " 'VIL170': 0.9703051198967779,\n",
       " 'VIL191': 1.0133233564114257,\n",
       " 'VIL212': 1.0058813340676236,\n",
       " 'VIL213': 1.007235092118753,\n",
       " 'VIL215': 1.0153005506280268,\n",
       " 'VIL233': 0.9933294542608413,\n",
       " 'VIL241': 1.0054940563298556,\n",
       " 'VIL258': 1.0252976891591172,\n",
       " 'VIL266': 0.9856123785103926,\n",
       " 'VIL286': 0.9972192030247432,\n",
       " 'VIL287': 1.0002024204451645,\n",
       " 'VIL291': 1.002097113093207,\n",
       " 'VIL309': 1.0077077352774064,\n",
       " 'VIL314': 1.0120881530993953,\n",
       " 'VIL321': 1.0045785854620075,\n",
       " 'VIL352': 0.9918877670405734,\n",
       " 'VIL365': 0.9812929369407738,\n",
       " 'VIL384': 0.9907406096253757,\n",
       " 'VIL431': 1.0015964865697067,\n",
       " 'VIL447': 1.038097755546305,\n",
       " 'VIL469': 1.006148427683539,\n",
       " 'VIL476': 0.9887779943423042,\n",
       " 'VIL481': 0.9997704273688918,\n",
       " 'VIL489': 1.0349026947999642,\n",
       " 'VIL494': 0.9659092052350661,\n",
       " 'VIL5': 0.991137817006042,\n",
       " 'VIL508': 0.9873198772393419,\n",
       " 'VIL519': 0.984433203100564,\n",
       " 'VIL556': 0.9674413055393121,\n",
       " 'VIL559': 1.0140205845789658,\n",
       " 'VIL569': 0.9860887229810847,\n",
       " 'VIL572': 0.9696697871119443,\n",
       " 'VIL585': 1.0091648567857265,\n",
       " 'VIL588': 1.01413971467858,\n",
       " 'VIL590': 0.9956813766739975,\n",
       " 'VIL591': 0.9979014835195358,\n",
       " 'VIL606': 0.9870175650385828,\n",
       " 'VIL619': 0.9928869671320051,\n",
       " 'VIL629': 1.003815982413405,\n",
       " 'VIL635': 0.9950437319673969,\n",
       " 'VIL649': 1.001376442945252,\n",
       " 'VIL666': 1.0220411905526825,\n",
       " 'VIL700': 0.9960659237764996,\n",
       " 'VIL705': 0.9785020712906819,\n",
       " 'VIL716': 1.0167608191737842,\n",
       " 'VIL719': 1.0330101432771268,\n",
       " 'VIL734': 1.003577809658896,\n",
       " 'VIL739': 0.9967400174335419,\n",
       " 'VIL74': 1.031356378504304,\n",
       " 'VIL748': 1.005954146067591,\n",
       " 'VIL782': 1.011814648676456,\n",
       " 'VIL786': 0.9800523465603345,\n",
       " 'VIL789': 0.9958169409340895,\n",
       " 'VIL80': 1.0113195924402885,\n",
       " 'VIL804': 1.0110890525640923,\n",
       " 'VIL811': 0.9852803811023914,\n",
       " 'VIL83': 1.0124434826788584,\n",
       " 'VIL844': 0.9719341288394733,\n",
       " 'VIL85': 0.9999245762263012,\n",
       " 'VIL862': 0.9994253046114826,\n",
       " 'VIL870': 1.0074534528463153,\n",
       " 'VIL96': 1.0002537182333848}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {} \n",
    "for key, subdf in df2.groupby(\"Group\"):\n",
    "    if key == \"Vector\":\n",
    "        continue\n",
    "    res[key] = np.mean(subdf[\"Area\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res</th>\n",
       "      <th>board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL114</th>\n",
       "      <td>0.961639</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL129</th>\n",
       "      <td>0.982724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL135</th>\n",
       "      <td>0.973960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL157</th>\n",
       "      <td>0.956989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL170</th>\n",
       "      <td>0.963284</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL786</th>\n",
       "      <td>0.965786</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL811</th>\n",
       "      <td>0.964437</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL844</th>\n",
       "      <td>0.951699</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL862</th>\n",
       "      <td>0.978822</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL870</th>\n",
       "      <td>0.964877</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             res  board\n",
       "VIL114  0.961639      1\n",
       "VIL129  0.982724      1\n",
       "VIL135  0.973960      1\n",
       "VIL157  0.956989      1\n",
       "VIL170  0.963284      1\n",
       "...          ...    ...\n",
       "VIL786  0.965786      6\n",
       "VIL811  0.964437      6\n",
       "VIL844  0.951699      6\n",
       "VIL862  0.978822      6\n",
       "VIL870  0.964877      6\n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "res = {}\n",
    "keys = {}\n",
    "for key, subdf in dfexp3.groupby(\"Type\"):\n",
    "    df2 = subdf[subdf[\"Group\"] == \"Vector\"] \n",
    "    for key2, subdf2 in subdf.groupby(\"Group\"):\n",
    "        if key2 == \"Vector\":\n",
    "            continue\n",
    "        t = np.mean(subdf2[\"Area\"]) / np.mean(df2[\"Area\"])\n",
    "        # t_stat, p_value = ttest_ind(df2[\"Area\"], subdf2[\"Area\"], alternative=\"greater\")\n",
    "        res[key2] = t\n",
    "        keys[key2] = key\n",
    "    # break\n",
    "res = pd.DataFrame({\"res\":res, \"board\":keys})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.set_index(\"\") \n",
    "# subres = res#[res[\"board\"] == 4]\n",
    "# print(subres)\n",
    "df[\"allres_area\"] = dict(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name2</th>\n",
       "      <th>predict</th>\n",
       "      <th>res1_area</th>\n",
       "      <th>res1_max</th>\n",
       "      <th>res2_max</th>\n",
       "      <th>res2_area</th>\n",
       "      <th>res1_area_value</th>\n",
       "      <th>res1_max_value</th>\n",
       "      <th>res2_area_value</th>\n",
       "      <th>res2_max_value</th>\n",
       "      <th>allres_max</th>\n",
       "      <th>allres_area</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL1</th>\n",
       "      <td>0</td>\n",
       "      <td>MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...</td>\n",
       "      <td>GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...</td>\n",
       "      <td>VILH-1</td>\n",
       "      <td>0.412242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.258889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993396</td>\n",
       "      <td>0.970283</td>\n",
       "      <td>0.989087</td>\n",
       "      <td>0.994915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL2</th>\n",
       "      <td>1</td>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VILH-2</td>\n",
       "      <td>0.241429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL3</th>\n",
       "      <td>2</td>\n",
       "      <td>MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...</td>\n",
       "      <td>GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...</td>\n",
       "      <td>VILH-3</td>\n",
       "      <td>0.413478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL4</th>\n",
       "      <td>3</td>\n",
       "      <td>MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...</td>\n",
       "      <td>GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...</td>\n",
       "      <td>VILH-4</td>\n",
       "      <td>0.465354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL5</th>\n",
       "      <td>4</td>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VILH-5</td>\n",
       "      <td>0.440059</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0.021245</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.967561</td>\n",
       "      <td>0.942221</td>\n",
       "      <td>0.948404</td>\n",
       "      <td>0.930512</td>\n",
       "      <td>0.987171</td>\n",
       "      <td>0.991138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL869</th>\n",
       "      <td>880</td>\n",
       "      <td>MDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYPII...</td>\n",
       "      <td>GSMDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYP...</td>\n",
       "      <td>VILH-869</td>\n",
       "      <td>0.056472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL870</th>\n",
       "      <td>881</td>\n",
       "      <td>MLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQLL...</td>\n",
       "      <td>GSMLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQ...</td>\n",
       "      <td>VILH-870</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.982582</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.964877</td>\n",
       "      <td>0.961165</td>\n",
       "      <td>1.082575</td>\n",
       "      <td>1.051115</td>\n",
       "      <td>1.013477</td>\n",
       "      <td>1.007453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL871</th>\n",
       "      <td>882</td>\n",
       "      <td>MNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVSRL...</td>\n",
       "      <td>GSMNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVS...</td>\n",
       "      <td>VILH-871</td>\n",
       "      <td>0.615352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL872</th>\n",
       "      <td>883</td>\n",
       "      <td>MDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNISR...</td>\n",
       "      <td>GSMDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNI...</td>\n",
       "      <td>VILH-872</td>\n",
       "      <td>0.231382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL873</th>\n",
       "      <td>884</td>\n",
       "      <td>MATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFLPD...</td>\n",
       "      <td>GSMATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFL...</td>\n",
       "      <td>VILH-873</td>\n",
       "      <td>0.065387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num                                                 \\\n",
       "                                                            \n",
       "VIL1      0  MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...   \n",
       "VIL2      1  MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "VIL3      2  MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...   \n",
       "VIL4      3  MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...   \n",
       "VIL5      4  MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "...     ...                                                ...   \n",
       "VIL869  880  MDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYPII...   \n",
       "VIL870  881  MLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQLL...   \n",
       "VIL871  882  MNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVSRL...   \n",
       "VIL872  883  MDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNISR...   \n",
       "VIL873  884  MATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFLPD...   \n",
       "\n",
       "                                                         name2   predict  \\\n",
       "                                                                           \n",
       "VIL1    GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...    VILH-1  0.412242   \n",
       "VIL2    GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...    VILH-2  0.241429   \n",
       "VIL3    GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...    VILH-3  0.413478   \n",
       "VIL4    GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...    VILH-4  0.465354   \n",
       "VIL5    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...    VILH-5  0.440059   \n",
       "...                                                   ...       ...       ...   \n",
       "VIL869  GSMDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYP...  VILH-869  0.056472   \n",
       "VIL870  GSMLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQ...  VILH-870  0.050262   \n",
       "VIL871  GSMNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVS...  VILH-871  0.615352   \n",
       "VIL872  GSMDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNI...  VILH-872  0.231382   \n",
       "VIL873  GSMATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFL...  VILH-873  0.065387   \n",
       "\n",
       "        res1_area  res1_max  res2_max  res2_area  res1_area_value  \\\n",
       "                                                               \n",
       "VIL1          NaN       NaN  0.024162   0.258889              NaN   \n",
       "VIL2          NaN       NaN       NaN        NaN              NaN   \n",
       "VIL3          NaN       NaN       NaN        NaN              NaN   \n",
       "VIL4          NaN       NaN       NaN        NaN              NaN   \n",
       "VIL5     0.006986  0.021245  0.003078   0.005220         0.967561   \n",
       "...           ...       ...       ...        ...              ...   \n",
       "VIL869        NaN       NaN       NaN        NaN              NaN   \n",
       "VIL870   0.000228  0.019792  0.982582   0.999200         0.964877   \n",
       "VIL871        NaN       NaN       NaN        NaN              NaN   \n",
       "VIL872        NaN       NaN       NaN        NaN              NaN   \n",
       "VIL873        NaN       NaN       NaN        NaN              NaN   \n",
       "\n",
       "        res1_max_value  res2_area_value  res2_max_value  allres_max  \\\n",
       "                                                                 \n",
       "VIL1               NaN         0.993396        0.970283    0.989087   \n",
       "VIL2               NaN              NaN             NaN         NaN   \n",
       "VIL3               NaN              NaN             NaN         NaN   \n",
       "VIL4               NaN              NaN             NaN         NaN   \n",
       "VIL5          0.942221         0.948404        0.930512    0.987171   \n",
       "...                ...              ...             ...         ...   \n",
       "VIL869             NaN              NaN             NaN         NaN   \n",
       "VIL870        0.961165         1.082575        1.051115    1.013477   \n",
       "VIL871             NaN              NaN             NaN         NaN   \n",
       "VIL872             NaN              NaN             NaN         NaN   \n",
       "VIL873             NaN              NaN             NaN         NaN   \n",
       "\n",
       "        allres_area  \n",
       "                \n",
       "VIL1       0.994915  \n",
       "VIL2            NaN  \n",
       "VIL3            NaN  \n",
       "VIL4            NaN  \n",
       "VIL5       0.991138  \n",
       "...             ...  \n",
       "VIL869          NaN  \n",
       "VIL870     1.007453  \n",
       "VIL871          NaN  \n",
       "VIL872          NaN  \n",
       "VIL873          NaN  \n",
       "\n",
       "[873 rows x 15 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./exp_res/exp3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name2</th>\n",
       "      <th>predict</th>\n",
       "      <th>res1_area</th>\n",
       "      <th>res1_max</th>\n",
       "      <th>res2_max</th>\n",
       "      <th>res2_area</th>\n",
       "      <th>res1_area_value</th>\n",
       "      <th>res1_max_value</th>\n",
       "      <th>res2_area_value</th>\n",
       "      <th>res2_max_value</th>\n",
       "      <th>allres_max</th>\n",
       "      <th>allres_area</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL5</th>\n",
       "      <td>4</td>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VILH-5</td>\n",
       "      <td>0.440059</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0.021245</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.967561</td>\n",
       "      <td>0.942221</td>\n",
       "      <td>0.948404</td>\n",
       "      <td>0.930512</td>\n",
       "      <td>0.987171</td>\n",
       "      <td>0.991138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL74</th>\n",
       "      <td>75</td>\n",
       "      <td>MQTWEIILVAVTAFLAFIALMYMIIWGYLNRKYAPWCLTPCVERIS...</td>\n",
       "      <td>GSMQTWEIILVAVTAFLAFIALMYMIIWGYLNRKYAPWCLTPCVER...</td>\n",
       "      <td>VILH-74</td>\n",
       "      <td>0.561506</td>\n",
       "      <td>0.938561</td>\n",
       "      <td>0.888777</td>\n",
       "      <td>0.087827</td>\n",
       "      <td>0.126210</td>\n",
       "      <td>1.013181</td>\n",
       "      <td>1.019826</td>\n",
       "      <td>0.978658</td>\n",
       "      <td>0.974032</td>\n",
       "      <td>1.053450</td>\n",
       "      <td>1.031356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL80</th>\n",
       "      <td>81</td>\n",
       "      <td>MKCLIGICMLASVVLRCGGHIPLRMPTLYGKHETFVPPPSPPPLPP...</td>\n",
       "      <td>GSMKCLIGICMLASVVLRCGGHIPLRMPTLYGKHETFVPPPSPPPL...</td>\n",
       "      <td>VILH-80</td>\n",
       "      <td>0.214980</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.021961</td>\n",
       "      <td>0.990634</td>\n",
       "      <td>0.964502</td>\n",
       "      <td>0.963352</td>\n",
       "      <td>0.968502</td>\n",
       "      <td>1.017769</td>\n",
       "      <td>1.011320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL83</th>\n",
       "      <td>84</td>\n",
       "      <td>MPRYLLLCSLAASVFWKLSSSHDCEDDYFIMPKMNVRKSMLYRVPV...</td>\n",
       "      <td>GSMPRYLLLCSLAASVFWKLSSSHDCEDDYFIMPKMNVRKSMLYRV...</td>\n",
       "      <td>VILH-83</td>\n",
       "      <td>0.197795</td>\n",
       "      <td>0.141105</td>\n",
       "      <td>0.010248</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>0.018409</td>\n",
       "      <td>0.988519</td>\n",
       "      <td>0.952795</td>\n",
       "      <td>0.968586</td>\n",
       "      <td>0.966579</td>\n",
       "      <td>1.009877</td>\n",
       "      <td>1.012443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL85</th>\n",
       "      <td>86</td>\n",
       "      <td>MKLASVCLLIWTLDHLLQNSNGSMTQNQLYFDEEPTEDDIETPLSI...</td>\n",
       "      <td>GSMKLASVCLLIWTLDHLLQNSNGSMTQNQLYFDEEPTEDDIETPL...</td>\n",
       "      <td>VILH-85</td>\n",
       "      <td>0.111320</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.027248</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>0.968636</td>\n",
       "      <td>0.936745</td>\n",
       "      <td>0.966526</td>\n",
       "      <td>0.962491</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.999925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL804</th>\n",
       "      <td>812</td>\n",
       "      <td>MAHINSLILLLMLTETGSSVNIQLLQSLGVINTHKRQLAFYNQQPP...</td>\n",
       "      <td>GSMAHINSLILLLMLTETGSSVNIQLLQSLGVINTHKRQLAFYNQQ...</td>\n",
       "      <td>VILH-804</td>\n",
       "      <td>0.038971</td>\n",
       "      <td>0.937200</td>\n",
       "      <td>0.939110</td>\n",
       "      <td>0.911651</td>\n",
       "      <td>0.513347</td>\n",
       "      <td>1.016195</td>\n",
       "      <td>1.037211</td>\n",
       "      <td>1.000452</td>\n",
       "      <td>1.016932</td>\n",
       "      <td>1.036435</td>\n",
       "      <td>1.011089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL811</th>\n",
       "      <td>819</td>\n",
       "      <td>MEKAILTELSSFLSSIRAFFISLSPYLNLILAMAFMYCLAYLISSI...</td>\n",
       "      <td>GSMEKAILTELSSFLSSIRAFFISLSPYLNLILAMAFMYCLAYLIS...</td>\n",
       "      <td>VILH-811</td>\n",
       "      <td>0.489038</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.067272</td>\n",
       "      <td>0.167929</td>\n",
       "      <td>0.964437</td>\n",
       "      <td>0.939059</td>\n",
       "      <td>0.988132</td>\n",
       "      <td>0.972939</td>\n",
       "      <td>0.977464</td>\n",
       "      <td>0.985280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL844</th>\n",
       "      <td>853</td>\n",
       "      <td>MFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHMFC...</td>\n",
       "      <td>GSMFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHM...</td>\n",
       "      <td>VILH-844</td>\n",
       "      <td>0.064153</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.007893</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>0.951699</td>\n",
       "      <td>0.919492</td>\n",
       "      <td>0.973641</td>\n",
       "      <td>0.948133</td>\n",
       "      <td>0.956031</td>\n",
       "      <td>0.971934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL862</th>\n",
       "      <td>873</td>\n",
       "      <td>MSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQGLP...</td>\n",
       "      <td>GSMSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQG...</td>\n",
       "      <td>VILH-862</td>\n",
       "      <td>0.117439</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.397193</td>\n",
       "      <td>0.508998</td>\n",
       "      <td>0.978822</td>\n",
       "      <td>0.955788</td>\n",
       "      <td>1.000486</td>\n",
       "      <td>0.992233</td>\n",
       "      <td>0.995339</td>\n",
       "      <td>0.999425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL870</th>\n",
       "      <td>881</td>\n",
       "      <td>MLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQLL...</td>\n",
       "      <td>GSMLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQ...</td>\n",
       "      <td>VILH-870</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.982582</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.964877</td>\n",
       "      <td>0.961165</td>\n",
       "      <td>1.082575</td>\n",
       "      <td>1.051115</td>\n",
       "      <td>1.013477</td>\n",
       "      <td>1.007453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num                                                 \\\n",
       "                                                            \n",
       "VIL5      4  MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "VIL74    75  MQTWEIILVAVTAFLAFIALMYMIIWGYLNRKYAPWCLTPCVERIS...   \n",
       "VIL80    81  MKCLIGICMLASVVLRCGGHIPLRMPTLYGKHETFVPPPSPPPLPP...   \n",
       "VIL83    84  MPRYLLLCSLAASVFWKLSSSHDCEDDYFIMPKMNVRKSMLYRVPV...   \n",
       "VIL85    86  MKLASVCLLIWTLDHLLQNSNGSMTQNQLYFDEEPTEDDIETPLSI...   \n",
       "...     ...                                                ...   \n",
       "VIL804  812  MAHINSLILLLMLTETGSSVNIQLLQSLGVINTHKRQLAFYNQQPP...   \n",
       "VIL811  819  MEKAILTELSSFLSSIRAFFISLSPYLNLILAMAFMYCLAYLISSI...   \n",
       "VIL844  853  MFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHMFC...   \n",
       "VIL862  873  MSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQGLP...   \n",
       "VIL870  881  MLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQLL...   \n",
       "\n",
       "                                                         name2   predict  \\\n",
       "                                                                           \n",
       "VIL5    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...    VILH-5  0.440059   \n",
       "VIL74   GSMQTWEIILVAVTAFLAFIALMYMIIWGYLNRKYAPWCLTPCVER...   VILH-74  0.561506   \n",
       "VIL80   GSMKCLIGICMLASVVLRCGGHIPLRMPTLYGKHETFVPPPSPPPL...   VILH-80  0.214980   \n",
       "VIL83   GSMPRYLLLCSLAASVFWKLSSSHDCEDDYFIMPKMNVRKSMLYRV...   VILH-83  0.197795   \n",
       "VIL85   GSMKLASVCLLIWTLDHLLQNSNGSMTQNQLYFDEEPTEDDIETPL...   VILH-85  0.111320   \n",
       "...                                                   ...       ...       ...   \n",
       "VIL804  GSMAHINSLILLLMLTETGSSVNIQLLQSLGVINTHKRQLAFYNQQ...  VILH-804  0.038971   \n",
       "VIL811  GSMEKAILTELSSFLSSIRAFFISLSPYLNLILAMAFMYCLAYLIS...  VILH-811  0.489038   \n",
       "VIL844  GSMFLKLVDDHALVVNVILWCVVLIVLLLVCITIIKLIKLCFTCHM...  VILH-844  0.064153   \n",
       "VIL862  GSMSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQG...  VILH-862  0.117439   \n",
       "VIL870  GSMLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQ...  VILH-870  0.050262   \n",
       "\n",
       "        res1_area  res1_max  res2_max  res2_area  res1_area_value  \\\n",
       "                                                               \n",
       "VIL5     0.006986  0.021245  0.003078   0.005220         0.967561   \n",
       "VIL74    0.938561  0.888777  0.087827   0.126210         1.013181   \n",
       "VIL80    0.043579  0.010450  0.028689   0.021961         0.990634   \n",
       "VIL83    0.141105  0.010248  0.020457   0.018409         0.988519   \n",
       "VIL85    0.001034  0.000777  0.027248   0.016527         0.968636   \n",
       "...           ...       ...       ...        ...              ...   \n",
       "VIL804   0.937200  0.939110  0.911651   0.513347         1.016195   \n",
       "VIL811   0.000261  0.003081  0.067272   0.167929         0.964437   \n",
       "VIL844   0.000515  0.001484  0.007893   0.030656         0.951699   \n",
       "VIL862   0.012208  0.014536  0.397193   0.508998         0.978822   \n",
       "VIL870   0.000228  0.019792  0.982582   0.999200         0.964877   \n",
       "\n",
       "        res1_max_value  res2_area_value  res2_max_value  allres_max  \\\n",
       "                                                                 \n",
       "VIL5          0.942221         0.948404        0.930512    0.987171   \n",
       "VIL74         1.019826         0.978658        0.974032    1.053450   \n",
       "VIL80         0.964502         0.963352        0.968502    1.017769   \n",
       "VIL83         0.952795         0.968586        0.966579    1.009877   \n",
       "VIL85         0.936745         0.966526        0.962491    0.998400   \n",
       "...                ...              ...             ...         ...   \n",
       "VIL804        1.037211         1.000452        1.016932    1.036435   \n",
       "VIL811        0.939059         0.988132        0.972939    0.977464   \n",
       "VIL844        0.919492         0.973641        0.948133    0.956031   \n",
       "VIL862        0.955788         1.000486        0.992233    0.995339   \n",
       "VIL870        0.961165         1.082575        1.051115    1.013477   \n",
       "\n",
       "        allres_area  \n",
       "                \n",
       "VIL5       0.991138  \n",
       "VIL74      1.031356  \n",
       "VIL80      1.011320  \n",
       "VIL83      1.012443  \n",
       "VIL85      0.999925  \n",
       "...             ...  \n",
       "VIL804     1.011089  \n",
       "VIL811     0.985280  \n",
       "VIL844     0.971934  \n",
       "VIL862     0.999425  \n",
       "VIL870     1.007453  \n",
       "\n",
       "[63 rows x 15 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df[~pd.isnull(df[\"res1_max\"])]\n",
    "df3 = df3[~pd.isnull(df3[\"res2_max\"])]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=-0.19945276497695855, pvalue=0.11706848002335887)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "stats.spearmanr(-df3[\"predict\"], df3[\"allres_area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name2</th>\n",
       "      <th>predict</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL1</th>\n",
       "      <td>0</td>\n",
       "      <td>MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...</td>\n",
       "      <td>GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...</td>\n",
       "      <td>VILH-1</td>\n",
       "      <td>0.412242</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL2</th>\n",
       "      <td>1</td>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VILH-2</td>\n",
       "      <td>0.241429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL3</th>\n",
       "      <td>2</td>\n",
       "      <td>MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...</td>\n",
       "      <td>GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...</td>\n",
       "      <td>VILH-3</td>\n",
       "      <td>0.413478</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL4</th>\n",
       "      <td>3</td>\n",
       "      <td>MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...</td>\n",
       "      <td>GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...</td>\n",
       "      <td>VILH-4</td>\n",
       "      <td>0.465354</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL5</th>\n",
       "      <td>4</td>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VILH-5</td>\n",
       "      <td>0.440059</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL869</th>\n",
       "      <td>880</td>\n",
       "      <td>MDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYPII...</td>\n",
       "      <td>GSMDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYP...</td>\n",
       "      <td>VILH-869</td>\n",
       "      <td>0.056472</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL870</th>\n",
       "      <td>881</td>\n",
       "      <td>MLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQLL...</td>\n",
       "      <td>GSMLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQ...</td>\n",
       "      <td>VILH-870</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL871</th>\n",
       "      <td>882</td>\n",
       "      <td>MNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVSRL...</td>\n",
       "      <td>GSMNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVS...</td>\n",
       "      <td>VILH-871</td>\n",
       "      <td>0.615352</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL872</th>\n",
       "      <td>883</td>\n",
       "      <td>MDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNISR...</td>\n",
       "      <td>GSMDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNI...</td>\n",
       "      <td>VILH-872</td>\n",
       "      <td>0.231382</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL873</th>\n",
       "      <td>884</td>\n",
       "      <td>MATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFLPD...</td>\n",
       "      <td>GSMATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFL...</td>\n",
       "      <td>VILH-873</td>\n",
       "      <td>0.065387</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num                                                 \\\n",
       "                                                            \n",
       "VIL1      0  MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...   \n",
       "VIL2      1  MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "VIL3      2  MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...   \n",
       "VIL4      3  MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...   \n",
       "VIL5      4  MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "...     ...                                                ...   \n",
       "VIL869  880  MDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYPII...   \n",
       "VIL870  881  MLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQLL...   \n",
       "VIL871  882  MNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVSRL...   \n",
       "VIL872  883  MDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNISR...   \n",
       "VIL873  884  MATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFLPD...   \n",
       "\n",
       "                                                         name2   predict  \\\n",
       "                                                                           \n",
       "VIL1    GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...    VILH-1  0.412242   \n",
       "VIL2    GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...    VILH-2  0.241429   \n",
       "VIL3    GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...    VILH-3  0.413478   \n",
       "VIL4    GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...    VILH-4  0.465354   \n",
       "VIL5    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...    VILH-5  0.440059   \n",
       "...                                                   ...       ...       ...   \n",
       "VIL869  GSMDPNQTNVVPPALHLVDPQIQLTITRMEDAMGQGQNSADPKVYP...  VILH-869  0.056472   \n",
       "VIL870  GSMLPPCYNFLKEQHCQKASTQREAEAAVKPLLAPHHVVAVIQEIQ...  VILH-870  0.050262   \n",
       "VIL871  GSMNALLVILVAIAAKSADQAELCNKQQQQGPFTFADYQESPLNVS...  VILH-871  0.615352   \n",
       "VIL872  GSMDSTIRLVATIFLISLTQQIEVCNKAQQQGPYTLVDYQEKPLNI...  VILH-872  0.231382   \n",
       "VIL873  GSMATTHTLLSFDNLEFLLHRKDLTDLYGKRCGTLNLVINPYELFL...  VILH-873  0.065387   \n",
       "\n",
       "        res  \n",
       "        \n",
       "VIL1    NaN  \n",
       "VIL2    NaN  \n",
       "VIL3    NaN  \n",
       "VIL4    NaN  \n",
       "VIL5    NaN  \n",
       "...     ...  \n",
       "VIL869  NaN  \n",
       "VIL870  NaN  \n",
       "VIL871  NaN  \n",
       "VIL872  NaN  \n",
       "VIL873  NaN  \n",
       "\n",
       "[873 rows x 6 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
