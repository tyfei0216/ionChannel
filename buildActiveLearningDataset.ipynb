{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test train mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import trainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/newtest/0_test7/\"\n",
    "with open(os.path.join(path, \"config.json\"), \"r\") as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initized model for active_learning stage\n",
      "fix model for active learning\n"
     ]
    }
   ],
   "source": [
    "pretrain_model = trainUtils.loadPretrainModel(configs)\n",
    "model = trainUtils.buildModel(configs, pretrain_model)\n",
    "ds = trainUtils.loadDataset(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.l1.weight Parameter containing:\n",
      "tensor([[ 4.7598e-03,  2.7982e-04,  2.3509e-04,  ...,  9.4208e-04,\n",
      "         -8.4217e-03, -3.2199e-04],\n",
      "        [-6.7978e-05,  6.4566e-05, -1.7740e-04,  ..., -7.2887e-05,\n",
      "         -1.9631e-04, -1.0491e-04],\n",
      "        [-1.5523e-04,  4.0233e-04,  1.1416e-04,  ...,  7.5458e-05,\n",
      "         -9.3520e-05,  3.8248e-04],\n",
      "        ...,\n",
      "        [-5.3433e-05, -7.5677e-05,  6.6843e-05,  ...,  1.2642e-04,\n",
      "         -1.9088e-04,  5.5274e-04],\n",
      "        [ 9.8540e-06,  1.0747e-04,  3.4970e-04,  ...,  6.1645e-05,\n",
      "          2.0251e-05,  1.6988e-06],\n",
      "        [-2.2824e-03,  5.7999e-03, -4.2507e-03,  ...,  4.6479e-03,\n",
      "          9.7588e-04,  6.5786e-03]], requires_grad=True)\n",
      "clf.l1.bias Parameter containing:\n",
      "tensor([ 2.7059e-02, -4.9259e-03,  2.7718e-03,  2.0553e-02, -2.1533e-03,\n",
      "         7.9627e-04,  8.5251e-03,  2.4621e-02,  1.8997e-02, -1.3601e-03,\n",
      "         1.6795e-02, -5.0541e-03, -3.2794e-03, -3.4163e-04, -3.4138e-03,\n",
      "         3.1380e-03,  1.8376e-03,  5.2309e-03,  3.4471e-04, -2.1188e-03,\n",
      "         6.9405e-03,  1.7858e-02,  5.4046e-03, -6.3481e-04,  1.8994e-02,\n",
      "         1.0636e-02,  1.1988e-02,  2.7653e-03,  4.7640e-04, -2.9453e-03,\n",
      "         1.4375e-02, -6.7162e-04,  1.0564e-02,  7.6413e-03, -4.6240e-03,\n",
      "         1.9092e-02,  2.0097e-02, -2.7166e-03,  1.5500e-03,  7.2315e-03,\n",
      "         1.1271e-02, -7.5486e-03,  7.1833e-03, -6.7571e-05,  2.5260e-02,\n",
      "         9.6401e-03,  1.9571e-02,  8.5482e-04,  1.5371e-02,  3.0834e-03,\n",
      "        -1.0163e-03,  1.4931e-02,  3.2885e-05, -1.4255e-03, -3.9001e-03,\n",
      "         6.3257e-03,  2.1988e-02,  2.1619e-02,  5.2513e-03,  8.8026e-03,\n",
      "        -3.5181e-03, -4.1491e-04,  4.7775e-03,  4.1770e-03,  9.3377e-03,\n",
      "         1.7613e-02,  1.2844e-02,  1.6569e-02,  1.3979e-03,  6.3432e-04,\n",
      "        -4.4290e-03,  2.3942e-03,  2.0433e-02,  2.7272e-02, -3.2290e-04,\n",
      "        -1.3797e-03,  8.7811e-03, -2.9789e-03,  4.4533e-03, -6.8444e-04,\n",
      "        -6.0964e-03, -3.6044e-03,  7.5350e-03,  3.7027e-03,  6.7724e-03,\n",
      "         7.0945e-03,  4.2549e-03, -3.9474e-03,  7.1785e-04, -3.2531e-03,\n",
      "        -3.4390e-04,  4.0403e-03,  1.7918e-03, -1.4318e-03, -8.2056e-04,\n",
      "         1.3510e-02, -6.8720e-04,  5.0448e-03,  1.3663e-02,  1.6795e-02,\n",
      "         2.1157e-02,  2.3493e-02,  4.9648e-05,  1.2877e-02,  1.1987e-02,\n",
      "        -3.6802e-03,  1.8053e-03, -1.2312e-03, -2.3499e-03, -3.0143e-05,\n",
      "        -7.1021e-04, -7.3528e-04, -1.6768e-03, -8.9369e-03,  7.4819e-05,\n",
      "         5.7809e-03, -1.3686e-03,  8.2701e-03,  4.6834e-03, -5.3501e-04,\n",
      "         7.0048e-03,  5.4708e-04,  8.4394e-03,  1.9587e-02,  2.5364e-03,\n",
      "        -2.1603e-03,  9.6282e-03, -2.3973e-04,  3.0173e-03, -2.4738e-03,\n",
      "         1.4490e-02,  1.0928e-02,  1.6869e-03,  3.1616e-02,  1.0933e-02,\n",
      "        -2.8990e-03, -1.4757e-03,  1.2988e-02,  6.7962e-03,  1.2717e-02,\n",
      "        -5.6548e-04, -2.2132e-03, -1.7649e-03, -4.6272e-03, -4.3288e-03,\n",
      "        -1.3283e-03,  1.0941e-04,  1.3182e-03,  1.2819e-03,  3.1767e-03,\n",
      "         1.0514e-02,  1.0943e-02,  2.0514e-03,  2.5799e-03, -5.0479e-03,\n",
      "         3.0746e-04, -8.7498e-03,  1.8564e-03,  3.3741e-03,  1.3036e-02,\n",
      "        -8.8196e-04,  3.1790e-03,  6.0961e-03,  1.3905e-02,  8.1442e-03,\n",
      "         7.0504e-04,  3.0642e-02, -1.2619e-03, -3.3708e-03, -5.0228e-03,\n",
      "        -6.0879e-04, -5.2651e-04,  5.5066e-04, -2.9769e-03,  2.7063e-02,\n",
      "         4.5581e-04,  1.0504e-04, -1.5428e-03, -2.6831e-03,  1.0995e-02,\n",
      "         7.1602e-04, -2.4632e-03,  2.5742e-02,  2.6252e-03,  7.9425e-03,\n",
      "         1.6540e-02,  7.2700e-04,  1.4615e-02,  9.5034e-04, -2.2643e-03,\n",
      "        -1.5288e-03,  1.2399e-03,  2.3937e-02,  7.1039e-03, -2.1557e-03,\n",
      "         1.4468e-02,  1.8020e-03, -9.5773e-05, -4.8904e-03, -2.0957e-03,\n",
      "         3.1461e-03,  4.5388e-03, -1.3354e-03,  2.0716e-02,  2.1788e-02,\n",
      "         7.8400e-03, -9.3019e-03,  6.8221e-03,  4.5849e-03,  1.4871e-03,\n",
      "         1.3279e-03,  6.1626e-03, -6.7602e-03, -2.4222e-03,  1.2276e-02,\n",
      "         1.9063e-02, -4.8886e-03,  8.2698e-03, -5.2445e-03, -1.4363e-03,\n",
      "         1.0675e-02,  2.4509e-03, -8.1368e-03,  1.6686e-02, -3.5806e-03,\n",
      "         3.7915e-03, -9.8592e-04,  2.4423e-04,  6.9823e-03,  7.8695e-03,\n",
      "         1.7099e-02,  2.2576e-03,  1.2357e-02,  2.1900e-03, -2.2207e-03,\n",
      "         2.7575e-03,  1.3977e-03, -5.9502e-03,  2.5560e-02,  5.0582e-03,\n",
      "        -6.1385e-04,  9.0380e-03, -3.5629e-03, -2.2944e-03,  8.0536e-04,\n",
      "         3.5326e-03,  3.3200e-03,  8.3192e-03,  3.7504e-03,  4.0423e-03,\n",
      "         1.1597e-02,  1.1839e-03, -1.3299e-03, -5.0426e-03,  8.2780e-03,\n",
      "        -1.2192e-02,  8.0012e-04,  9.3295e-04,  1.7818e-02, -1.3538e-03,\n",
      "        -4.3910e-03,  4.5436e-03,  1.5396e-03,  4.3449e-04, -5.3030e-05,\n",
      "         1.0604e-02,  6.7441e-04,  1.5474e-02, -2.0972e-03, -3.1207e-03,\n",
      "         9.7049e-05,  2.7884e-03,  2.3000e-02, -3.8325e-03, -6.0103e-04,\n",
      "         4.6048e-03,  1.8129e-03, -1.1277e-03,  5.4807e-03, -4.4414e-03,\n",
      "         8.9184e-03, -6.0679e-03, -2.9926e-04,  2.1193e-02,  3.0398e-03,\n",
      "         2.0653e-03,  4.6319e-03, -6.3409e-03,  1.4469e-02,  7.4128e-03,\n",
      "         7.5106e-03,  6.6806e-03,  4.0835e-03,  1.0623e-02,  6.6581e-03,\n",
      "        -9.8173e-04,  9.6303e-03,  4.6771e-03, -2.0889e-03, -1.2257e-03,\n",
      "        -3.9640e-03,  7.6039e-03,  1.0969e-02,  1.1895e-02,  3.3002e-03,\n",
      "        -2.9613e-03,  7.5659e-03, -2.4943e-03,  1.7753e-02, -7.7484e-03,\n",
      "         1.0409e-03,  2.1341e-02, -3.3067e-03, -3.6302e-04,  6.6347e-04,\n",
      "         1.4229e-02,  1.7168e-03,  1.6224e-03,  2.6636e-03,  1.4528e-03,\n",
      "        -3.8831e-03,  2.6177e-03,  1.2487e-03,  3.4330e-02, -1.5426e-03,\n",
      "         6.1356e-03,  6.7863e-03,  1.8940e-02,  1.3457e-02, -4.7447e-03,\n",
      "        -4.3471e-03,  1.7144e-02, -3.7257e-03,  9.5209e-03, -1.2313e-02,\n",
      "         1.4010e-03,  2.9750e-03, -2.0666e-03,  5.4936e-03, -1.9283e-03,\n",
      "         5.2129e-03,  7.2292e-04,  4.4486e-04, -4.0783e-03,  2.3992e-02,\n",
      "        -1.7951e-03,  2.7905e-03,  1.4486e-02,  1.1818e-02,  2.3981e-02,\n",
      "        -2.7768e-03,  9.5448e-04, -2.1491e-03, -1.3335e-03,  4.2485e-03,\n",
      "        -3.5585e-03,  7.5255e-05,  1.9174e-02,  1.7852e-02, -1.7051e-03,\n",
      "         1.1748e-02, -1.4736e-03,  9.9530e-03, -2.4044e-03,  1.0269e-03,\n",
      "         6.0402e-03,  1.3579e-02,  6.1073e-03,  1.6490e-02,  1.2079e-02,\n",
      "        -8.1956e-03,  4.0720e-04,  4.0213e-03, -4.2037e-04,  2.4058e-03,\n",
      "         1.2557e-02, -1.8284e-03,  3.5235e-03,  5.8917e-03, -3.1339e-03,\n",
      "        -3.2735e-04, -6.5394e-03,  5.8536e-05,  7.4382e-03, -4.4282e-03,\n",
      "         1.5009e-02, -4.3444e-03, -3.2285e-03,  4.5215e-03,  1.1931e-03,\n",
      "         3.6385e-03,  3.0046e-04, -3.4239e-03, -3.6538e-03, -2.4582e-03,\n",
      "         8.5725e-03,  3.0659e-02, -5.5656e-04,  1.0124e-02, -3.6027e-03,\n",
      "         3.9538e-03,  1.4337e-02, -3.2659e-03,  1.2460e-02,  2.8021e-03,\n",
      "         3.3459e-03,  1.5633e-02, -1.6991e-03, -2.8325e-03,  7.3342e-03,\n",
      "         3.9029e-03, -3.5698e-03,  1.4134e-02, -1.7944e-03,  5.6104e-04,\n",
      "         1.0041e-02, -7.6268e-03, -3.3405e-03, -1.7449e-03, -7.7305e-05,\n",
      "        -6.4162e-04, -2.0342e-03,  7.3709e-03,  1.3969e-02,  2.5623e-02,\n",
      "        -2.6111e-03,  6.5217e-03,  1.8845e-03,  9.7355e-04, -8.5702e-04,\n",
      "        -2.1923e-03, -5.4815e-04,  1.4190e-02,  6.9969e-03,  1.5043e-03,\n",
      "         4.6042e-03,  2.0241e-04,  6.3654e-03, -2.9341e-03, -6.4787e-04,\n",
      "        -1.0521e-03, -1.0815e-02, -3.6657e-04, -7.1704e-04, -8.6927e-04,\n",
      "         7.5885e-04, -6.7042e-03, -9.2226e-03,  6.4922e-03, -1.2684e-03,\n",
      "        -3.7609e-04,  4.0937e-04,  5.1048e-03,  1.1922e-03,  6.3933e-03,\n",
      "        -2.1272e-04,  2.4106e-02,  2.1387e-04,  1.2420e-02, -7.7245e-04,\n",
      "         1.1770e-02, -2.3846e-04,  1.6786e-02,  2.7256e-03,  9.4636e-04,\n",
      "        -2.0598e-03, -1.6212e-03,  5.0318e-03,  1.7709e-02, -5.1467e-03,\n",
      "         2.3995e-03, -4.7790e-03, -1.0886e-03, -3.4055e-05,  9.4659e-03,\n",
      "         3.1977e-03,  1.0257e-02,  2.5483e-02,  4.3573e-03,  7.4730e-03,\n",
      "         2.7757e-03,  9.5854e-04,  4.5710e-03,  4.6297e-03,  2.4058e-03,\n",
      "         3.6280e-03, -3.6066e-03, -6.0953e-03,  6.7053e-03, -2.8799e-03,\n",
      "        -3.3198e-03,  3.6459e-03,  4.3398e-04,  7.3781e-03,  5.1935e-03,\n",
      "         9.4867e-04,  1.1463e-02, -3.0102e-03,  1.5304e-03,  2.2625e-02,\n",
      "        -3.4773e-03, -1.3378e-04, -3.8356e-03,  1.4304e-02, -2.4689e-03,\n",
      "        -1.1984e-03, -4.8994e-03, -2.1751e-04, -1.0675e-03, -2.1473e-03,\n",
      "        -1.9161e-03, -6.8240e-03,  2.2865e-02, -6.0986e-04,  1.2337e-02,\n",
      "         8.3907e-03,  4.8635e-03, -5.2572e-03,  6.0721e-03,  1.5649e-02,\n",
      "        -8.4767e-04,  8.1277e-03, -6.9241e-03,  2.4293e-02,  1.9428e-02,\n",
      "        -7.2072e-04,  5.3872e-03, -7.6721e-03,  2.7796e-02, -1.0936e-02,\n",
      "         1.0251e-05,  1.1329e-02,  1.2317e-02,  6.3855e-04, -1.1190e-03,\n",
      "        -9.9704e-04,  4.0542e-03, -1.9842e-03,  1.1769e-02,  2.3883e-03,\n",
      "         9.1591e-03,  1.9136e-02, -5.4380e-05, -8.1386e-04, -3.8896e-03,\n",
      "         2.7890e-03, -2.0991e-03,  1.0926e-02,  4.2812e-03,  4.9322e-03,\n",
      "         5.7512e-05, -1.5945e-03, -2.1210e-03,  1.3882e-02,  1.2195e-02,\n",
      "        -1.0504e-03, -1.1317e-03,  7.0554e-03,  1.8687e-03,  8.9614e-03,\n",
      "         4.2880e-04,  2.4065e-02,  4.1156e-03,  5.3827e-03,  2.6708e-03,\n",
      "         4.8298e-04,  6.6941e-03,  1.3832e-05,  1.1702e-02, -7.7028e-03,\n",
      "         1.6393e-02, -1.3685e-03,  2.0122e-02,  1.4129e-02, -4.4543e-04,\n",
      "         2.4045e-02], requires_grad=True)\n",
      "clf.l2.weight Parameter containing:\n",
      "tensor([[ 5.3656e-03,  8.9494e-04, -5.9618e-05,  ..., -1.6101e-04,\n",
      "         -4.6840e-04, -1.6331e-03],\n",
      "        [-5.4522e-03,  2.0471e-04, -9.5389e-05,  ..., -4.2170e-05,\n",
      "         -2.0142e-04, -5.0896e-03],\n",
      "        [-2.0934e-03, -1.9743e-04,  4.6005e-04,  ..., -3.0320e-03,\n",
      "          3.4003e-04, -1.2971e-02],\n",
      "        ...,\n",
      "        [-7.8885e-03,  2.3574e-04,  1.0265e-04,  ..., -1.0546e-03,\n",
      "          1.0104e-04, -4.9791e-03],\n",
      "        [-7.3522e-03,  1.7895e-05,  1.4032e-04,  ...,  3.6491e-04,\n",
      "          1.1263e-04,  3.3954e-04],\n",
      "        [ 3.1058e-02,  8.9437e-04, -1.1500e-03,  ...,  1.9340e-03,\n",
      "         -1.6909e-03, -1.0032e-02]], requires_grad=True)\n",
      "clf.l2.bias Parameter containing:\n",
      "tensor([-1.9728e-03, -8.3903e-03,  7.5776e-03, -1.8973e-03,  5.6366e-03,\n",
      "        -9.3929e-03, -2.9533e-03,  1.5076e-03, -1.1096e-02, -6.9245e-03,\n",
      "        -3.6664e-03,  6.3051e-03,  1.2078e-02,  9.8795e-03, -7.0351e-03,\n",
      "         2.4977e-03, -3.9693e-03, -8.6341e-03, -9.3763e-03, -9.7032e-03,\n",
      "        -1.1606e-02, -2.8888e-04, -3.7281e-03, -5.1542e-03,  2.3087e-04,\n",
      "        -1.6648e-02, -8.4779e-03,  1.6695e-03, -1.3505e-03,  1.2491e-02,\n",
      "        -7.5332e-03,  1.0582e-02, -9.8232e-03, -6.6667e-03,  6.9241e-04,\n",
      "        -4.4841e-03, -1.5136e-03,  2.0842e-02, -4.3641e-03,  2.0549e-05,\n",
      "         1.6372e-02,  2.9840e-03, -2.1206e-04, -4.5282e-03, -3.0852e-03,\n",
      "        -3.5016e-03, -3.7501e-03,  2.1679e-03,  2.6038e-03, -8.8139e-04,\n",
      "         1.2926e-02,  1.5803e-02,  5.0842e-04,  1.2151e-02,  6.4199e-03,\n",
      "         8.7120e-03, -8.9602e-03, -5.9167e-03, -6.5175e-03,  9.1201e-03,\n",
      "         7.4381e-03, -6.4711e-03, -9.7644e-03, -1.4218e-03,  3.0098e-03,\n",
      "         1.0642e-02, -7.8350e-03,  1.9818e-02,  1.6537e-02,  8.5309e-03,\n",
      "        -1.4105e-02, -2.1661e-04, -8.6846e-03, -7.6276e-03,  5.8035e-03,\n",
      "         6.3249e-04,  8.1483e-03, -2.9400e-03, -3.7444e-03, -3.8590e-03,\n",
      "        -3.4479e-03,  1.4814e-02,  1.9987e-02,  4.1036e-04,  2.3055e-02,\n",
      "        -3.9706e-03,  1.4932e-03, -5.6283e-03, -8.8798e-03,  3.9429e-03,\n",
      "        -4.6571e-03,  2.2937e-02, -2.6984e-04,  1.2370e-02,  9.2004e-03,\n",
      "         7.1636e-03, -3.7097e-03, -6.4807e-04, -1.0495e-03, -6.8437e-03,\n",
      "        -1.1465e-03, -3.7977e-03, -5.7053e-03, -9.2626e-03, -1.4377e-02,\n",
      "        -1.8962e-03, -5.3687e-03,  2.9414e-03, -4.7766e-03, -1.2963e-02,\n",
      "        -3.0284e-03,  7.1503e-04,  7.9890e-03, -1.1907e-03, -5.5830e-03,\n",
      "         3.6347e-03, -6.9661e-03,  9.0768e-03, -1.9783e-03,  5.5392e-03,\n",
      "        -5.6933e-03,  1.5468e-04,  1.3603e-02,  7.7787e-03, -4.5086e-03,\n",
      "        -1.0014e-02, -6.1479e-03,  1.0205e-04,  1.2736e-02, -8.1098e-04,\n",
      "        -4.5548e-03, -9.0244e-03, -5.4383e-03,  9.8775e-03, -5.1572e-03,\n",
      "         8.0433e-03, -5.4545e-03, -1.0763e-02, -9.8379e-03,  9.4006e-03,\n",
      "        -6.4388e-03, -6.9757e-04, -7.1277e-03, -3.2819e-03, -4.4599e-03,\n",
      "        -7.4699e-03, -1.0252e-02, -1.3529e-03, -4.9705e-03,  1.1365e-03,\n",
      "        -5.3028e-03, -9.0735e-03, -3.2729e-03, -4.4177e-03,  1.5344e-02,\n",
      "        -2.6908e-03, -3.7764e-03, -3.5702e-03,  2.4736e-03, -3.5948e-03,\n",
      "         2.0753e-02,  9.6749e-03, -1.4293e-03, -1.4361e-03, -6.1826e-03,\n",
      "        -4.7904e-03,  1.3376e-03, -5.4812e-03, -9.6341e-03, -3.6412e-03,\n",
      "        -7.5515e-03,  9.4915e-03, -4.8424e-03,  1.4351e-02, -1.6688e-03,\n",
      "        -6.1794e-03,  1.9234e-02,  1.1837e-03, -9.4230e-03, -2.4119e-03,\n",
      "        -7.0678e-03, -9.6409e-03, -7.9349e-03, -9.9118e-03, -8.2414e-03,\n",
      "        -9.7045e-03,  1.3246e-02,  1.8249e-02, -4.2021e-03,  1.9869e-02,\n",
      "        -5.0858e-04, -5.2772e-03, -6.4319e-03,  7.2490e-03, -4.5312e-03,\n",
      "         1.4192e-02,  5.1071e-03, -8.5891e-03, -3.3385e-03, -3.1485e-03,\n",
      "        -3.1848e-03, -9.7976e-03, -9.8106e-03,  7.9354e-03, -5.4911e-03,\n",
      "        -1.8734e-02, -1.1907e-02, -4.8037e-03, -2.0788e-03, -1.7741e-03,\n",
      "         1.8048e-04, -6.8579e-03, -6.0083e-03,  1.0543e-02, -2.6548e-03,\n",
      "        -5.0269e-03, -1.5396e-03, -5.3006e-03, -1.1064e-02,  1.3799e-03,\n",
      "         4.1931e-04, -7.0164e-05, -7.2252e-03, -6.0243e-03, -9.0205e-03,\n",
      "        -6.6021e-03,  6.2623e-03, -3.2638e-03,  2.2248e-03, -6.2115e-03,\n",
      "        -3.1210e-03, -4.0477e-03, -3.9065e-03,  7.0814e-03,  4.1193e-03,\n",
      "         5.1008e-03, -9.9319e-03, -4.3118e-03,  5.4361e-03,  2.9801e-03,\n",
      "         7.4688e-03, -4.1556e-03,  1.2148e-02, -6.5674e-03, -6.3469e-03,\n",
      "        -6.4712e-03,  5.7402e-03, -2.2282e-03, -6.4029e-03, -7.3270e-03,\n",
      "         2.1719e-02,  3.3436e-03, -3.9325e-03,  2.4362e-02, -1.1026e-02,\n",
      "         1.6774e-03, -2.7981e-03, -8.5506e-03, -4.3229e-03, -6.0931e-04,\n",
      "        -5.9209e-03, -1.0000e-02,  5.1050e-03, -8.1191e-03,  2.3349e-04,\n",
      "        -4.4130e-03,  3.1649e-03, -3.6993e-03, -9.6100e-03, -9.1296e-03,\n",
      "        -1.4410e-02, -4.3523e-03, -1.8522e-03,  1.4412e-02,  1.1394e-02,\n",
      "        -3.9690e-03,  2.1586e-02, -2.4492e-03, -3.7054e-03, -7.0286e-03,\n",
      "        -5.5009e-03, -9.5089e-04, -3.0485e-03,  6.0799e-03, -4.2307e-03,\n",
      "        -9.8614e-03, -4.7571e-04,  4.3334e-03], requires_grad=True)\n",
      "clf.l3.weight Parameter containing:\n",
      "tensor([[-6.6223e-02, -3.1385e-02, -7.3257e-02,  3.1943e-02,  5.6158e-02,\n",
      "          5.9748e-02,  5.4857e-02,  4.0662e-02,  2.9900e-02, -3.5711e-02,\n",
      "          1.9744e-02,  6.4086e-02,  7.3462e-02,  6.1586e-02,  4.1390e-02,\n",
      "         -9.6816e-02,  3.7468e-03,  1.1691e-03, -2.1968e-03, -4.9985e-03,\n",
      "         -2.6968e-02, -5.2279e-02, -8.2755e-03,  7.2070e-03,  2.5735e-02,\n",
      "          6.1607e-02,  5.8244e-02, -6.1248e-02, -8.5430e-02, -6.9167e-02,\n",
      "         -3.4699e-03, -6.2349e-02,  4.6607e-02, -1.7564e-02,  6.5478e-02,\n",
      "          2.8186e-02,  1.7941e-02,  6.0710e-02,  6.2741e-03, -6.3390e-02,\n",
      "          4.9413e-02,  5.2103e-02,  1.9338e-02, -1.3180e-02,  1.6604e-02,\n",
      "         -9.2361e-02,  1.3126e-02, -9.8155e-02, -6.3778e-02, -2.0516e-02,\n",
      "         -8.6020e-02, -6.2983e-02, -7.2877e-02, -6.9081e-02, -9.3536e-02,\n",
      "          5.3040e-02, -1.9396e-02, -9.0018e-03,  1.4305e-02,  5.6399e-02,\n",
      "          4.0866e-02, -1.2616e-02,  4.8311e-03,  1.9392e-02,  5.0756e-02,\n",
      "         -7.9100e-02,  5.1365e-02, -8.2433e-02, -9.3389e-02, -5.8014e-02,\n",
      "          4.2988e-02,  1.8417e-02, -1.2330e-03,  7.5171e-02,  6.2864e-02,\n",
      "         -1.9159e-02,  5.6558e-02, -9.5944e-03,  7.6878e-03,  5.1970e-04,\n",
      "          3.8653e-02,  6.3291e-02,  6.9675e-02, -5.8300e-02,  5.7000e-02,\n",
      "          1.2181e-02, -7.5585e-02,  5.2140e-02,  4.8154e-02, -8.2679e-02,\n",
      "         -2.3488e-04, -7.9902e-02,  3.6774e-02, -6.0652e-02, -8.8326e-02,\n",
      "         -9.2822e-02, -1.9281e-02, -1.8627e-02,  9.9569e-03, -7.6897e-02,\n",
      "         -9.6375e-02,  1.6674e-02, -7.4955e-02,  3.4687e-04, -3.9714e-02,\n",
      "         -8.2541e-02,  1.8914e-02,  3.2373e-02, -1.0183e-03, -2.4651e-02,\n",
      "          2.4745e-02,  5.1702e-02,  5.4329e-02,  4.0638e-02, -3.0181e-02,\n",
      "          7.0013e-02, -3.2904e-03,  4.4560e-02,  1.5451e-02, -7.0867e-02,\n",
      "          1.1095e-02, -6.2906e-02,  4.0204e-02,  5.8401e-02, -8.1838e-03,\n",
      "         -7.5251e-02,  1.6756e-02,  5.4701e-02,  5.4948e-02, -7.0943e-02,\n",
      "         -9.7761e-03,  6.5136e-02,  2.2669e-03, -8.8395e-02, -8.8096e-02,\n",
      "          5.9042e-02, -7.6117e-02, -2.4968e-02, -1.6094e-03,  5.6282e-02,\n",
      "          3.2197e-02,  2.1229e-02, -6.5851e-03,  5.1671e-02,  5.0760e-02,\n",
      "         -1.0815e-02,  2.4978e-02,  5.5511e-02, -1.1582e-02,  5.8429e-02,\n",
      "          5.5237e-03,  5.7704e-02,  2.6086e-02,  1.8562e-03, -9.4225e-02,\n",
      "          8.6984e-03, -8.5211e-02, -1.4960e-02,  6.3545e-02, -7.6271e-03,\n",
      "         -7.5195e-02, -9.4531e-02,  3.7758e-02,  6.2450e-02,  3.6007e-02,\n",
      "          3.0231e-02,  7.6164e-02,  1.6466e-02,  6.5109e-02,  6.3572e-02,\n",
      "          1.8923e-02,  5.9340e-02,  1.4946e-02, -4.9717e-02, -8.0716e-02,\n",
      "         -6.5469e-02, -9.2982e-02,  2.9794e-02,  7.2808e-03, -1.1843e-02,\n",
      "         -1.2933e-03, -4.3043e-02,  4.7187e-02,  6.4586e-02, -5.7772e-02,\n",
      "         -1.5856e-03,  5.8669e-02, -8.7589e-02,  3.0566e-02, -9.2507e-02,\n",
      "          8.1116e-02, -1.0273e-02, -1.2038e-02,  5.0628e-02, -1.0145e-02,\n",
      "         -8.8949e-02,  4.8687e-02, -9.0017e-03,  8.0931e-03,  2.8705e-02,\n",
      "         -5.7079e-02, -3.5818e-03, -2.5177e-02, -9.2532e-02,  1.1494e-02,\n",
      "         -9.2987e-02, -9.7010e-02, -3.2870e-02, -9.5285e-02, -1.7705e-02,\n",
      "          5.5558e-02, -2.3358e-02,  1.7586e-03,  6.3027e-02,  2.1005e-02,\n",
      "          1.3653e-02,  7.3001e-02,  2.0897e-02, -3.2118e-02, -5.5380e-02,\n",
      "          6.4051e-02,  2.8113e-02,  1.7158e-02, -2.5037e-02, -7.5765e-03,\n",
      "          2.8960e-02,  5.5766e-02,  6.6560e-03,  3.6969e-02, -5.1797e-02,\n",
      "          5.8769e-02,  3.0481e-03,  9.8976e-04,  6.2503e-02,  5.7932e-02,\n",
      "          6.5940e-02, -6.2538e-03,  6.5166e-03,  6.4225e-02,  5.5427e-02,\n",
      "          6.3653e-02,  1.6380e-02,  5.9810e-02, -7.7345e-02, -9.0586e-05,\n",
      "          3.6757e-02,  6.1677e-02, -9.2794e-02, -1.5564e-02,  1.3818e-02,\n",
      "         -8.1024e-02,  6.6720e-02, -7.7589e-02, -7.4686e-02, -1.2006e-02,\n",
      "          3.8304e-02,  2.0108e-02,  9.9597e-04,  3.9243e-02,  2.0801e-02,\n",
      "         -4.0560e-03, -4.4848e-03,  4.4009e-02,  1.2759e-02, -8.5595e-02,\n",
      "          6.9948e-02, -1.0086e-01,  7.3766e-04,  8.2319e-03, -1.1953e-02,\n",
      "         -4.0216e-02,  1.0065e-02, -4.5204e-02,  6.3380e-02,  6.9025e-02,\n",
      "          1.6616e-02,  6.3362e-02,  2.0825e-02, -2.5383e-02, -5.7316e-02,\n",
      "         -4.8963e-02,  7.0149e-02, -5.4437e-03, -7.5720e-02,  1.0917e-02,\n",
      "         -4.1632e-02,  3.6462e-02,  4.8759e-02]], requires_grad=True)\n",
      "clf.l3.bias Parameter containing:\n",
      "tensor([0.0501], requires_grad=True)\n",
      "clf.ln1.weight Parameter containing:\n",
      "tensor([0.4869, 0.0787, 0.0811, 0.2762, 0.1016, 0.0780, 0.2025, 0.2564, 0.1150,\n",
      "        0.0767, 0.2583, 0.0852, 0.0786, 0.0740, 0.0773, 0.0781, 0.0941, 0.1412,\n",
      "        0.0752, 0.1278, 0.1221, 0.3340, 0.1416, 0.0929, 0.1804, 0.2907, 0.1104,\n",
      "        0.1200, 0.2268, 0.0776, 0.3002, 0.0775, 0.1162, 0.0921, 0.1229, 0.2808,\n",
      "        0.1982, 0.0763, 0.1417, 0.1265, 0.2335, 0.0872, 0.1170, 0.0786, 0.2833,\n",
      "        0.1447, 0.3063, 0.1783, 0.1525, 0.1804, 0.0758, 0.1631, 0.1035, 0.0951,\n",
      "        0.0756, 0.1591, 0.2182, 0.2406, 0.2536, 0.1653, 0.0761, 0.0836, 0.1856,\n",
      "        0.1237, 0.1906, 0.1064, 0.2500, 0.1716, 0.0855, 0.0792, 0.2273, 0.1093,\n",
      "        0.1704, 0.2460, 0.1571, 0.0869, 0.1820, 0.0882, 0.0861, 0.0862, 0.0803,\n",
      "        0.1335, 0.1814, 0.0924, 0.1387, 0.1495, 0.2700, 0.1625, 0.1077, 0.0884,\n",
      "        0.0759, 0.0805, 0.0830, 0.0826, 0.1163, 0.2100, 0.0961, 0.2566, 0.2636,\n",
      "        0.2039, 0.1945, 0.3273, 0.2297, 0.1586, 0.1828, 0.0784, 0.1738, 0.0835,\n",
      "        0.0798, 0.0836, 0.1260, 0.0902, 0.0791, 0.1205, 0.1586, 0.1411, 0.0790,\n",
      "        0.1754, 0.2000, 0.0753, 0.1258, 0.0793, 0.1369, 0.3364, 0.0825, 0.1002,\n",
      "        0.1134, 0.0739, 0.1323, 0.0811, 0.2293, 0.1884, 0.0824, 0.3900, 0.3178,\n",
      "        0.0838, 0.0842, 0.1267, 0.1870, 0.1203, 0.1059, 0.0840, 0.0772, 0.0978,\n",
      "        0.0954, 0.1771, 0.0748, 0.0981, 0.0744, 0.0838, 0.1699, 0.1324, 0.1145,\n",
      "        0.0842, 0.0842, 0.1124, 0.0924, 0.0877, 0.1021, 0.1264, 0.0813, 0.0834,\n",
      "        0.0961, 0.3162, 0.2216, 0.0743, 0.2658, 0.0957, 0.0986, 0.0827, 0.0744,\n",
      "        0.0741, 0.0780, 0.1126, 0.2483, 0.0843, 0.0744, 0.0802, 0.0789, 0.2299,\n",
      "        0.0760, 0.1006, 0.3419, 0.0919, 0.1550, 0.3201, 0.0810, 0.2664, 0.0771,\n",
      "        0.0949, 0.0737, 0.1675, 0.3020, 0.2525, 0.0803, 0.1261, 0.2022, 0.0829,\n",
      "        0.0951, 0.0750, 0.2939, 0.1917, 0.0878, 0.2631, 0.2855, 0.2148, 0.1223,\n",
      "        0.2438, 0.1581, 0.0933, 0.1666, 0.0923, 0.0979, 0.0878, 0.2016, 0.2313,\n",
      "        0.1218, 0.1004, 0.0882, 0.0754, 0.1299, 0.1653, 0.0944, 0.2988, 0.1097,\n",
      "        0.0996, 0.0850, 0.0898, 0.2074, 0.1956, 0.3038, 0.1268, 0.2115, 0.0860,\n",
      "        0.1041, 0.1325, 0.0827, 0.2889, 0.3160, 0.1407, 0.0833, 0.1135, 0.0763,\n",
      "        0.0925, 0.0901, 0.1473, 0.0769, 0.1061, 0.1442, 0.1691, 0.2261, 0.0855,\n",
      "        0.0759, 0.1186, 0.1574, 0.1890, 0.1084, 0.1294, 0.2309, 0.0739, 0.0849,\n",
      "        0.2115, 0.1138, 0.0874, 0.0744, 0.3045, 0.0769, 0.2706, 0.0907, 0.0928,\n",
      "        0.0740, 0.1513, 0.2990, 0.0751, 0.1006, 0.2090, 0.0804, 0.0779, 0.2397,\n",
      "        0.0806, 0.1198, 0.1234, 0.0801, 0.2789, 0.1941, 0.0903, 0.2556, 0.1237,\n",
      "        0.3425, 0.0868, 0.0916, 0.1322, 0.0787, 0.1334, 0.1149, 0.1143, 0.1221,\n",
      "        0.1310, 0.0808, 0.0826, 0.2039, 0.2499, 0.1580, 0.1219, 0.0764, 0.0897,\n",
      "        0.1012, 0.2932, 0.2578, 0.1772, 0.2795, 0.2342, 0.0806, 0.0747, 0.0736,\n",
      "        0.1983, 0.1180, 0.0770, 0.1477, 0.0908, 0.0753, 0.1389, 0.1367, 0.3120,\n",
      "        0.0938, 0.1552, 0.1258, 0.2192, 0.2047, 0.0931, 0.0860, 0.1583, 0.0861,\n",
      "        0.2449, 0.1065, 0.0877, 0.0806, 0.0882, 0.1308, 0.0809, 0.1941, 0.0774,\n",
      "        0.1119, 0.0819, 0.2252, 0.0947, 0.1382, 0.2001, 0.1099, 0.3089, 0.0845,\n",
      "        0.0894, 0.0921, 0.0808, 0.0830, 0.0992, 0.1064, 0.2027, 0.2302, 0.1080,\n",
      "        0.1149, 0.0746, 0.1750, 0.1592, 0.0756, 0.0871, 0.1472, 0.2957, 0.1162,\n",
      "        0.2186, 0.2099, 0.0782, 0.0819, 0.0819, 0.1584, 0.2622, 0.0817, 0.0909,\n",
      "        0.1027, 0.1079, 0.0737, 0.0825, 0.0747, 0.2404, 0.1337, 0.2240, 0.0811,\n",
      "        0.0833, 0.0954, 0.0887, 0.0897, 0.0751, 0.0787, 0.0853, 0.0873, 0.1818,\n",
      "        0.3416, 0.0842, 0.2004, 0.0861, 0.1084, 0.1616, 0.1810, 0.2952, 0.1263,\n",
      "        0.0990, 0.2159, 0.0743, 0.0808, 0.1300, 0.1164, 0.2802, 0.3357, 0.0804,\n",
      "        0.0744, 0.1022, 0.0981, 0.1178, 0.0861, 0.0762, 0.0760, 0.0827, 0.1275,\n",
      "        0.1535, 0.3110, 0.1561, 0.1200, 0.0848, 0.0808, 0.0747, 0.0852, 0.0744,\n",
      "        0.2767, 0.1002, 0.0972, 0.0999, 0.0743, 0.1049, 0.0805, 0.0749, 0.0750,\n",
      "        0.1002, 0.0803, 0.1310, 0.1112, 0.0895, 0.0827, 0.2729, 0.1012, 0.0775,\n",
      "        0.0873, 0.0810, 0.1682, 0.0799, 0.2081, 0.0748, 0.2569, 0.0861, 0.1383,\n",
      "        0.0922, 0.1086, 0.0785, 0.2181, 0.0782, 0.0759, 0.0768, 0.0739, 0.3191,\n",
      "        0.2189, 0.1342, 0.0761, 0.0840, 0.0742, 0.0762, 0.1790, 0.0888, 0.2838,\n",
      "        0.2779, 0.1566, 0.1207, 0.0944, 0.0790, 0.1535, 0.1007, 0.1223, 0.0915,\n",
      "        0.0868, 0.0916, 0.1814, 0.0874, 0.0785, 0.0812, 0.1092, 0.2464, 0.1786,\n",
      "        0.2009, 0.1008, 0.0936, 0.1059, 0.1754, 0.0952, 0.1039, 0.0762, 0.3570,\n",
      "        0.0862, 0.0767, 0.1082, 0.0792, 0.0756, 0.0743, 0.0880, 0.0916, 0.2977,\n",
      "        0.0884, 0.2629, 0.1460, 0.1352, 0.1019, 0.1651, 0.1964, 0.0746, 0.2205,\n",
      "        0.1239, 0.2929, 0.2623, 0.0956, 0.0764, 0.0971, 0.3795, 0.1251, 0.1948,\n",
      "        0.1411, 0.2764, 0.0775, 0.0801, 0.0838, 0.0868, 0.0758, 0.0995, 0.1255,\n",
      "        0.2246, 0.3978, 0.0742, 0.0740, 0.0968, 0.1130, 0.0766, 0.2565, 0.0833,\n",
      "        0.0942, 0.0818, 0.0755, 0.0749, 0.1439, 0.2189, 0.0741, 0.0865, 0.1464,\n",
      "        0.3030, 0.1602, 0.0844, 0.2917, 0.0978, 0.3026, 0.0742, 0.1046, 0.1952,\n",
      "        0.0853, 0.2265, 0.1346, 0.2280, 0.0799, 0.2300, 0.1341, 0.0765, 0.3499],\n",
      "       requires_grad=True)\n",
      "clf.ln1.bias Parameter containing:\n",
      "tensor([-3.2522e-03, -1.0082e-04,  7.8318e-04, -3.9337e-03, -1.2778e-03,\n",
      "         3.5847e-05,  3.6106e-05,  8.0785e-03,  5.0513e-03, -4.7810e-05,\n",
      "         8.5121e-03,  1.4093e-03,  4.7525e-04, -5.5653e-06,  2.2374e-04,\n",
      "         1.7676e-03, -1.2225e-03,  1.3010e-03,  1.1320e-03,  1.3995e-04,\n",
      "        -7.3479e-05, -4.7623e-03, -6.7160e-04, -1.2350e-03,  2.1033e-03,\n",
      "        -4.9948e-04, -2.7703e-04,  1.3315e-03,  2.0334e-03,  2.4937e-03,\n",
      "         7.4512e-03,  6.1351e-04,  7.0456e-04,  2.4339e-03, -1.7785e-03,\n",
      "         4.5336e-03,  8.0766e-04,  3.3868e-05, -5.2900e-04, -2.3050e-03,\n",
      "         2.4668e-03,  2.2626e-03,  1.6219e-04,  5.4174e-05,  8.3548e-03,\n",
      "         2.9000e-03,  1.7142e-03,  4.6552e-03, -7.7782e-04,  4.0486e-03,\n",
      "         6.7516e-04,  3.3063e-03, -8.8662e-04,  9.0326e-04, -2.5321e-03,\n",
      "         2.1934e-03,  6.3172e-03,  8.1008e-03,  1.3870e-03, -8.7326e-04,\n",
      "        -4.1197e-04,  3.7993e-04, -5.0706e-04, -9.0783e-05,  5.6761e-04,\n",
      "         3.8254e-03,  3.5433e-03,  4.5931e-04,  1.0108e-03,  6.0867e-04,\n",
      "        -7.3638e-04, -6.5710e-05, -8.6377e-04,  6.7811e-03,  2.9464e-03,\n",
      "         4.8012e-05, -2.1942e-03,  1.8858e-03, -9.4300e-04, -2.4358e-04,\n",
      "        -2.4710e-03,  9.7884e-04,  5.7128e-03,  2.0795e-04, -1.0924e-03,\n",
      "        -1.8251e-03, -4.2247e-03, -5.9688e-04, -1.6586e-04, -1.1395e-04,\n",
      "         2.8986e-04,  2.9737e-04,  2.4868e-04, -1.4623e-04,  1.2699e-03,\n",
      "         4.8022e-04,  9.7592e-04,  2.7204e-03,  1.7242e-03,  2.3476e-04,\n",
      "         5.7194e-03, -4.7784e-03,  3.8340e-03,  4.9706e-03,  5.0791e-03,\n",
      "        -1.5142e-03,  2.2826e-03,  8.5970e-04,  5.6846e-04,  1.9932e-03,\n",
      "        -2.9878e-04,  1.5906e-04,  1.1716e-03, -7.3774e-04,  2.4428e-03,\n",
      "         2.2066e-03,  1.6653e-03,  2.9015e-03,  3.1301e-03,  5.3437e-04,\n",
      "        -3.9032e-04,  6.0499e-04, -1.9805e-03, -1.1652e-03,  8.1909e-04,\n",
      "        -4.6464e-04,  2.2984e-03,  1.0884e-04,  8.8098e-04, -3.4253e-04,\n",
      "        -2.0872e-04,  2.0498e-03, -2.0180e-04,  5.1742e-03,  2.8119e-03,\n",
      "         7.6654e-04,  2.1580e-04, -9.0577e-04,  6.2523e-03, -3.0801e-05,\n",
      "         1.1891e-03, -1.4040e-04, -5.2202e-04,  1.5012e-03,  5.6136e-04,\n",
      "         4.7230e-04,  1.0848e-03, -2.2283e-04,  2.7087e-03,  4.4704e-04,\n",
      "        -2.3487e-03, -2.1172e-04, -1.4323e-03, -1.0475e-03, -4.5600e-04,\n",
      "        -1.3207e-04, -6.0455e-04, -1.3850e-03, -9.9176e-04,  1.6485e-03,\n",
      "         1.2041e-03, -6.5822e-04, -8.2465e-04,  4.2298e-04,  1.6058e-03,\n",
      "         5.3384e-04,  1.0393e-02, -5.0711e-04, -7.2732e-04, -4.4114e-04,\n",
      "         3.3829e-04,  1.4902e-03,  2.0255e-03, -4.0416e-04, -9.4096e-04,\n",
      "         8.2294e-04,  6.5742e-04,  4.4323e-04,  1.3490e-03,  7.6252e-03,\n",
      "         1.7660e-03,  1.5771e-03,  1.0968e-03,  1.2876e-04, -1.2768e-03,\n",
      "         1.0384e-02, -3.8522e-04,  5.2590e-03,  1.8201e-03,  2.2784e-03,\n",
      "         3.2185e-04,  1.0291e-04,  4.9700e-03,  2.2682e-03,  7.0579e-04,\n",
      "         3.6974e-04, -2.5515e-03,  3.1335e-04,  1.2212e-03,  9.3919e-04,\n",
      "         7.5105e-03, -1.6673e-03,  5.4891e-04,  5.1631e-03, -6.3002e-04,\n",
      "         5.8069e-03,  6.7165e-04,  6.8786e-03,  5.5794e-03,  1.1353e-03,\n",
      "         2.0322e-03,  4.2975e-04, -8.2370e-04, -1.3776e-03, -8.7838e-04,\n",
      "         2.4358e-03,  1.0754e-03,  3.0798e-04, -1.5150e-04,  2.0884e-04,\n",
      "         1.5421e-04, -2.7888e-03, -2.5509e-04,  6.9834e-03, -2.3090e-03,\n",
      "         1.6369e-03, -1.0086e-04,  5.1131e-04, -3.3618e-03, -4.6180e-03,\n",
      "         6.2187e-03, -5.6819e-05,  1.1493e-02, -1.9536e-04,  1.1614e-03,\n",
      "         8.8788e-04,  1.9822e-03,  4.2872e-03,  9.0145e-03, -7.0149e-04,\n",
      "         1.2888e-03,  1.4856e-04, -1.4904e-03,  3.6095e-04,  7.0409e-04,\n",
      "         5.3179e-03,  4.6076e-03,  1.5436e-03,  7.0509e-04,  2.0287e-03,\n",
      "         2.5319e-03, -1.4326e-03,  4.5389e-04, -1.1619e-05,  2.6244e-04,\n",
      "         1.7750e-03,  3.6708e-03,  3.6825e-03,  6.5344e-03,  9.8666e-04,\n",
      "         8.8643e-04, -1.4514e-03, -7.7532e-04,  8.9155e-04,  8.3141e-04,\n",
      "         6.6046e-03,  4.3863e-05,  6.1566e-03, -1.2216e-03, -1.4802e-03,\n",
      "        -2.7447e-04,  5.2122e-04,  2.9649e-03, -6.6032e-04,  1.3130e-03,\n",
      "        -3.3259e-03,  5.0646e-04,  4.2458e-04,  4.0650e-03,  1.7634e-03,\n",
      "        -2.9572e-04, -3.6078e-04,  3.5927e-05,  8.1367e-04,  4.4771e-03,\n",
      "         8.4752e-04,  4.6835e-03, -1.6383e-03,  4.5655e-03,  3.1069e-04,\n",
      "        -6.0585e-04,  4.6511e-04,  1.4677e-03, -1.2096e-03,  3.0001e-03,\n",
      "        -1.8279e-03,  2.2561e-03, -3.6729e-04,  2.6965e-03,  2.9358e-04,\n",
      "         1.6129e-03, -1.8801e-03, -2.7930e-04,  1.3129e-03,  5.3894e-04,\n",
      "         6.2873e-04,  1.6104e-04,  1.0643e-03,  1.5473e-03,  1.0712e-03,\n",
      "         8.0073e-03,  1.7805e-03,  3.2812e-04,  2.1768e-03,  2.7261e-04,\n",
      "         1.3807e-03,  2.0288e-04, -7.0704e-04, -1.5906e-03,  5.4669e-04,\n",
      "         6.7298e-04, -1.0296e-03, -2.7367e-04,  1.0387e-02,  1.5846e-04,\n",
      "         1.1618e-03, -1.7184e-03,  4.1378e-03, -2.9618e-03,  1.7294e-03,\n",
      "         2.2021e-04,  8.2588e-04,  1.4537e-03,  5.6116e-03,  1.3306e-03,\n",
      "         4.9185e-04,  3.5504e-04,  3.4690e-04, -3.2938e-03, -4.3317e-04,\n",
      "         7.0004e-05,  7.7303e-04,  3.9534e-04,  1.7133e-03,  2.2936e-03,\n",
      "         3.1597e-04,  4.7983e-04,  5.6216e-03,  3.4258e-05,  4.6399e-04,\n",
      "         3.9946e-04,  2.4886e-04,  1.2912e-04, -3.3528e-04, -6.3650e-04,\n",
      "        -3.3335e-03, -9.0284e-04,  9.1056e-04,  5.8059e-05,  3.9584e-04,\n",
      "         1.5458e-02, -7.1543e-05, -1.5313e-03,  2.5106e-05, -1.2201e-04,\n",
      "         5.0014e-04, -1.0869e-03,  7.5688e-03,  2.4717e-03, -8.2999e-04,\n",
      "        -1.5919e-03,  7.8545e-04,  5.9140e-04,  1.7578e-03,  5.8982e-03,\n",
      "         3.2560e-03,  1.6716e-04, -4.4665e-04, -1.7419e-03, -2.7845e-04,\n",
      "         1.9552e-04, -3.6498e-04, -3.1159e-05,  8.7132e-03,  2.6431e-03,\n",
      "        -7.6688e-04,  3.8638e-04,  8.2582e-04,  5.0669e-03,  4.2021e-04,\n",
      "        -1.9970e-05,  8.1797e-04,  1.0624e-03, -1.5035e-03,  7.7874e-04,\n",
      "        -1.3896e-03,  4.2492e-03, -3.3503e-04, -1.7293e-03,  2.6558e-05,\n",
      "         3.6390e-03, -2.6993e-03,  6.7741e-03,  6.2136e-03, -2.9905e-03,\n",
      "         1.0954e-04,  7.9107e-03, -4.7267e-04,  7.3633e-04, -1.1927e-03,\n",
      "         6.9594e-04,  3.1636e-03,  9.2388e-03, -2.8058e-04, -6.2428e-05,\n",
      "         1.6341e-03,  9.9422e-04,  1.3185e-03,  4.7694e-04,  1.0008e-03,\n",
      "         1.4640e-03, -7.8066e-04, -2.6683e-04, -7.5333e-04,  3.5252e-03,\n",
      "        -8.5980e-04, -1.6509e-03, -1.3516e-03, -5.8886e-04,  6.3125e-04,\n",
      "        -2.7862e-05,  8.9309e-04,  6.8483e-03,  5.8888e-04,  1.0199e-03,\n",
      "         3.0784e-03,  1.4521e-03, -1.4776e-04,  1.2183e-03, -1.3251e-03,\n",
      "         5.7273e-04,  2.4372e-03,  1.4585e-03,  8.5245e-04, -1.1373e-03,\n",
      "         1.7741e-03, -1.1848e-03,  4.1709e-03,  2.6996e-03, -3.6158e-04,\n",
      "         7.2821e-04,  2.7873e-04, -2.6446e-03,  1.0958e-03,  1.7561e-03,\n",
      "         2.9693e-04,  2.6553e-03, -1.4118e-04,  1.5019e-03, -1.5333e-03,\n",
      "        -1.6201e-04,  3.0602e-04, -4.0631e-04,  2.7535e-03,  4.9977e-04,\n",
      "         6.2617e-04, -2.8208e-04,  8.7290e-03, -4.8070e-03, -6.3756e-04,\n",
      "         2.7337e-03, -1.0493e-03,  8.6980e-04,  6.4782e-04, -1.1860e-03,\n",
      "         1.5383e-03,  4.0679e-03, -1.9489e-04, -1.8962e-04,  1.3333e-04,\n",
      "        -1.8241e-03,  6.6735e-04, -4.1080e-04,  6.6691e-04, -1.0164e-03,\n",
      "         2.1598e-03,  1.6131e-03, -1.6820e-03, -3.2904e-04,  1.8534e-03,\n",
      "        -7.2467e-05, -1.0365e-04, -1.5493e-03,  9.9416e-03, -2.3473e-03,\n",
      "        -2.0123e-04,  2.0657e-03,  5.9477e-04, -4.0645e-04, -5.5651e-05,\n",
      "         2.4506e-04,  4.8140e-04,  3.5349e-04,  9.5908e-03,  2.0524e-03,\n",
      "        -2.1914e-04, -3.4230e-04,  6.0075e-04,  7.8891e-04,  4.2297e-04,\n",
      "        -2.8026e-04,  7.2862e-04, -4.4926e-03,  9.9990e-04,  4.5388e-03,\n",
      "        -2.0849e-03,  1.5939e-03,  2.9828e-04, -3.7580e-03,  1.7753e-03,\n",
      "         9.1015e-04,  2.2701e-03, -1.3644e-03,  1.0995e-02,  2.9222e-04,\n",
      "         8.8373e-05,  2.8922e-03,  1.2004e-04,  8.1824e-04, -1.8367e-04,\n",
      "         6.8134e-04, -6.9973e-04,  2.2266e-03,  3.7217e-04,  6.7760e-04,\n",
      "         3.2401e-04, -1.2345e-03,  2.9596e-04,  3.8939e-03, -3.5236e-04,\n",
      "         3.2917e-03,  4.3540e-03,  5.2504e-04,  8.2660e-04,  2.0758e-03,\n",
      "        -8.8184e-04,  1.2507e-03, -2.4229e-04,  9.2719e-04,  2.8945e-03,\n",
      "        -2.1858e-04,  5.5935e-05,  5.0472e-05,  2.1182e-03, -9.1911e-04,\n",
      "        -3.8271e-07,  6.6031e-04, -4.7445e-04,  3.6846e-03, -2.2094e-03,\n",
      "         2.0245e-04,  5.7720e-04,  7.8730e-04,  3.8288e-03,  1.9463e-03,\n",
      "        -6.3653e-04, -3.7772e-03, -9.4386e-04, -1.3621e-03,  1.6416e-03,\n",
      "         3.8766e-03,  1.3099e-03, -6.9952e-04, -3.9241e-03,  2.3380e-03,\n",
      "         5.7364e-03], requires_grad=True)\n",
      "clf.ln2.weight Parameter containing:\n",
      "tensor([0.2830, 0.0989, 0.4299, 0.1629, 0.3346, 0.4420, 0.2983, 0.2748, 0.1261,\n",
      "        0.1929, 0.1236, 0.3978, 0.5091, 0.3803, 0.1300, 0.3780, 0.0770, 0.0735,\n",
      "        0.0736, 0.0741, 0.0975, 0.2979, 0.1121, 0.0787, 0.1487, 0.3955, 0.3861,\n",
      "        0.3194, 0.3476, 0.3745, 0.0778, 0.3409, 0.2181, 0.1084, 0.3617, 0.1909,\n",
      "        0.1575, 0.5338, 0.0805, 0.4002, 0.3884, 0.4471, 0.1662, 0.1001, 0.1289,\n",
      "        0.3599, 0.0974, 0.3765, 0.2865, 0.1767, 0.4423, 0.3953, 0.2797, 0.3427,\n",
      "        0.4747, 0.3990, 0.0921, 0.1020, 0.1044, 0.3252, 0.3012, 0.1106, 0.0741,\n",
      "        0.1249, 0.3129, 0.4220, 0.3561, 0.5136, 0.3651, 0.3120, 0.2180, 0.1442,\n",
      "        0.0737, 0.4371, 0.4782, 0.1890, 0.3936, 0.1239, 0.0866, 0.0799, 0.2597,\n",
      "        0.3856, 0.5364, 0.2584, 0.4569, 0.0845, 0.3597, 0.2774, 0.3752, 0.3443,\n",
      "        0.0763, 0.5170, 0.1573, 0.3217, 0.4319, 0.3825, 0.1532, 0.1642, 0.0953,\n",
      "        0.4732, 0.4544, 0.0953, 0.3562, 0.0736, 0.1365, 0.3197, 0.0893, 0.1990,\n",
      "        0.0879, 0.0886, 0.1203, 0.2942, 0.5259, 0.2221, 0.1828, 0.4947, 0.0863,\n",
      "        0.2613, 0.1123, 0.3380, 0.0871, 0.3617, 0.3782, 0.4282, 0.0850, 0.3439,\n",
      "        0.0985, 0.3222, 0.4557, 0.3643, 0.1093, 0.5391, 0.0765, 0.4165, 0.4130,\n",
      "        0.4629, 0.4026, 0.1167, 0.0737, 0.3365, 0.1315, 0.1481, 0.0974, 0.2200,\n",
      "        0.3525, 0.1083, 0.1129, 0.3908, 0.1254, 0.4359, 0.0755, 0.4508, 0.1264,\n",
      "        0.0750, 0.4870, 0.0839, 0.4180, 0.1436, 0.3451, 0.0992, 0.4686, 0.4274,\n",
      "        0.2187, 0.3917, 0.1478, 0.1838, 0.4595, 0.0900, 0.4766, 0.4695, 0.0798,\n",
      "        0.5027, 0.0948, 0.3194, 0.4292, 0.4092, 0.3614, 0.2104, 0.0843, 0.1215,\n",
      "        0.0752, 0.1475, 0.2733, 0.4201, 0.3075, 0.0736, 0.3622, 0.4473, 0.1363,\n",
      "        0.4015, 0.4533, 0.1184, 0.0851, 0.2549, 0.1369, 0.4766, 0.3244, 0.0784,\n",
      "        0.0799, 0.1271, 0.2851, 0.0738, 0.1071, 0.4579, 0.0873, 0.4381, 0.3844,\n",
      "        0.1813, 0.4705, 0.1625, 0.2463, 0.1010, 0.0761, 0.3643, 0.1085, 0.0856,\n",
      "        0.3599, 0.1226, 0.1423, 0.2584, 0.3517, 0.1653, 0.1000, 0.1373, 0.0752,\n",
      "        0.1236, 0.4421, 0.0836, 0.1970, 0.1962, 0.3273, 0.0749, 0.0818, 0.3534,\n",
      "        0.3438, 0.2711, 0.0747, 0.0870, 0.2739, 0.2087, 0.5016, 0.1218, 0.4148,\n",
      "        0.5351, 0.0752, 0.1552, 0.3500, 0.4293, 0.1161, 0.0823, 0.4637, 0.4036,\n",
      "        0.4395, 0.5080, 0.0910, 0.2582, 0.1089, 0.0754, 0.2196, 0.1461, 0.0816,\n",
      "        0.0805, 0.2129, 0.0804, 0.3311, 0.4165, 0.3768, 0.0786, 0.0757, 0.0770,\n",
      "        0.1334, 0.0912, 0.2191, 0.4956, 0.3328, 0.1086, 0.4217, 0.1230, 0.1537,\n",
      "        0.2070, 0.1817, 0.4504, 0.0927, 0.4849, 0.0932, 0.1768, 0.1865, 0.3812],\n",
      "       requires_grad=True)\n",
      "clf.ln2.bias Parameter containing:\n",
      "tensor([ 1.3450e-02,  1.8572e-04,  1.7272e-02,  7.3000e-03,  1.3824e-02,\n",
      "         9.7199e-03,  8.8974e-03,  9.5564e-03,  1.1633e-03,  6.2959e-03,\n",
      "         5.1045e-03,  1.7870e-02,  1.0969e-02,  1.0888e-02,  7.1590e-03,\n",
      "         2.6949e-02, -4.9921e-04,  3.0349e-04,  1.0389e-03,  1.5743e-04,\n",
      "         3.2958e-03,  1.1938e-02, -6.2499e-04,  4.8471e-04,  1.0152e-02,\n",
      "         6.4356e-03,  9.8426e-03,  1.2685e-02,  2.2036e-02,  1.8164e-02,\n",
      "        -9.9567e-04,  2.0594e-02,  4.1270e-03, -4.9453e-04,  8.1783e-03,\n",
      "         5.4241e-03,  2.1624e-03,  1.2154e-02,  1.5577e-03,  1.6701e-02,\n",
      "         1.4347e-02,  9.8570e-03,  1.0060e-03,  7.8936e-04,  1.0496e-03,\n",
      "         1.9654e-02, -1.4949e-04,  3.0540e-02,  1.3048e-02,  3.7423e-03,\n",
      "         2.2295e-02,  1.8639e-02,  2.1018e-02,  2.6345e-02,  2.4192e-02,\n",
      "         9.1701e-03, -1.3648e-03,  1.4305e-04,  3.6226e-03,  1.6118e-02,\n",
      "         8.4128e-03, -1.2140e-03, -4.9110e-05,  4.4773e-03,  1.2073e-02,\n",
      "         2.1362e-02,  1.1351e-02,  1.4382e-02,  3.3945e-02,  1.2888e-02,\n",
      "         2.1198e-03,  6.6076e-03,  4.3374e-04,  1.3372e-02,  1.1989e-02,\n",
      "         2.4759e-03,  1.6039e-02,  1.2327e-03,  1.4060e-03,  1.7915e-04,\n",
      "         8.5507e-03,  2.3329e-02,  1.3603e-02,  1.7476e-02,  1.1946e-02,\n",
      "        -2.4583e-04,  1.5497e-02,  1.2852e-02,  1.1452e-03,  2.0617e-02,\n",
      "         1.3188e-04,  1.1700e-02,  1.0850e-02,  2.3913e-02,  2.6800e-02,\n",
      "         3.2088e-02,  2.2254e-03,  1.0312e-03,  1.7949e-05,  1.3836e-02,\n",
      "         2.1651e-02,  1.8439e-03,  2.3769e-02,  6.3379e-04,  4.3481e-04,\n",
      "         1.8459e-02,  1.5581e-03,  1.3750e-02,  1.4622e-04, -4.6212e-03,\n",
      "         4.4963e-03,  1.0778e-02,  7.3740e-03,  1.0295e-02,  4.2289e-03,\n",
      "         1.2465e-02, -1.8161e-04,  1.3258e-02,  2.5279e-03,  1.3690e-02,\n",
      "         1.0099e-03,  1.4794e-02,  7.4562e-03,  1.1714e-02, -2.8627e-04,\n",
      "         1.5866e-02,  1.6446e-03,  6.2774e-03,  1.4988e-02,  1.8100e-02,\n",
      "        -5.1447e-04,  9.7531e-03,  1.2531e-04,  2.9698e-02,  2.1337e-02,\n",
      "         1.3537e-02,  1.7836e-02, -3.5840e-04,  1.1149e-03,  1.4999e-02,\n",
      "         5.5882e-03,  4.8471e-03, -4.4708e-04,  1.3600e-02,  1.4361e-02,\n",
      "        -1.0504e-03, -2.1753e-04,  4.9689e-03, -7.7426e-04,  1.5465e-02,\n",
      "         4.0564e-04,  8.8197e-03,  2.5452e-03, -9.1373e-04,  1.6469e-02,\n",
      "         5.0109e-05,  2.5445e-02,  2.6793e-03,  1.1530e-02, -3.7387e-05,\n",
      "         1.4117e-02,  1.9375e-02,  1.0745e-02,  1.4521e-02,  4.3021e-03,\n",
      "         4.5508e-03,  1.5673e-02,  1.2280e-03,  4.1088e-03,  8.3065e-03,\n",
      "         5.2005e-03,  1.2465e-02, -1.1918e-03,  1.7223e-02,  1.4315e-02,\n",
      "         1.1309e-02,  3.5352e-02,  9.4237e-03,  1.0305e-03, -8.9864e-04,\n",
      "         5.8880e-04,  6.7401e-03,  6.5353e-03,  1.0210e-02,  1.6081e-02,\n",
      "         7.8558e-04,  1.8690e-02,  2.3942e-02,  4.6455e-03,  2.3462e-02,\n",
      "         1.8754e-02, -1.3932e-03,  7.6933e-04,  1.4977e-02, -1.3726e-04,\n",
      "         1.9767e-02,  1.4421e-02,  5.0351e-04, -2.9845e-04,  5.7121e-03,\n",
      "         1.1647e-02,  7.0951e-04,  1.1239e-03,  1.7841e-02,  9.9067e-04,\n",
      "         1.3846e-02,  2.1507e-02,  8.5652e-03,  2.1654e-02, -9.9387e-04,\n",
      "         1.3729e-02, -3.3411e-03,  1.9724e-04,  1.3275e-02,  2.8963e-03,\n",
      "         7.7705e-04,  1.5216e-02,  1.8274e-03,  1.9625e-03,  1.4966e-02,\n",
      "         1.6084e-02,  5.8825e-03, -6.9072e-04,  3.4318e-03,  3.5145e-04,\n",
      "         3.9496e-03,  8.9753e-03, -9.0014e-04, -1.5576e-03,  1.0795e-02,\n",
      "         1.3416e-02,  9.6533e-05,  6.2047e-04,  8.3613e-03,  1.1783e-02,\n",
      "         1.4708e-02,  1.2788e-04, -2.5443e-04,  1.5303e-02,  1.7980e-02,\n",
      "         7.0222e-03, -4.7803e-04,  1.7778e-02,  1.3350e-02,  2.9907e-04,\n",
      "         6.4739e-03,  1.8950e-02,  2.3136e-02, -2.3249e-03,  2.0872e-03,\n",
      "         2.1755e-02,  1.0822e-02,  1.6599e-02,  1.6767e-02,  1.2718e-03,\n",
      "         5.5113e-03,  4.7294e-03,  2.7480e-04,  1.1118e-02,  4.8314e-03,\n",
      "        -3.7028e-04,  9.5291e-04,  1.4267e-02,  5.2923e-04,  2.6301e-02,\n",
      "         1.5967e-02,  2.1763e-02, -1.6234e-04, -3.7408e-04,  1.8963e-04,\n",
      "         5.3944e-04, -1.6675e-04,  1.5457e-02,  1.4670e-02,  1.6183e-02,\n",
      "         4.0082e-03,  1.5907e-02,  3.7624e-03,  2.3354e-03,  1.0605e-02,\n",
      "         9.0492e-03,  1.1634e-02, -1.1652e-03,  2.0834e-02, -5.7332e-04,\n",
      "         2.5661e-03,  1.0391e-02,  1.4378e-02], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i, j in model.named_parameters():\n",
    "    if j.requires_grad:\n",
    "        print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train loader\n",
      "called new epoch\n"
     ]
    }
   ],
   "source": [
    "dl = ds.train_dataloader() \n",
    "for b, data in enumerate(dl):\n",
    "    # print(x.shape, y.shape)\n",
    "    break\n",
    "data = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = [] \n",
    "with torch.no_grad():\n",
    "    for i in data:\n",
    "        embeds.append(model.getEmbedding(i)[0, 0])\n",
    "embeds = torch.stack(embeds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsmodel = models.Linearcls(1152, dropout=0.2, p0=0.2, take_embed=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4, device='cuda:2') tensor(-0.0921, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(295.0769, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "295.0769348144531\n",
      "tensor(4, device='cuda:2') tensor(-0.0926, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(295.0636, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "295.0635681152344\n",
      "tensor(4, device='cuda:2') tensor(-0.0930, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(295.0497, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "295.04974365234375\n",
      "tensor(4, device='cuda:2') tensor(-0.0931, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(295.0352, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "295.0352478027344\n",
      "tensor(4, device='cuda:2') tensor(-0.0929, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(295.0199, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "295.0198974609375\n",
      "tensor(4, device='cuda:2') tensor(-0.0926, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(295.0037, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "295.00372314453125\n",
      "tensor(4, device='cuda:2') tensor(-0.0921, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.9866, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.98663330078125\n",
      "tensor(4, device='cuda:2') tensor(-0.0914, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.9688, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.9687805175781\n",
      "tensor(4, device='cuda:2') tensor(-0.0908, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.9500, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.9500427246094\n",
      "tensor(4, device='cuda:2') tensor(-0.0902, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.9305, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.9305114746094\n",
      "tensor(4, device='cuda:2') tensor(-0.0897, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.9102, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.91021728515625\n",
      "tensor(4, device='cuda:2') tensor(-0.0893, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.8892, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.8891906738281\n",
      "tensor(4, device='cuda:2') tensor(-0.0890, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.8675, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.8675231933594\n",
      "tensor(4, device='cuda:2') tensor(-0.0888, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.8452, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.8452453613281\n",
      "tensor(4, device='cuda:2') tensor(-0.0885, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.8225, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.822509765625\n",
      "tensor(4, device='cuda:2') tensor(-0.0882, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.7993, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.7993469238281\n",
      "tensor(4, device='cuda:2') tensor(-0.0878, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.7760, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.7760009765625\n",
      "tensor(4, device='cuda:2') tensor(-0.0873, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.7525, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.75250244140625\n",
      "tensor(4, device='cuda:2') tensor(-0.0868, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.7290, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.7290344238281\n",
      "tensor(4, device='cuda:2') tensor(-0.0863, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.7058, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.7057800292969\n",
      "tensor(4, device='cuda:2') tensor(-0.0859, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.6829, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.682861328125\n",
      "tensor(4, device='cuda:2') tensor(-0.0856, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.6604, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.6604309082031\n",
      "tensor(4, device='cuda:2') tensor(-0.0853, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.6387, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.638671875\n",
      "tensor(4, device='cuda:2') tensor(-0.0849, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.6177, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.61767578125\n",
      "tensor(4, device='cuda:2') tensor(-0.0845, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.5977, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.5976867675781\n",
      "tensor(4, device='cuda:2') tensor(-0.0840, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.5787, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.5787353515625\n",
      "tensor(4, device='cuda:2') tensor(-0.0835, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.5610, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.56097412109375\n",
      "tensor(4, device='cuda:2') tensor(-0.0831, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.5444, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.54443359375\n",
      "tensor(4, device='cuda:2') tensor(-0.0828, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.5292, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.5292053222656\n",
      "tensor(4, device='cuda:2') tensor(-0.0824, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.5153, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.5152587890625\n",
      "tensor(4, device='cuda:2') tensor(-0.0819, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.5026, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.5025634765625\n",
      "tensor(4, device='cuda:2') tensor(-0.0814, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4911, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.4910583496094\n",
      "tensor(4, device='cuda:2') tensor(-0.0810, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4807, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.48065185546875\n",
      "tensor(4, device='cuda:2') tensor(-0.0806, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4712, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.47119140625\n",
      "tensor(4, device='cuda:2') tensor(-0.0803, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4626, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.4625549316406\n",
      "tensor(4, device='cuda:2') tensor(-0.0799, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4545, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.45452880859375\n",
      "tensor(4, device='cuda:2') tensor(-0.0794, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4469, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.4468994140625\n",
      "tensor(4, device='cuda:2') tensor(-0.0790, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4395, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.43951416015625\n",
      "tensor(4, device='cuda:2') tensor(-0.0786, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4322, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.43218994140625\n",
      "tensor(4, device='cuda:2') tensor(-0.0783, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4248, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.4248046875\n",
      "tensor(4, device='cuda:2') tensor(-0.0779, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4172, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.41717529296875\n",
      "tensor(4, device='cuda:2') tensor(-0.0774, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4092, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.40924072265625\n",
      "tensor(4, device='cuda:2') tensor(-0.0770, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.4009, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.4009094238281\n",
      "tensor(4, device='cuda:2') tensor(-0.0767, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3922, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.39215087890625\n",
      "tensor(4, device='cuda:2') tensor(-0.0763, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3830, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.38299560546875\n",
      "tensor(4, device='cuda:2') tensor(-0.0758, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3734, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.37335205078125\n",
      "tensor(4, device='cuda:2') tensor(-0.0754, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3633, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.36334228515625\n",
      "tensor(4, device='cuda:2') tensor(-0.0750, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3530, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.35296630859375\n",
      "tensor(4, device='cuda:2') tensor(-0.0746, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3422, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.34222412109375\n",
      "tensor(4, device='cuda:2') tensor(-0.0741, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3312, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.33123779296875\n",
      "tensor(4, device='cuda:2') tensor(-0.0736, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3200, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.3199768066406\n",
      "tensor(4, device='cuda:2') tensor(-0.0731, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.3085, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.30853271484375\n",
      "tensor(4, device='cuda:2') tensor(-0.0727, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2968, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.2968444824219\n",
      "tensor(4, device='cuda:2') tensor(-0.0721, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2850, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.2850341796875\n",
      "tensor(4, device='cuda:2') tensor(-0.0716, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2730, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.27301025390625\n",
      "tensor(4, device='cuda:2') tensor(-0.0710, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2609, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.2608642578125\n",
      "tensor(4, device='cuda:2') tensor(-0.0705, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2485, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.24853515625\n",
      "tensor(4, device='cuda:2') tensor(-0.0698, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2360, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.23602294921875\n",
      "tensor(4, device='cuda:2') tensor(-0.0692, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2233, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.2232666015625\n",
      "tensor(4, device='cuda:2') tensor(-0.0686, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.2103, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.2103271484375\n",
      "tensor(4, device='cuda:2') tensor(-0.0679, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.1971, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.1971435546875\n",
      "tensor(4, device='cuda:2') tensor(-0.0671, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.1837, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.1837158203125\n",
      "tensor(4, device='cuda:2') tensor(-0.0663, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.1700, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.1700439453125\n",
      "tensor(4, device='cuda:2') tensor(-0.0656, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.1561, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.15606689453125\n",
      "tensor(4, device='cuda:2') tensor(-0.0648, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.1418, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.14178466796875\n",
      "tensor(4, device='cuda:2') tensor(-0.0639, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.1272, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.127197265625\n",
      "tensor(4, device='cuda:2') tensor(-0.0630, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.1124, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.11236572265625\n",
      "tensor(4, device='cuda:2') tensor(-0.0620, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.0972, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.0971984863281\n",
      "tensor(4, device='cuda:2') tensor(-0.0610, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.0818, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.081787109375\n",
      "tensor(4, device='cuda:2') tensor(-0.0600, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.0660, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.0660400390625\n",
      "tensor(4, device='cuda:2') tensor(-0.0590, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.0500, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.050048828125\n",
      "tensor(4, device='cuda:2') tensor(-0.0579, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.0338, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.0337829589844\n",
      "tensor(4, device='cuda:2') tensor(-0.0568, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.0173, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.0173034667969\n",
      "tensor(4, device='cuda:2') tensor(-0.0556, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(294.0007, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294.00067138671875\n",
      "tensor(4, device='cuda:2') tensor(-0.0544, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.9838, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.9837646484375\n",
      "tensor(4, device='cuda:2') tensor(-0.0531, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.9667, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.96673583984375\n",
      "tensor(4, device='cuda:2') tensor(-0.0518, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.9496, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.9495849609375\n",
      "tensor(4, device='cuda:2') tensor(-0.0505, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.9323, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.93231201171875\n",
      "tensor(4, device='cuda:2') tensor(-0.0491, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.9150, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.91497802734375\n",
      "tensor(4, device='cuda:2') tensor(-0.0477, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.8976, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.8976135253906\n",
      "tensor(4, device='cuda:2') tensor(-0.0462, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.8802, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.8802490234375\n",
      "tensor(4, device='cuda:2') tensor(-0.0447, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.8629, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.8629150390625\n",
      "tensor(4, device='cuda:2') tensor(-0.0432, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.8456, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.84564208984375\n",
      "tensor(4, device='cuda:2') tensor(-0.0415, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.8285, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.8284606933594\n",
      "tensor(4, device='cuda:2') tensor(-0.0400, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.8115, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.81146240234375\n",
      "tensor(4, device='cuda:2') tensor(-0.0381, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.7946, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.7945556640625\n",
      "tensor(4, device='cuda:2') tensor(-0.0368, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.7780, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.7779541015625\n",
      "tensor(4, device='cuda:2') tensor(-0.0340, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.7616, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.7615966796875\n",
      "tensor(4, device='cuda:2') tensor(-0.0348, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.7457, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.7456970214844\n",
      "tensor(4, device='cuda:2') tensor(-0.0268, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.7308, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.73077392578125\n",
      "tensor(4, device='cuda:2') tensor(-0.0376, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.7186, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.7185974121094\n",
      "tensor(4, device='cuda:2') tensor(-0.0197, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.7043, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.70428466796875\n",
      "tensor(4, device='cuda:2') tensor(-0.0259, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6884, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.6884460449219\n",
      "tensor(4, device='cuda:2') tensor(-0.0296, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6779, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.67791748046875\n",
      "tensor(4, device='cuda:2') tensor(-0.0155, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6655, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.66552734375\n",
      "tensor(4, device='cuda:2') tensor(-0.0181, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6524, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.6523742675781\n",
      "tensor(4, device='cuda:2') tensor(-0.0231, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6431, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.64312744140625\n",
      "tensor(4, device='cuda:2') tensor(-0.0112, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6312, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.6312255859375\n",
      "tensor(4, device='cuda:2') tensor(-0.0101, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6206, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.6205749511719\n",
      "tensor(4, device='cuda:2') tensor(-0.0160, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6118, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.61181640625\n",
      "tensor(4, device='cuda:2') tensor(-0.0064, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.6008, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.6007995605469\n",
      "tensor(4, device='cuda:2') tensor(-0.0028, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5918, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.5918273925781\n",
      "tensor(4, device='cuda:2') tensor(-0.0087, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5833, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.58331298828125\n",
      "tensor(4, device='cuda:2') tensor(-0.0008, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5733, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.57330322265625\n",
      "tensor(4, device='cuda:2') tensor(0.0038, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5652, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.565185546875\n",
      "tensor(4, device='cuda:2') tensor(-0.0015, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5570, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.5569763183594\n",
      "tensor(4, device='cuda:2') tensor(0.0055, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5477, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.5477294921875\n",
      "tensor(4, device='cuda:2') tensor(0.0099, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5399, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.53985595703125\n",
      "tensor(4, device='cuda:2') tensor(0.0052, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5320, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.5319519042969\n",
      "tensor(4, device='cuda:2') tensor(0.0122, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5231, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.52313232421875\n",
      "tensor(4, device='cuda:2') tensor(0.0154, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5152, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.51519775390625\n",
      "tensor(4, device='cuda:2') tensor(0.0117, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.5075, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.5075378417969\n",
      "tensor(4, device='cuda:2') tensor(0.0191, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4990, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.4990234375\n",
      "tensor(4, device='cuda:2') tensor(0.0201, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4908, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.4908142089844\n",
      "tensor(4, device='cuda:2') tensor(0.0181, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4832, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.483154296875\n",
      "tensor(4, device='cuda:2') tensor(0.0255, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4750, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.4750061035156\n",
      "tensor(4, device='cuda:2') tensor(0.0240, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4666, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.4665832519531\n",
      "tensor(4, device='cuda:2') tensor(0.0248, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4586, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.45855712890625\n",
      "tensor(4, device='cuda:2') tensor(0.0307, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4507, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.45068359375\n",
      "tensor(4, device='cuda:2') tensor(0.0276, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4424, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.4423828125\n",
      "tensor(4, device='cuda:2') tensor(0.0319, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4340, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.43402099609375\n",
      "tensor(4, device='cuda:2') tensor(0.0342, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4258, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.42584228515625\n",
      "tensor(4, device='cuda:2') tensor(0.0324, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4178, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.41778564453125\n",
      "tensor(4, device='cuda:2') tensor(0.0383, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4095, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.40948486328125\n",
      "tensor(4, device='cuda:2') tensor(0.0364, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.4010, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.4010314941406\n",
      "tensor(4, device='cuda:2') tensor(0.0391, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3925, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.3925476074219\n",
      "tensor(4, device='cuda:2') tensor(0.0419, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3842, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.38421630859375\n",
      "tensor(4, device='cuda:2') tensor(0.0402, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3759, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.3758544921875\n",
      "tensor(4, device='cuda:2') tensor(0.0457, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3674, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.36737060546875\n",
      "tensor(4, device='cuda:2') tensor(0.0434, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3588, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.3587646484375\n",
      "tensor(4, device='cuda:2') tensor(0.0476, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3500, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.3500061035156\n",
      "tensor(4, device='cuda:2') tensor(0.0478, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3412, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.34124755859375\n",
      "tensor(4, device='cuda:2') tensor(0.0490, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3325, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.33245849609375\n",
      "tensor(4, device='cuda:2') tensor(0.0522, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3236, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.3236389160156\n",
      "tensor(4, device='cuda:2') tensor(0.0508, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3148, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.3147888183594\n",
      "tensor(4, device='cuda:2') tensor(0.0563, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.3058, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.30584716796875\n",
      "tensor(4, device='cuda:2') tensor(0.0528, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2969, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.296875\n",
      "tensor(4, device='cuda:2') tensor(0.0607, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2880, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.2879638671875\n",
      "tensor(4, device='cuda:2') tensor(0.0545, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2791, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.27911376953125\n",
      "tensor(4, device='cuda:2') tensor(0.0658, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2704, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.2703857421875\n",
      "tensor(4, device='cuda:2') tensor(0.0560, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2617, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.2617492675781\n",
      "tensor(4, device='cuda:2') tensor(0.0704, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2528, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.2527770996094\n",
      "tensor(4, device='cuda:2') tensor(0.0599, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2431, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.2430725097656\n",
      "tensor(4, device='cuda:2') tensor(0.0709, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2328, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.23284912109375\n",
      "tensor(4, device='cuda:2') tensor(0.0684, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2230, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.22296142578125\n",
      "tensor(4, device='cuda:2') tensor(0.0690, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2138, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.2138366699219\n",
      "tensor(4, device='cuda:2') tensor(0.0769, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.2052, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.2052001953125\n",
      "tensor(4, device='cuda:2') tensor(0.0694, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1964, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.1964111328125\n",
      "tensor(4, device='cuda:2') tensor(0.0821, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1871, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.18707275390625\n",
      "tensor(4, device='cuda:2') tensor(0.0742, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1770, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.1770324707031\n",
      "tensor(4, device='cuda:2') tensor(0.0831, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1667, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.166748046875\n",
      "tensor(4, device='cuda:2') tensor(0.0825, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1569, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.1568603515625\n",
      "tensor(4, device='cuda:2') tensor(0.0828, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1474, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.14739990234375\n",
      "tensor(4, device='cuda:2') tensor(0.0905, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1382, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.1381530761719\n",
      "tensor(4, device='cuda:2') tensor(0.0842, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1289, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.12890625\n",
      "tensor(4, device='cuda:2') tensor(0.0969, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1194, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.11944580078125\n",
      "tensor(4, device='cuda:2') tensor(0.0880, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.1096, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.109619140625\n",
      "tensor(4, device='cuda:2') tensor(0.1008, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0993, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.09930419921875\n",
      "tensor(4, device='cuda:2') tensor(0.0947, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0887, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.08868408203125\n",
      "tensor(4, device='cuda:2') tensor(0.1026, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0782, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.07818603515625\n",
      "tensor(4, device='cuda:2') tensor(0.1030, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0678, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.06781005859375\n",
      "tensor(4, device='cuda:2') tensor(0.1044, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0577, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.05767822265625\n",
      "tensor(4, device='cuda:2') tensor(0.1112, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0477, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.04766845703125\n",
      "tensor(4, device='cuda:2') tensor(0.1068, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0379, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.0378723144531\n",
      "tensor(4, device='cuda:2') tensor(0.1198, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0284, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.02838134765625\n",
      "tensor(4, device='cuda:2') tensor(0.1087, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0195, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.01947021484375\n",
      "tensor(4, device='cuda:2') tensor(0.1290, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0108, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.01080322265625\n",
      "tensor(4, device='cuda:2') tensor(0.1121, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(293.0011, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293.0010681152344\n",
      "tensor(4, device='cuda:2') tensor(0.1329, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9886, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.9886474609375\n",
      "tensor(4, device='cuda:2') tensor(0.1244, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9755, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.97552490234375\n",
      "tensor(4, device='cuda:2') tensor(0.1287, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9648, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.9648132324219\n",
      "tensor(4, device='cuda:2') tensor(0.1398, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9565, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.95654296875\n",
      "tensor(4, device='cuda:2') tensor(0.1283, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9479, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.94793701171875\n",
      "tensor(4, device='cuda:2') tensor(0.1470, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9369, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.9368896484375\n",
      "tensor(4, device='cuda:2') tensor(0.1385, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9243, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.92431640625\n",
      "tensor(4, device='cuda:2') tensor(0.1452, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9130, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.9129943847656\n",
      "tensor(4, device='cuda:2') tensor(0.1532, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.9037, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.9036560058594\n",
      "tensor(4, device='cuda:2') tensor(0.1452, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8947, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.8946838378906\n",
      "tensor(4, device='cuda:2') tensor(0.1622, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8844, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.8843688964844\n",
      "tensor(4, device='cuda:2') tensor(0.1531, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8726, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.87255859375\n",
      "tensor(4, device='cuda:2') tensor(0.1640, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8606, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.860595703125\n",
      "tensor(4, device='cuda:2') tensor(0.1663, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8496, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.8496398925781\n",
      "tensor(4, device='cuda:2') tensor(0.1647, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8397, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.8397216796875\n",
      "tensor(4, device='cuda:2') tensor(0.1782, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8300, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.83001708984375\n",
      "tensor(4, device='cuda:2') tensor(0.1688, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8199, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.8199462890625\n",
      "tensor(4, device='cuda:2') tensor(0.1863, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.8090, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.8089599609375\n",
      "tensor(4, device='cuda:2') tensor(0.1771, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7973, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.79730224609375\n",
      "tensor(4, device='cuda:2') tensor(0.1908, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7853, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.78533935546875\n",
      "tensor(4, device='cuda:2') tensor(0.1886, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7736, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.7735595703125\n",
      "tensor(4, device='cuda:2') tensor(0.1942, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7623, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.76226806640625\n",
      "tensor(4, device='cuda:2') tensor(0.2004, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7513, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.7513427734375\n",
      "tensor(4, device='cuda:2') tensor(0.1985, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7407, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.74072265625\n",
      "tensor(4, device='cuda:2') tensor(0.2123, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7306, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.7305908203125\n",
      "tensor(4, device='cuda:2') tensor(0.2022, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7215, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.72149658203125\n",
      "tensor(4, device='cuda:2') tensor(0.2263, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7140, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.7139892578125\n",
      "tensor(4, device='cuda:2') tensor(0.2044, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.7076, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.70758056640625\n",
      "tensor(4, device='cuda:2') tensor(0.2376, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6975, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.697509765625\n",
      "tensor(4, device='cuda:2') tensor(0.2161, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6805, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.68048095703125\n",
      "tensor(4, device='cuda:2') tensor(0.2329, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6635, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.66351318359375\n",
      "tensor(4, device='cuda:2') tensor(0.2407, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6550, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.65496826171875\n",
      "tensor(4, device='cuda:2') tensor(0.2270, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6502, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.65020751953125\n",
      "tensor(4, device='cuda:2') tensor(0.2539, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6397, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.63970947265625\n",
      "tensor(4, device='cuda:2') tensor(0.2403, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6237, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.62371826171875\n",
      "tensor(4, device='cuda:2') tensor(0.2487, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6113, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.61126708984375\n",
      "tensor(4, device='cuda:2') tensor(0.2625, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.6047, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.60467529296875\n",
      "tensor(4, device='cuda:2') tensor(0.2482, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5970, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.5970153808594\n",
      "tensor(4, device='cuda:2') tensor(0.2705, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5839, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.58392333984375\n",
      "tensor(4, device='cuda:2') tensor(0.2646, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5701, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.57012939453125\n",
      "tensor(4, device='cuda:2') tensor(0.2669, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5605, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.560546875\n",
      "tensor(4, device='cuda:2') tensor(0.2833, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5532, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.55322265625\n",
      "tensor(4, device='cuda:2') tensor(0.2707, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5438, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.54376220703125\n",
      "tensor(4, device='cuda:2') tensor(0.2901, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5311, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.5310974121094\n",
      "tensor(4, device='cuda:2') tensor(0.2863, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5186, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.5185546875\n",
      "tensor(4, device='cuda:2') tensor(0.2897, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5085, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.508544921875\n",
      "tensor(4, device='cuda:2') tensor(0.3034, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.5002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.500244140625\n",
      "tensor(4, device='cuda:2') tensor(0.2935, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4915, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.491455078125\n",
      "tensor(4, device='cuda:2') tensor(0.3139, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4809, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.48089599609375\n",
      "tensor(4, device='cuda:2') tensor(0.3048, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4691, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.46905517578125\n",
      "tensor(4, device='cuda:2') tensor(0.3183, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4572, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.45721435546875\n",
      "tensor(4, device='cuda:2') tensor(0.3200, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4464, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.44635009765625\n",
      "tensor(4, device='cuda:2') tensor(0.3218, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4366, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.43658447265625\n",
      "tensor(4, device='cuda:2') tensor(0.3348, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4276, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.42755126953125\n",
      "tensor(4, device='cuda:2') tensor(0.3269, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4190, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.4189758300781\n",
      "tensor(4, device='cuda:2') tensor(0.3486, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4108, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.41082763671875\n",
      "tensor(4, device='cuda:2') tensor(0.3328, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.4032, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.4031677246094\n",
      "tensor(4, device='cuda:2') tensor(0.3614, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3951, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.3951110839844\n",
      "tensor(4, device='cuda:2') tensor(0.3409, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3856, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.3856201171875\n",
      "tensor(4, device='cuda:2') tensor(0.3693, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3732, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.3731689453125\n",
      "tensor(4, device='cuda:2') tensor(0.3558, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3593, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.35931396484375\n",
      "tensor(4, device='cuda:2') tensor(0.3704, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3467, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.34674072265625\n",
      "tensor(4, device='cuda:2') tensor(0.3748, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3370, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.3370361328125\n",
      "tensor(4, device='cuda:2') tensor(0.3712, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3297, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.32965087890625\n",
      "tensor(4, device='cuda:2') tensor(0.3917, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3233, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.32330322265625\n",
      "tensor(4, device='cuda:2') tensor(0.3755, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3168, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.31683349609375\n",
      "tensor(4, device='cuda:2') tensor(0.4043, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.3087, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.30865478515625\n",
      "tensor(4, device='cuda:2') tensor(0.3853, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2980, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2980041503906\n",
      "tensor(4, device='cuda:2') tensor(0.4099, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2851, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2850646972656\n",
      "tensor(4, device='cuda:2') tensor(0.4020, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2725, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.27252197265625\n",
      "tensor(4, device='cuda:2') tensor(0.4108, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2623, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2622985839844\n",
      "tensor(4, device='cuda:2') tensor(0.4204, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2545, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2544860839844\n",
      "tensor(4, device='cuda:2') tensor(0.4132, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2481, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2481384277344\n",
      "tensor(4, device='cuda:2') tensor(0.4363, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2423, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2422790527344\n",
      "tensor(4, device='cuda:2') tensor(0.4185, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2362, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2361755371094\n",
      "tensor(4, device='cuda:2') tensor(0.4484, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2283, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.228271484375\n",
      "tensor(4, device='cuda:2') tensor(0.4287, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2180, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.2179870605469\n",
      "tensor(4, device='cuda:2') tensor(0.4541, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.2055, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.20550537109375\n",
      "tensor(4, device='cuda:2') tensor(0.4453, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1933, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.19329833984375\n",
      "tensor(4, device='cuda:2') tensor(0.4551, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1833, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.183349609375\n",
      "tensor(4, device='cuda:2') tensor(0.4635, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1758, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.17584228515625\n",
      "tensor(4, device='cuda:2') tensor(0.4573, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1699, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.169921875\n",
      "tensor(4, device='cuda:2') tensor(0.4795, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1647, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.1647033691406\n",
      "tensor(4, device='cuda:2') tensor(0.4620, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1596, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.1596374511719\n",
      "tensor(4, device='cuda:2') tensor(0.4925, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1534, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.15338134765625\n",
      "tensor(4, device='cuda:2') tensor(0.4706, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1449, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.1448974609375\n",
      "tensor(4, device='cuda:2') tensor(0.4995, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1333, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.13330078125\n",
      "tensor(4, device='cuda:2') tensor(0.4860, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1208, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.12078857421875\n",
      "tensor(4, device='cuda:2') tensor(0.5004, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1099, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.10986328125\n",
      "tensor(4, device='cuda:2') tensor(0.5046, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.1018, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.101806640625\n",
      "tensor(4, device='cuda:2') tensor(0.5013, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0961, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.0960693359375\n",
      "tensor(4, device='cuda:2') tensor(0.5213, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0915, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.09149169921875\n",
      "tensor(4, device='cuda:2') tensor(0.5052, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0871, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.08709716796875\n",
      "tensor(4, device='cuda:2') tensor(0.5346, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0817, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.08172607421875\n",
      "tensor(4, device='cuda:2') tensor(0.5130, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0744, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.0743713378906\n",
      "tensor(4, device='cuda:2') tensor(0.5424, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0640, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.06402587890625\n",
      "tensor(4, device='cuda:2') tensor(0.5271, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0522, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.05224609375\n",
      "tensor(4, device='cuda:2') tensor(0.5442, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0413, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.041259765625\n",
      "tensor(4, device='cuda:2') tensor(0.5450, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0327, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.03265380859375\n",
      "tensor(4, device='cuda:2') tensor(0.5451, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0265, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.0264587402344\n",
      "tensor(4, device='cuda:2') tensor(0.5616, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0217, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.021728515625\n",
      "tensor(4, device='cuda:2') tensor(0.5484, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0177, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.0177001953125\n",
      "tensor(4, device='cuda:2') tensor(0.5758, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0135, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.0135498046875\n",
      "tensor(4, device='cuda:2') tensor(0.5543, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0086, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.0086364746094\n",
      "tensor(4, device='cuda:2') tensor(0.5862, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(292.0013, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292.00128173828125\n",
      "tensor(4, device='cuda:2') tensor(0.5651, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9914, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.99139404296875\n",
      "tensor(4, device='cuda:2') tensor(0.5903, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9796, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.9796447753906\n",
      "tensor(4, device='cuda:2') tensor(0.5816, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9688, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.96881103515625\n",
      "tensor(4, device='cuda:2') tensor(0.5905, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9605, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.9604797363281\n",
      "tensor(4, device='cuda:2') tensor(0.5991, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9546, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.95458984375\n",
      "tensor(4, device='cuda:2') tensor(0.5921, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9503, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.9503173828125\n",
      "tensor(4, device='cuda:2') tensor(0.6144, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9467, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.9466552734375\n",
      "tensor(4, device='cuda:2') tensor(0.5962, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9431, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.9430847167969\n",
      "tensor(4, device='cuda:2') tensor(0.6267, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9384, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.9383850097656\n",
      "tensor(4, device='cuda:2') tensor(0.6038, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9318, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.93182373046875\n",
      "tensor(4, device='cuda:2') tensor(0.6341, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9223, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.92230224609375\n",
      "tensor(4, device='cuda:2') tensor(0.6170, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9112, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.91119384765625\n",
      "tensor(4, device='cuda:2') tensor(0.6357, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.9005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.9004821777344\n",
      "tensor(4, device='cuda:2') tensor(0.6340, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8920, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8919982910156\n",
      "tensor(4, device='cuda:2') tensor(0.6362, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8859, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8858947753906\n",
      "tensor(4, device='cuda:2') tensor(0.6502, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8815, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8814697265625\n",
      "tensor(4, device='cuda:2') tensor(0.6387, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8780, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.87799072265625\n",
      "tensor(4, device='cuda:2') tensor(0.6642, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8746, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8746337890625\n",
      "tensor(4, device='cuda:2') tensor(0.6435, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8711, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.87109375\n",
      "tensor(4, device='cuda:2') tensor(0.6755, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8659, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8658752441406\n",
      "tensor(4, device='cuda:2') tensor(0.6519, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8585, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8585205078125\n",
      "tensor(4, device='cuda:2') tensor(0.6815, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8483, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.84832763671875\n",
      "tensor(4, device='cuda:2') tensor(0.6660, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8372, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.83721923828125\n",
      "tensor(4, device='cuda:2') tensor(0.6825, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8272, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.82720947265625\n",
      "tensor(4, device='cuda:2') tensor(0.6830, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8197, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8196716308594\n",
      "tensor(4, device='cuda:2') tensor(0.6830, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8144, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.81439208984375\n",
      "tensor(4, device='cuda:2') tensor(0.6988, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8105, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.81048583984375\n",
      "tensor(4, device='cuda:2') tensor(0.6857, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8073, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8073425292969\n",
      "tensor(4, device='cuda:2') tensor(0.7123, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8042, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.80419921875\n",
      "tensor(4, device='cuda:2') tensor(0.6907, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.8007, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.8006896972656\n",
      "tensor(4, device='cuda:2') tensor(0.7229, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7953, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.7953186035156\n",
      "tensor(4, device='cuda:2') tensor(0.6993, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7878, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.78778076171875\n",
      "tensor(4, device='cuda:2') tensor(0.7285, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7776, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.777587890625\n",
      "tensor(4, device='cuda:2') tensor(0.7133, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7668, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.76678466796875\n",
      "tensor(4, device='cuda:2') tensor(0.7294, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7571, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.757080078125\n",
      "tensor(4, device='cuda:2') tensor(0.7299, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7497, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.74969482421875\n",
      "tensor(4, device='cuda:2') tensor(0.7301, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7445, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.7444763183594\n",
      "tensor(4, device='cuda:2') tensor(0.7453, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7406, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.7406311035156\n",
      "tensor(4, device='cuda:2') tensor(0.7328, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7375, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.7375183105469\n",
      "tensor(4, device='cuda:2') tensor(0.7587, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7346, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.73455810546875\n",
      "tensor(4, device='cuda:2') tensor(0.7374, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7315, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.73150634765625\n",
      "tensor(4, device='cuda:2') tensor(0.7699, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7271, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.72705078125\n",
      "tensor(4, device='cuda:2') tensor(0.7449, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7208, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.72076416015625\n",
      "tensor(4, device='cuda:2') tensor(0.7765, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7114, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.71142578125\n",
      "tensor(4, device='cuda:2') tensor(0.7577, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.7006, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.70062255859375\n",
      "tensor(4, device='cuda:2') tensor(0.7779, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6900, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.69000244140625\n",
      "tensor(4, device='cuda:2') tensor(0.7741, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6815, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.6815490722656\n",
      "tensor(4, device='cuda:2') tensor(0.7780, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6754, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.6754455566406\n",
      "tensor(4, device='cuda:2') tensor(0.7900, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6712, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.67120361328125\n",
      "tensor(4, device='cuda:2') tensor(0.7799, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6680, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.66802978515625\n",
      "tensor(4, device='cuda:2') tensor(0.8037, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6652, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.6651611328125\n",
      "tensor(4, device='cuda:2') tensor(0.7839, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6624, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.66241455078125\n",
      "tensor(4, device='cuda:2') tensor(0.8154, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6587, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.6586608886719\n",
      "tensor(4, device='cuda:2') tensor(0.7905, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6536, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.65362548828125\n",
      "tensor(4, device='cuda:2') tensor(0.8234, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6458, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.645751953125\n",
      "tensor(4, device='cuda:2') tensor(0.8017, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6359, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.63592529296875\n",
      "tensor(4, device='cuda:2') tensor(0.8262, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6251, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.62506103515625\n",
      "tensor(4, device='cuda:2') tensor(0.8172, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6154, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.6153564453125\n",
      "tensor(4, device='cuda:2') tensor(0.8265, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6078, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.6077880859375\n",
      "tensor(4, device='cuda:2') tensor(0.8332, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.6024, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.60235595703125\n",
      "tensor(4, device='cuda:2') tensor(0.8280, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5983, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.59832763671875\n",
      "tensor(4, device='cuda:2') tensor(0.8475, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5952, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.59515380859375\n",
      "tensor(4, device='cuda:2') tensor(0.8314, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5925, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.59246826171875\n",
      "tensor(4, device='cuda:2') tensor(0.8603, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5898, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.58978271484375\n",
      "tensor(4, device='cuda:2') tensor(0.8363, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5869, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.58685302734375\n",
      "tensor(4, device='cuda:2') tensor(0.8709, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5822, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.58221435546875\n",
      "tensor(4, device='cuda:2') tensor(0.8442, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5754, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.575439453125\n",
      "tensor(4, device='cuda:2') tensor(0.8769, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5654, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.5654296875\n",
      "tensor(4, device='cuda:2') tensor(0.8576, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5542, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.5541687011719\n",
      "tensor(4, device='cuda:2') tensor(0.8778, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5434, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.5434265136719\n",
      "tensor(4, device='cuda:2') tensor(0.8744, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5351, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.5350646972656\n",
      "tensor(4, device='cuda:2') tensor(0.8777, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5293, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.5292663574219\n",
      "tensor(4, device='cuda:2') tensor(0.8905, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5254, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.525390625\n",
      "tensor(4, device='cuda:2') tensor(0.8796, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5225, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.52252197265625\n",
      "tensor(4, device='cuda:2') tensor(0.9045, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5199, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.5199279785156\n",
      "tensor(4, device='cuda:2') tensor(0.8839, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5173, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.5173034667969\n",
      "tensor(4, device='cuda:2') tensor(0.9162, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5136, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.51361083984375\n",
      "tensor(4, device='cuda:2') tensor(0.8907, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5085, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.508544921875\n",
      "tensor(4, device='cuda:2') tensor(0.9242, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.5008, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.50079345703125\n",
      "tensor(4, device='cuda:2') tensor(0.9018, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4912, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4912109375\n",
      "tensor(4, device='cuda:2') tensor(0.9277, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4805, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.48046875\n",
      "tensor(4, device='cuda:2') tensor(0.9170, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4706, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4705810546875\n",
      "tensor(4, device='cuda:2') tensor(0.9287, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4625, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4625244140625\n",
      "tensor(4, device='cuda:2') tensor(0.9329, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4565, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4564514160156\n",
      "tensor(4, device='cuda:2') tensor(0.9306, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4519, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4518737792969\n",
      "tensor(4, device='cuda:2') tensor(0.9475, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4482, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4482421875\n",
      "tensor(4, device='cuda:2') tensor(0.9340, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4453, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4453430175781\n",
      "tensor(4, device='cuda:2') tensor(0.9612, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4429, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4429016113281\n",
      "tensor(4, device='cuda:2') tensor(0.9383, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4412, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.441162109375\n",
      "tensor(4, device='cuda:2') tensor(0.9739, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4389, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.43890380859375\n",
      "tensor(4, device='cuda:2') tensor(0.9441, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4359, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.43585205078125\n",
      "tensor(4, device='cuda:2') tensor(0.9836, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4291, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.42913818359375\n",
      "tensor(4, device='cuda:2') tensor(0.9547, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4191, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4190979003906\n",
      "tensor(4, device='cuda:2') tensor(0.9868, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.4059, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.4059143066406\n",
      "tensor(4, device='cuda:2') tensor(0.9718, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3933, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.393310546875\n",
      "tensor(4, device='cuda:2') tensor(0.9852, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3839, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.38385009765625\n",
      "tensor(4, device='cuda:2') tensor(0.9908, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3783, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3782958984375\n",
      "tensor(4, device='cuda:2') tensor(0.9846, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3755, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.37548828125\n",
      "tensor(4, device='cuda:2') tensor(1.0072, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3737, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.37371826171875\n",
      "tensor(4, device='cuda:2') tensor(0.9877, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3718, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.371826171875\n",
      "tensor(4, device='cuda:2') tensor(1.0198, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3682, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.36822509765625\n",
      "tensor(4, device='cuda:2') tensor(0.9951, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3627, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3626708984375\n",
      "tensor(4, device='cuda:2') tensor(1.0277, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3542, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3542175292969\n",
      "tensor(4, device='cuda:2') tensor(1.0074, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3442, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.34423828125\n",
      "tensor(4, device='cuda:2') tensor(1.0311, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3338, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3338317871094\n",
      "tensor(4, device='cuda:2') tensor(1.0232, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3246, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3246154785156\n",
      "tensor(4, device='cuda:2') tensor(1.0329, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3173, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3172607421875\n",
      "tensor(4, device='cuda:2') tensor(1.0391, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3115, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3115234375\n",
      "tensor(4, device='cuda:2') tensor(1.0358, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3070, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3070373535156\n",
      "tensor(4, device='cuda:2') tensor(1.0538, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3033, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.3033447265625\n",
      "tensor(4, device='cuda:2') tensor(1.0401, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.3004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.30035400390625\n",
      "tensor(4, device='cuda:2') tensor(1.0679, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2980, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.2979736328125\n",
      "tensor(4, device='cuda:2') tensor(1.0447, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2966, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.296630859375\n",
      "tensor(4, device='cuda:2') tensor(1.0818, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2954, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.29541015625\n",
      "tensor(4, device='cuda:2') tensor(1.0500, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2941, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.29412841796875\n",
      "tensor(4, device='cuda:2') tensor(1.0934, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2894, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.2894287109375\n",
      "tensor(4, device='cuda:2') tensor(1.0596, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2809, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.2808837890625\n",
      "tensor(4, device='cuda:2') tensor(1.0983, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2669, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.2668762207031\n",
      "tensor(4, device='cuda:2') tensor(1.0770, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2518, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.25177001953125\n",
      "tensor(4, device='cuda:2') tensor(1.0965, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2394, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.2393798828125\n",
      "tensor(4, device='cuda:2') tensor(1.0983, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2321, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.23211669921875\n",
      "tensor(4, device='cuda:2') tensor(1.0945, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2292, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.229248046875\n",
      "tensor(4, device='cuda:2') tensor(1.1170, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2284, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.2283630371094\n",
      "tensor(4, device='cuda:2') tensor(1.0969, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2275, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.22747802734375\n",
      "tensor(4, device='cuda:2') tensor(1.1307, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2242, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.22418212890625\n",
      "tensor(4, device='cuda:2') tensor(1.1050, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2181, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.2181396484375\n",
      "tensor(4, device='cuda:2') tensor(1.1385, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.2085, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.20849609375\n",
      "tensor(4, device='cuda:2') tensor(1.1191, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1974, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.19744873046875\n",
      "tensor(4, device='cuda:2') tensor(1.1412, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1866, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1866455078125\n",
      "tensor(4, device='cuda:2') tensor(1.1366, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1778, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.17779541015625\n",
      "tensor(4, device='cuda:2') tensor(1.1428, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1713, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.17132568359375\n",
      "tensor(4, device='cuda:2') tensor(1.1537, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1667, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1666564941406\n",
      "tensor(4, device='cuda:2') tensor(1.1461, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1632, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1631774902344\n",
      "tensor(4, device='cuda:2') tensor(1.1691, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1602, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.16021728515625\n",
      "tensor(4, device='cuda:2') tensor(1.1512, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1577, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1577453613281\n",
      "tensor(4, device='cuda:2') tensor(1.1832, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1551, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1551208496094\n",
      "tensor(4, device='cuda:2') tensor(1.1576, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1526, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1526184082031\n",
      "tensor(4, device='cuda:2') tensor(1.1959, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1487, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.14874267578125\n",
      "tensor(4, device='cuda:2') tensor(1.1661, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1437, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1437072753906\n",
      "tensor(4, device='cuda:2') tensor(1.2053, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1355, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1354675292969\n",
      "tensor(4, device='cuda:2') tensor(1.1786, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1252, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1252136230469\n",
      "tensor(4, device='cuda:2') tensor(1.2102, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1132, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.11322021484375\n",
      "tensor(4, device='cuda:2') tensor(1.1954, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.1018, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.1017761230469\n",
      "tensor(4, device='cuda:2') tensor(1.2123, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.0922, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.0921630859375\n",
      "tensor(4, device='cuda:2') tensor(1.2135, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.0849, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.08489990234375\n",
      "tensor(4, device='cuda:2') tensor(1.2149, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.0797, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.07965087890625\n",
      "tensor(4, device='cuda:2') tensor(1.2304, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.0757, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.07574462890625\n",
      "tensor(4, device='cuda:2') tensor(1.2191, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.0728, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.07281494140625\n",
      "tensor(4, device='cuda:2') tensor(1.2462, device='cuda:2', grad_fn=<SqueezeBackward0>)\n",
      "tensor(291.0706, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291.07061767578125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      7\u001b[0m     q\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_list_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# loss = loss * 0.0\u001b[39;00m\n",
      "File \u001b[0;32m~/ionChannel/models.py:425\u001b[0m, in \u001b[0;36mIonBaseclf._list_training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    421\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# print(x[\"id\"][0])\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m42\u001b[39m:\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_t\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m12\u001b[39m], s)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clsmodel = clsmodel.to(2)\n",
    "embeds = embeds.to(2)\n",
    "cri = models.ListMLE()\n",
    "# clsmodel.eval()\n",
    "# for i in data:\n",
    "#     i[\"seq_t\"] = i[\"seq_t\"].to(2)\n",
    "q = torch.optim.Adam(filter(lambda x: x.requires_grad, clsmodel.parameters()), lr=0.0001)\n",
    "for i in range(1000):\n",
    "    q.zero_grad()\n",
    "    loss = cri(clsmodel(embeds).squeeze(1))\n",
    "    # loss = clsmodel._list_training_step(data)\n",
    "    print(loss)\n",
    "    # loss = loss * 0.0\n",
    "    loss.backward()\n",
    "    q.step()\n",
    "    print(loss.item())\n",
    "    # print(model(data).item())\n",
    "    # print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.l1.weight Parameter containing:\n",
      "tensor([[ 5.6183e-03, -7.2583e-04,  9.2370e-04,  ...,  2.3575e-03,\n",
      "         -6.6478e-03, -2.2759e-03],\n",
      "        [-4.2368e-03,  3.8491e-03, -3.8733e-03,  ..., -3.8524e-03,\n",
      "         -4.0215e-03,  3.9197e-03],\n",
      "        [ 1.6638e-03,  8.2855e-04,  1.1220e-03,  ..., -5.4048e-04,\n",
      "         -3.3162e-05, -1.0610e-03],\n",
      "        ...,\n",
      "        [-4.1821e-03,  2.9903e-03, -2.4395e-03,  ..., -1.5212e-03,\n",
      "         -2.8255e-03,  3.6214e-03],\n",
      "        [ 1.8198e-03, -1.1366e-03,  1.9718e-03,  ...,  9.9019e-05,\n",
      "          7.1877e-04, -1.6034e-03],\n",
      "        [-1.1829e-03,  5.6399e-03, -4.5914e-03,  ...,  3.6545e-03,\n",
      "         -1.4379e-04,  5.3215e-03]], device='cuda:2', requires_grad=True)\n",
      "clf.l1.bias Parameter containing:\n",
      "tensor([ 2.9316e-02, -6.9022e-03, -6.3497e-04,  2.0163e-02,  1.9693e-03,\n",
      "         4.1604e-03,  7.1277e-03,  2.1045e-02,  1.6818e-02,  1.6161e-03,\n",
      "         1.4598e-02, -1.0070e-02, -5.4684e-04, -4.7448e-03, -1.8326e-03,\n",
      "         6.9637e-03,  6.9514e-03,  2.1132e-03, -3.2294e-03, -7.6070e-03,\n",
      "         1.1627e-02,  2.2202e-02,  1.3819e-03,  1.1194e-03,  1.7981e-02,\n",
      "         8.4880e-03,  1.7360e-02, -2.1212e-04,  5.2687e-03, -7.0888e-03,\n",
      "         1.3577e-02,  7.2277e-04,  1.3848e-02,  4.1628e-03,  1.5805e-03,\n",
      "         1.4541e-02,  2.3371e-02,  5.3903e-05,  4.1947e-03,  1.0670e-02,\n",
      "         1.5079e-02, -1.1911e-02,  1.1945e-02, -3.8656e-03,  2.1448e-02,\n",
      "         1.4740e-02,  1.4879e-02, -4.2359e-03,  1.7297e-02,  7.6483e-03,\n",
      "         2.9752e-03,  1.0214e-02,  8.8905e-04,  1.3978e-03, -4.4592e-04,\n",
      "         9.9586e-03,  1.7862e-02,  2.0642e-02,  4.9035e-03,  1.1962e-02,\n",
      "        -7.2790e-03,  2.7525e-04,  7.4641e-04,  4.4860e-03,  1.2593e-02,\n",
      "         1.7946e-02,  9.8589e-03,  1.5900e-02,  4.0260e-03,  4.0096e-03,\n",
      "        -6.1903e-03, -2.9462e-03,  2.3569e-02,  2.6050e-02,  3.7106e-03,\n",
      "         2.6087e-03,  1.2064e-02, -8.6739e-05,  8.5181e-03,  2.0361e-04,\n",
      "        -2.9976e-03,  2.4627e-03,  5.2186e-03,  4.9930e-03,  1.0881e-02,\n",
      "         1.1715e-02,  3.3346e-03, -7.3297e-03,  4.6348e-03,  6.0813e-04,\n",
      "        -2.5928e-04,  6.9294e-03,  4.4036e-03, -5.0533e-03, -4.0425e-04,\n",
      "         1.3600e-02, -1.3423e-03,  9.2342e-03,  1.0444e-02,  1.9833e-02,\n",
      "         1.7261e-02,  2.7908e-02,  3.6758e-03,  1.1956e-02,  1.5728e-02,\n",
      "        -3.4458e-03, -4.1225e-03,  5.1880e-04, -1.5313e-04, -2.7326e-03,\n",
      "         6.9649e-03,  7.5553e-04,  1.7948e-03, -1.1249e-02, -4.9315e-03,\n",
      "         4.4669e-03, -3.0221e-03,  7.3532e-03,  8.6020e-03, -3.0623e-03,\n",
      "         8.8783e-03,  2.5940e-03,  1.3334e-02,  1.9238e-02,  4.7789e-03,\n",
      "         2.0239e-03,  7.6495e-03,  6.2369e-04,  3.1390e-03, -5.6758e-03,\n",
      "         1.6585e-02,  1.0855e-02, -1.5788e-03,  2.8579e-02,  7.1673e-03,\n",
      "        -2.0624e-03,  1.1089e-03,  1.1957e-02,  9.6818e-03,  1.5314e-02,\n",
      "        -4.8753e-03, -2.0971e-03, -4.7343e-03, -1.4695e-03, -6.0603e-04,\n",
      "         1.3654e-03,  2.5255e-03,  5.5901e-03,  3.7474e-03,  5.8833e-03,\n",
      "         1.5084e-02,  7.1234e-03,  6.0208e-03,  4.9580e-03, -3.3189e-03,\n",
      "        -4.0573e-03, -5.3487e-03,  2.4141e-03,  7.9498e-03,  1.2017e-02,\n",
      "        -5.2094e-03,  5.4687e-03,  6.2032e-03,  1.6029e-02,  1.1218e-02,\n",
      "         4.4153e-03,  2.8527e-02, -5.5969e-03, -6.2284e-04, -9.7085e-04,\n",
      "         2.8682e-03,  2.2677e-03,  2.9059e-03, -4.5419e-03,  3.0941e-02,\n",
      "         1.3289e-03,  2.4669e-03, -6.6660e-03, -6.1192e-03,  6.8022e-03,\n",
      "         1.4670e-03, -6.1687e-03,  2.9753e-02,  8.4967e-04,  1.1984e-02,\n",
      "         1.1585e-02, -2.2384e-03,  1.3972e-02,  9.7625e-04, -7.5939e-03,\n",
      "        -3.5041e-06,  5.6072e-03,  2.4469e-02,  1.1861e-02, -3.4935e-03,\n",
      "         1.4935e-02,  2.5431e-04,  2.5519e-03, -8.9539e-03, -1.5088e-03,\n",
      "        -1.3453e-03,  8.7516e-03,  1.4115e-03,  2.3824e-02,  2.5549e-02,\n",
      "         9.2784e-03, -1.2691e-02,  4.0351e-03,  1.8892e-03,  5.0049e-03,\n",
      "        -1.5777e-03,  1.0276e-02, -1.0264e-02,  2.4648e-03,  1.6148e-02,\n",
      "         1.8465e-02, -8.5780e-03,  7.9008e-03, -2.6921e-03,  2.3246e-03,\n",
      "         7.4658e-03,  6.1668e-03, -5.9395e-03,  1.4024e-02,  2.8172e-04,\n",
      "         7.7910e-03,  1.7675e-03, -2.8120e-03,  9.9277e-03,  1.1494e-02,\n",
      "         1.2200e-02,  4.1551e-04,  8.5567e-03,  1.5739e-03,  2.3703e-03,\n",
      "         6.5206e-03,  3.3215e-03, -5.6454e-03,  2.0893e-02,  9.3659e-03,\n",
      "        -4.4474e-03,  1.3512e-02, -7.6348e-04, -4.9629e-03, -5.2683e-04,\n",
      "         2.6304e-03,  2.5258e-03,  6.1397e-03, -6.7724e-04,  7.9233e-03,\n",
      "         1.5253e-02,  5.5759e-03,  9.7872e-04, -9.6808e-03,  4.4343e-03,\n",
      "        -1.3686e-02, -2.0651e-03, -3.0731e-03,  1.4955e-02, -2.9253e-04,\n",
      "        -1.0091e-02,  8.6937e-03,  5.9277e-03,  2.8592e-03,  1.1561e-03,\n",
      "         6.4625e-03,  4.2199e-03,  1.1651e-02,  2.4569e-03,  1.3970e-03,\n",
      "         1.4554e-03,  6.3237e-03,  2.1123e-02, -3.6490e-04, -3.9099e-03,\n",
      "         7.7017e-03,  3.8866e-03,  4.4428e-04,  9.6337e-03, -7.5204e-03,\n",
      "         7.1176e-03, -7.3598e-03,  4.1036e-03,  2.5905e-02,  2.6718e-05,\n",
      "         3.6008e-03,  4.4432e-03, -2.2302e-03,  1.1753e-02,  1.1404e-02,\n",
      "         1.2019e-02,  2.8390e-03,  7.2568e-03,  1.5399e-02,  5.7405e-03,\n",
      "         3.5565e-03,  1.2405e-02,  5.8824e-03, -4.7023e-03,  1.4737e-04,\n",
      "         5.8616e-04,  1.2182e-02,  1.5356e-02,  8.7733e-03,  6.8594e-03,\n",
      "        -7.1624e-03,  1.0725e-02, -5.2411e-03,  1.9329e-02, -3.6959e-03,\n",
      "        -2.5093e-03,  2.5536e-02,  1.0697e-03,  4.3422e-04,  3.2651e-03,\n",
      "         1.8039e-02,  6.0013e-03,  2.0646e-03,  3.0085e-03, -1.5119e-03,\n",
      "        -4.0721e-03,  5.8524e-03,  4.5170e-03,  3.0610e-02,  1.1355e-03,\n",
      "         8.0239e-03,  6.4897e-03,  1.5216e-02,  1.8676e-02, -7.8732e-03,\n",
      "        -3.1069e-03,  2.3816e-02, -3.0422e-03,  4.7021e-03, -1.5728e-02,\n",
      "        -4.2533e-04,  6.0497e-03, -2.6682e-03,  9.0384e-03, -1.7775e-04,\n",
      "         8.9290e-03,  5.5556e-03, -3.1449e-03, -7.5259e-03,  2.4541e-02,\n",
      "         1.6526e-03,  3.8601e-03,  1.0936e-02,  1.1048e-02,  2.6398e-02,\n",
      "        -5.5842e-03,  3.4133e-04, -1.7368e-03,  7.7660e-04,  1.3734e-03,\n",
      "         9.4519e-04,  8.2423e-04,  1.5443e-02,  1.9696e-02, -4.6283e-03,\n",
      "         9.3884e-03,  2.5341e-03,  1.0199e-02,  2.1147e-03,  2.7360e-03,\n",
      "         8.9003e-03,  1.4718e-02,  8.3167e-04,  1.4644e-02,  1.3827e-02,\n",
      "        -4.4238e-03,  1.8611e-03,  6.7546e-03,  7.3624e-04,  4.0510e-03,\n",
      "         8.8614e-03, -9.3987e-04, -4.5683e-04,  1.0306e-02, -1.0082e-03,\n",
      "         1.1301e-03, -5.1635e-03,  7.0008e-04,  1.0838e-02, -4.9538e-03,\n",
      "         1.6529e-02, -9.5349e-03, -8.1158e-04,  8.4450e-04,  2.2744e-03,\n",
      "         7.9906e-03, -1.0237e-03, -3.6544e-03,  7.9897e-04, -5.0357e-03,\n",
      "         1.2617e-02,  2.9553e-02, -5.6488e-04,  1.2867e-02, -1.3715e-03,\n",
      "         1.7846e-03,  1.7102e-02, -6.7703e-03,  8.2637e-03, -2.6049e-03,\n",
      "         8.1133e-03,  1.3280e-02,  5.1652e-04, -3.8213e-03,  6.9380e-03,\n",
      "        -9.9944e-06, -2.8532e-03,  1.0096e-02,  2.7325e-03,  3.1989e-03,\n",
      "         8.8716e-03, -1.2342e-02, -7.7108e-03, -7.6089e-05,  2.7764e-03,\n",
      "         2.5117e-03, -1.0036e-03,  9.0982e-03,  1.0625e-02,  2.9129e-02,\n",
      "         1.3713e-03,  7.6257e-03,  6.2738e-03,  1.8151e-03, -4.1802e-03,\n",
      "        -1.1506e-04,  2.3757e-03,  1.4782e-02,  1.1659e-02,  5.2484e-03,\n",
      "         7.6126e-03,  1.3734e-03,  1.0359e-02, -2.6740e-03,  1.3178e-03,\n",
      "         2.7269e-03, -1.1555e-02, -5.1569e-03,  4.3623e-03, -3.3022e-03,\n",
      "         3.0880e-03, -2.2619e-03, -8.8545e-03,  4.9827e-03,  1.9580e-03,\n",
      "        -3.9624e-03,  4.5348e-03,  7.6566e-03, -3.1407e-03,  4.0202e-03,\n",
      "         9.4930e-04,  2.6968e-02,  3.4018e-03,  1.2464e-02,  4.0089e-03,\n",
      "         1.4676e-02,  2.1449e-03,  1.8898e-02,  6.7609e-03, -1.2319e-03,\n",
      "        -1.8980e-03, -1.7640e-03,  2.2843e-04,  2.2552e-02, -5.4070e-03,\n",
      "        -1.0727e-04, -1.0343e-02,  1.2843e-03,  2.1454e-03,  1.5111e-02,\n",
      "         1.4971e-04,  7.0931e-03,  2.9511e-02,  5.3099e-03,  8.6640e-03,\n",
      "         4.2189e-03, -1.1322e-03,  4.9722e-03,  7.5171e-03, -1.8843e-03,\n",
      "        -4.0812e-04, -7.6176e-03, -5.2873e-03,  1.0018e-02, -6.4204e-03,\n",
      "         1.1764e-03,  7.6672e-03,  5.3810e-03,  3.0914e-03,  9.9772e-03,\n",
      "         5.0530e-03,  1.5327e-02, -7.4796e-03,  4.9013e-03,  2.6197e-02,\n",
      "        -3.2452e-03, -4.3659e-03, -2.0394e-03,  1.1334e-02, -7.4645e-04,\n",
      "         1.0585e-03, -6.2541e-03,  1.7839e-03, -1.0725e-03, -3.0155e-03,\n",
      "        -1.0248e-03, -1.0620e-02,  2.7396e-02, -3.6524e-03,  1.0203e-02,\n",
      "         1.2113e-02,  1.3590e-03, -5.0465e-03,  4.8773e-03,  2.0148e-02,\n",
      "        -1.9414e-03,  1.1700e-02, -1.1250e-02,  2.1542e-02,  1.7705e-02,\n",
      "        -2.4761e-03,  6.1369e-03, -1.0477e-02,  2.8691e-02, -1.0644e-02,\n",
      "         1.0329e-03,  1.3230e-02,  1.0969e-02, -2.2881e-03,  1.3080e-03,\n",
      "         1.1045e-03,  8.8854e-03,  1.9716e-04,  8.8035e-03,  2.9197e-04,\n",
      "         1.3495e-02,  1.4609e-02,  2.8087e-03,  8.8138e-04, -2.2266e-03,\n",
      "         1.1557e-04, -2.3870e-03,  8.4098e-03,  6.7591e-03,  3.8046e-03,\n",
      "         4.0739e-03,  3.7035e-04, -7.8080e-04,  1.2396e-02,  1.6661e-02,\n",
      "         1.8045e-04,  2.9213e-03,  4.1457e-03,  2.5601e-03,  9.1011e-03,\n",
      "         3.0431e-03,  2.1980e-02,  5.8035e-04,  7.7631e-03,  6.1293e-03,\n",
      "         3.3073e-03,  1.1145e-02,  4.3380e-03,  1.6358e-02, -1.2421e-02,\n",
      "         1.1271e-02,  1.0328e-03,  2.4292e-02,  1.5753e-02, -4.0981e-03,\n",
      "         2.3135e-02], device='cuda:2', requires_grad=True)\n",
      "clf.l2.weight Parameter containing:\n",
      "tensor([[ 4.2965e-03,  6.0798e-04, -9.0565e-04,  ..., -1.6891e-03,\n",
      "          7.8209e-05, -1.1572e-03],\n",
      "        [ 2.3009e-03, -9.4394e-04, -3.3502e-03,  ...,  3.2168e-03,\n",
      "         -4.7184e-03, -8.1374e-04],\n",
      "        [-4.0656e-03,  1.1829e-03,  9.5954e-04,  ..., -5.5230e-03,\n",
      "          4.7459e-04, -1.4069e-02],\n",
      "        ...,\n",
      "        [ 6.9185e-04, -8.0920e-03,  6.9711e-03,  ..., -5.9359e-03,\n",
      "          8.5139e-03, -7.9627e-03],\n",
      "        [-3.2872e-03, -4.7071e-03,  2.6649e-04,  ...,  7.3735e-04,\n",
      "          9.9917e-04,  2.2702e-03],\n",
      "        [ 2.7850e-02,  4.6591e-03, -4.3217e-03,  ...,  2.7488e-03,\n",
      "         -3.9468e-03, -1.0860e-02]], device='cuda:2', requires_grad=True)\n",
      "clf.l2.bias Parameter containing:\n",
      "tensor([-5.6135e-03, -2.4775e-03,  3.7666e-03,  3.5573e-03,  5.4907e-03,\n",
      "        -1.3027e-02, -5.7855e-03,  4.6155e-03, -9.0300e-03, -1.4956e-03,\n",
      "         1.8492e-03,  3.7777e-03,  8.8117e-03,  7.1679e-03, -2.4149e-03,\n",
      "        -1.4188e-03,  1.4512e-03, -3.8477e-03, -4.2684e-03, -4.9121e-03,\n",
      "        -7.7667e-03, -4.1025e-04,  1.7253e-03, -3.6733e-04,  6.3470e-03,\n",
      "        -1.9642e-02, -1.1641e-02,  3.3866e-03, -5.3215e-03,  1.3078e-02,\n",
      "        -2.7836e-03,  8.6718e-03, -4.7459e-03, -2.0679e-03, -1.6767e-03,\n",
      "         7.5403e-04,  4.1529e-03,  2.0647e-02,  4.4639e-04, -4.0388e-03,\n",
      "         1.7097e-02,  3.6974e-03,  5.2687e-03,  1.0887e-03,  2.3914e-03,\n",
      "        -7.6586e-03,  1.9702e-03, -2.0376e-03, -8.0484e-04,  5.0489e-03,\n",
      "         1.0950e-02,  1.3123e-02,  4.3572e-03,  1.2318e-02,  1.7302e-03,\n",
      "         9.6022e-03, -3.7654e-03, -2.1963e-04, -1.4956e-03,  8.3731e-03,\n",
      "         1.1438e-02, -8.2606e-04, -4.7375e-03,  4.1318e-03,  4.9683e-03,\n",
      "         6.6949e-03, -8.8367e-03,  1.5283e-02,  1.4356e-02,  1.1738e-02,\n",
      "        -1.1132e-02,  5.1700e-03, -3.8331e-03, -1.0764e-02,  5.5070e-03,\n",
      "         6.6328e-03,  6.5925e-03,  3.1266e-03,  1.4305e-03,  1.8518e-03,\n",
      "        -2.6139e-04,  1.3282e-02,  1.5849e-02,  5.2935e-03,  2.0419e-02,\n",
      "         8.3022e-04, -3.6799e-03, -3.3788e-03, -5.7941e-03,  3.8340e-03,\n",
      "         1.0748e-03,  1.7984e-02,  4.4706e-03,  1.1208e-02,  5.1381e-03,\n",
      "         3.7638e-03,  2.1524e-03,  5.3900e-03,  4.7189e-03, -1.0364e-02,\n",
      "        -5.1352e-03,  1.1553e-03, -8.9020e-03, -4.3286e-03, -1.0750e-02,\n",
      "        -6.1165e-03, -6.3132e-04,  8.2280e-03,  8.9764e-04, -1.1828e-02,\n",
      "         3.4651e-03,  1.2769e-03,  7.8395e-03,  3.5046e-03, -4.7333e-05,\n",
      "         1.0277e-03, -2.0866e-03,  1.3915e-02,  3.6790e-03,  5.2276e-03,\n",
      "        -4.6844e-04, -4.5381e-03,  1.4719e-02,  6.1945e-03,  6.1808e-04,\n",
      "        -1.5436e-02, -1.2705e-03, -5.1604e-04,  1.2219e-02, -3.0737e-03,\n",
      "         1.8482e-03, -9.3404e-03,  3.9903e-04,  9.3169e-03, -9.4593e-03,\n",
      "         4.9698e-03, -4.2307e-03, -2.7258e-03, -4.8582e-03,  1.1713e-02,\n",
      "        -1.6599e-03,  4.8008e-03, -1.9132e-03, -1.7272e-03, -6.6121e-03,\n",
      "        -1.3410e-03, -6.9821e-03, -4.3211e-03,  1.2039e-03, -1.6229e-03,\n",
      "        -5.5397e-04, -1.1735e-02,  2.0487e-03,  7.8113e-04,  1.0272e-02,\n",
      "         2.5434e-03, -2.3989e-03,  2.3333e-03,  3.5300e-03,  2.3377e-03,\n",
      "         1.7199e-02,  5.9835e-03,  4.0740e-03, -2.6631e-03, -1.6027e-04,\n",
      "        -1.0263e-03, -1.8095e-03, -4.9312e-04, -8.5947e-03, -6.7667e-03,\n",
      "        -2.9420e-03,  7.7879e-03,  8.5732e-05,  1.6346e-02, -6.9649e-03,\n",
      "        -6.4080e-03,  1.8751e-02,  6.4051e-03, -4.5746e-03,  3.5270e-03,\n",
      "        -2.2649e-03, -5.1700e-03, -6.3740e-03, -1.2751e-02, -5.1785e-03,\n",
      "        -4.8238e-03,  1.3823e-02,  1.9819e-02,  1.5037e-04,  1.5723e-02,\n",
      "        -3.2161e-03,  5.4303e-04, -1.9661e-03,  1.1709e-02,  9.8101e-04,\n",
      "         1.3116e-02,  8.5598e-03, -3.9223e-03,  2.3416e-03,  2.5561e-03,\n",
      "        -3.6545e-03, -4.9902e-03, -6.1405e-03,  3.3153e-03,  1.2204e-04,\n",
      "        -2.2799e-02, -1.6014e-02,  1.2011e-03, -6.5678e-03,  4.2516e-03,\n",
      "         5.1835e-03, -1.2623e-03, -1.2527e-03,  1.0055e-02,  2.4802e-03,\n",
      "        -1.4226e-04, -2.6428e-03,  8.6848e-04, -6.7983e-03,  4.5448e-03,\n",
      "        -1.5350e-03,  5.0959e-03, -1.8893e-03,  5.6460e-04, -4.1387e-03,\n",
      "        -9.2047e-04,  3.7695e-03,  2.3089e-03,  6.6326e-03, -2.0978e-03,\n",
      "        -2.8362e-03,  7.2234e-04,  1.6468e-03,  5.3604e-03,  3.1334e-03,\n",
      "         3.6527e-03, -5.8334e-03,  1.0622e-03,  6.7267e-03,  7.2582e-03,\n",
      "         7.5924e-03,  1.7697e-03,  9.5563e-03, -1.0625e-02, -1.5179e-03,\n",
      "        -1.8451e-03,  4.7181e-03, -6.1290e-03, -6.2610e-04, -2.3846e-03,\n",
      "         1.5672e-02,  1.1465e-03, -7.9196e-03,  2.0755e-02, -6.4004e-03,\n",
      "         3.8673e-03,  2.8850e-03, -3.6548e-03, -3.2545e-04,  5.0090e-03,\n",
      "        -1.0788e-03, -5.2878e-03,  7.8120e-03, -3.4543e-03, -3.8834e-03,\n",
      "        -6.4013e-03, -1.0864e-03,  1.9908e-03, -4.6335e-03, -4.4871e-03,\n",
      "        -1.0841e-02,  5.5851e-04,  3.7319e-03,  1.1776e-02,  1.0010e-02,\n",
      "         1.5748e-03,  1.9064e-02,  3.4534e-03,  2.5870e-03, -7.1518e-03,\n",
      "         1.2667e-03, -3.7195e-03,  2.2341e-03,  3.6125e-03,  1.0512e-03,\n",
      "        -2.8095e-03,  4.8018e-03,  4.0257e-03], device='cuda:2',\n",
      "       requires_grad=True)\n",
      "clf.l3.weight Parameter containing:\n",
      "tensor([[-0.0627, -0.0278, -0.0696,  0.0298,  0.0528,  0.0566,  0.0507,  0.0372,\n",
      "          0.0250, -0.0320,  0.0139,  0.0604,  0.0700,  0.0583,  0.0376, -0.0932,\n",
      "          0.0007, -0.0014, -0.0069, -0.0064, -0.0260, -0.0476, -0.0098,  0.0053,\n",
      "          0.0219,  0.0589,  0.0551, -0.0574, -0.0817, -0.0656, -0.0056, -0.0582,\n",
      "          0.0425, -0.0179,  0.0628,  0.0233,  0.0176,  0.0570,  0.0037, -0.0598,\n",
      "          0.0456,  0.0475,  0.0161, -0.0108,  0.0125, -0.0886,  0.0089, -0.0945,\n",
      "         -0.0592, -0.0174, -0.0824, -0.0589, -0.0694, -0.0642, -0.0892,  0.0497,\n",
      "         -0.0219, -0.0109,  0.0117,  0.0529,  0.0406, -0.0131,  0.0026,  0.0134,\n",
      "          0.0470, -0.0753,  0.0478, -0.0788, -0.0890, -0.0542,  0.0399,  0.0132,\n",
      "         -0.0030,  0.0722,  0.0588, -0.0163,  0.0532, -0.0076,  0.0042,  0.0011,\n",
      "          0.0357,  0.0596,  0.0663, -0.0543,  0.0534,  0.0078, -0.0721,  0.0489,\n",
      "          0.0479, -0.0794, -0.0033, -0.0759,  0.0321, -0.0554, -0.0845, -0.0887,\n",
      "         -0.0158, -0.0149,  0.0092, -0.0733, -0.0931,  0.0138, -0.0710, -0.0012,\n",
      "         -0.0357, -0.0791,  0.0138,  0.0304, -0.0026, -0.0200,  0.0274,  0.0488,\n",
      "          0.0506,  0.0379, -0.0280,  0.0670, -0.0045,  0.0418,  0.0113, -0.0678,\n",
      "          0.0073, -0.0593,  0.0359,  0.0551, -0.0102, -0.0717,  0.0151,  0.0519,\n",
      "          0.0516, -0.0673, -0.0068,  0.0613,  0.0002, -0.0844, -0.0843,  0.0559,\n",
      "         -0.0729, -0.0240, -0.0037,  0.0527,  0.0288,  0.0188, -0.0063,  0.0471,\n",
      "          0.0475, -0.0083,  0.0213,  0.0526, -0.0089,  0.0550,  0.0015,  0.0543,\n",
      "          0.0239, -0.0012, -0.0911,  0.0050, -0.0816, -0.0153,  0.0611, -0.0077,\n",
      "         -0.0712, -0.0908,  0.0340,  0.0592,  0.0359,  0.0262,  0.0732,  0.0136,\n",
      "          0.0618,  0.0602,  0.0149,  0.0562,  0.0117, -0.0449, -0.0779, -0.0622,\n",
      "         -0.0885,  0.0279,  0.0037, -0.0118, -0.0054, -0.0421,  0.0438,  0.0614,\n",
      "         -0.0541, -0.0057,  0.0547, -0.0840,  0.0262, -0.0888,  0.0782, -0.0079,\n",
      "         -0.0126,  0.0463, -0.0121, -0.0852,  0.0452, -0.0110,  0.0024,  0.0307,\n",
      "         -0.0542, -0.0035, -0.0206, -0.0890,  0.0068, -0.0894, -0.0935, -0.0295,\n",
      "         -0.0914, -0.0150,  0.0515, -0.0207, -0.0016,  0.0593,  0.0169,  0.0102,\n",
      "          0.0703,  0.0206, -0.0330, -0.0517,  0.0610,  0.0254,  0.0141, -0.0213,\n",
      "         -0.0092,  0.0270,  0.0526,  0.0032,  0.0343, -0.0471,  0.0555, -0.0002,\n",
      "         -0.0031,  0.0594,  0.0546,  0.0623, -0.0065,  0.0036,  0.0613,  0.0518,\n",
      "          0.0599,  0.0153,  0.0564, -0.0739, -0.0036,  0.0334,  0.0584, -0.0891,\n",
      "         -0.0159,  0.0116, -0.0775,  0.0636, -0.0740, -0.0705, -0.0149,  0.0347,\n",
      "          0.0165, -0.0011,  0.0352,  0.0199, -0.0072, -0.0056,  0.0395,  0.0091,\n",
      "         -0.0817,  0.0667, -0.0975, -0.0035,  0.0051, -0.0129, -0.0364,  0.0085,\n",
      "         -0.0412,  0.0596,  0.0652,  0.0120,  0.0598,  0.0217, -0.0210, -0.0527,\n",
      "         -0.0455,  0.0662, -0.0084, -0.0717,  0.0084, -0.0382,  0.0344,  0.0454]],\n",
      "       device='cuda:2', requires_grad=True)\n",
      "clf.l3.bias Parameter containing:\n",
      "tensor([0.0506], device='cuda:2', requires_grad=True)\n",
      "clf.ln1.weight Parameter containing:\n",
      "tensor([0.4885, 0.0835, 0.0859, 0.2820, 0.1061, 0.0835, 0.2024, 0.2524, 0.1146,\n",
      "        0.0812, 0.2565, 0.0899, 0.0825, 0.0805, 0.0811, 0.0840, 0.0981, 0.1420,\n",
      "        0.0797, 0.1321, 0.1270, 0.3389, 0.1401, 0.0959, 0.1774, 0.2897, 0.1166,\n",
      "        0.1235, 0.2321, 0.0846, 0.2981, 0.0818, 0.1197, 0.0964, 0.1281, 0.2771,\n",
      "        0.2012, 0.0800, 0.1461, 0.1307, 0.2378, 0.0920, 0.1190, 0.0816, 0.2791,\n",
      "        0.1497, 0.3030, 0.1746, 0.1575, 0.1856, 0.0817, 0.1599, 0.1068, 0.0989,\n",
      "        0.0794, 0.1607, 0.2145, 0.2388, 0.2545, 0.1696, 0.0802, 0.0871, 0.1859,\n",
      "        0.1270, 0.1925, 0.1115, 0.2466, 0.1691, 0.0902, 0.0829, 0.2260, 0.1087,\n",
      "        0.1775, 0.2441, 0.1597, 0.0915, 0.1878, 0.0921, 0.0905, 0.0882, 0.0814,\n",
      "        0.1388, 0.1787, 0.0947, 0.1447, 0.1543, 0.2707, 0.1649, 0.1111, 0.0920,\n",
      "        0.0788, 0.0842, 0.0874, 0.0861, 0.1212, 0.2106, 0.0988, 0.2612, 0.2612,\n",
      "        0.2086, 0.1909, 0.3320, 0.2325, 0.1554, 0.1861, 0.0799, 0.1725, 0.0875,\n",
      "        0.0840, 0.0878, 0.1345, 0.0940, 0.0841, 0.1240, 0.1603, 0.1386, 0.0839,\n",
      "        0.1733, 0.2036, 0.0792, 0.1299, 0.0831, 0.1425, 0.3366, 0.0863, 0.1048,\n",
      "        0.1164, 0.0771, 0.1345, 0.0871, 0.2311, 0.1958, 0.0866, 0.3882, 0.3141,\n",
      "        0.0875, 0.0883, 0.1254, 0.1898, 0.1271, 0.1099, 0.0875, 0.0809, 0.1037,\n",
      "        0.0991, 0.1801, 0.0784, 0.1029, 0.0788, 0.0881, 0.1750, 0.1324, 0.1192,\n",
      "        0.0874, 0.0881, 0.1137, 0.0960, 0.0914, 0.1067, 0.1271, 0.0864, 0.0876,\n",
      "        0.1013, 0.3188, 0.2228, 0.0790, 0.2633, 0.1009, 0.1029, 0.0861, 0.0783,\n",
      "        0.0793, 0.0820, 0.1146, 0.2524, 0.0888, 0.0784, 0.0881, 0.0837, 0.2262,\n",
      "        0.0793, 0.1045, 0.3463, 0.0964, 0.1598, 0.3154, 0.0855, 0.2653, 0.0813,\n",
      "        0.1014, 0.0781, 0.1718, 0.3025, 0.2582, 0.0840, 0.1313, 0.2034, 0.0878,\n",
      "        0.0966, 0.0788, 0.2897, 0.1963, 0.0915, 0.2661, 0.2896, 0.2141, 0.1275,\n",
      "        0.2404, 0.1575, 0.0979, 0.1638, 0.0971, 0.1030, 0.0930, 0.2070, 0.2338,\n",
      "        0.1253, 0.1047, 0.0942, 0.0804, 0.1341, 0.1672, 0.0978, 0.2950, 0.1147,\n",
      "        0.1047, 0.0877, 0.0934, 0.2081, 0.1991, 0.3000, 0.1254, 0.2078, 0.0876,\n",
      "        0.1085, 0.1331, 0.0871, 0.2876, 0.3114, 0.1462, 0.0875, 0.1195, 0.0796,\n",
      "        0.0973, 0.0950, 0.1446, 0.0809, 0.1116, 0.1443, 0.1731, 0.2296, 0.0900,\n",
      "        0.0795, 0.1201, 0.1576, 0.1874, 0.1139, 0.1327, 0.2275, 0.0779, 0.0913,\n",
      "        0.2138, 0.1169, 0.0921, 0.0782, 0.3003, 0.0811, 0.2680, 0.0944, 0.0970,\n",
      "        0.0780, 0.1527, 0.2985, 0.0786, 0.1055, 0.2094, 0.0843, 0.0810, 0.2442,\n",
      "        0.0841, 0.1255, 0.1263, 0.0857, 0.2833, 0.1908, 0.0934, 0.2520, 0.1283,\n",
      "        0.3391, 0.0921, 0.0974, 0.1347, 0.0830, 0.1407, 0.1134, 0.1197, 0.1272,\n",
      "        0.1362, 0.0848, 0.0860, 0.2066, 0.2552, 0.1605, 0.1260, 0.0809, 0.0944,\n",
      "        0.1047, 0.2904, 0.2560, 0.1792, 0.2761, 0.2392, 0.0858, 0.0797, 0.0770,\n",
      "        0.2035, 0.1221, 0.0790, 0.1513, 0.0946, 0.0803, 0.1438, 0.1395, 0.3091,\n",
      "        0.0989, 0.1595, 0.1305, 0.2161, 0.2100, 0.0978, 0.0899, 0.1651, 0.0899,\n",
      "        0.2415, 0.1065, 0.0925, 0.0837, 0.0931, 0.1292, 0.0846, 0.1962, 0.0827,\n",
      "        0.1165, 0.0865, 0.2262, 0.0988, 0.1424, 0.1966, 0.1159, 0.3116, 0.0875,\n",
      "        0.0930, 0.0955, 0.0839, 0.0867, 0.1056, 0.1104, 0.2004, 0.2334, 0.1136,\n",
      "        0.1145, 0.0790, 0.1810, 0.1659, 0.0792, 0.0902, 0.1512, 0.2910, 0.1186,\n",
      "        0.2201, 0.2140, 0.0825, 0.0850, 0.0859, 0.1559, 0.2586, 0.0840, 0.0953,\n",
      "        0.1070, 0.1121, 0.0783, 0.0860, 0.0793, 0.2392, 0.1377, 0.2244, 0.0879,\n",
      "        0.0872, 0.0919, 0.0927, 0.0955, 0.0786, 0.0829, 0.0908, 0.0897, 0.1843,\n",
      "        0.3394, 0.0876, 0.2028, 0.0904, 0.1068, 0.1631, 0.1771, 0.2902, 0.1314,\n",
      "        0.1037, 0.2122, 0.0779, 0.0844, 0.1314, 0.1213, 0.2795, 0.3319, 0.0862,\n",
      "        0.0791, 0.1068, 0.1024, 0.1236, 0.0898, 0.0802, 0.0797, 0.0849, 0.1327,\n",
      "        0.1551, 0.3144, 0.1591, 0.1251, 0.0896, 0.0851, 0.0785, 0.0881, 0.0784,\n",
      "        0.2773, 0.1060, 0.1022, 0.1032, 0.0790, 0.1090, 0.0849, 0.0789, 0.0786,\n",
      "        0.1041, 0.0853, 0.1369, 0.1136, 0.0931, 0.0871, 0.2722, 0.1050, 0.0811,\n",
      "        0.0918, 0.0844, 0.1665, 0.0859, 0.2054, 0.0786, 0.2612, 0.0897, 0.1444,\n",
      "        0.0972, 0.1138, 0.0827, 0.2220, 0.0838, 0.0803, 0.0807, 0.0776, 0.3147,\n",
      "        0.2241, 0.1376, 0.0814, 0.0904, 0.0779, 0.0803, 0.1854, 0.0939, 0.2806,\n",
      "        0.2829, 0.1623, 0.1253, 0.0960, 0.0831, 0.1577, 0.1044, 0.1254, 0.0960,\n",
      "        0.0914, 0.0922, 0.1847, 0.0921, 0.0832, 0.0858, 0.1152, 0.2426, 0.1834,\n",
      "        0.2030, 0.1081, 0.0968, 0.1086, 0.1794, 0.0991, 0.1084, 0.0801, 0.3531,\n",
      "        0.0905, 0.0815, 0.1108, 0.0830, 0.0796, 0.0779, 0.0921, 0.0958, 0.3026,\n",
      "        0.0930, 0.2608, 0.1508, 0.1359, 0.1100, 0.1678, 0.2025, 0.0781, 0.2237,\n",
      "        0.1296, 0.2896, 0.2599, 0.0989, 0.0796, 0.1009, 0.3806, 0.1277, 0.1996,\n",
      "        0.1456, 0.2750, 0.0805, 0.0845, 0.0879, 0.0916, 0.0795, 0.1023, 0.1277,\n",
      "        0.2293, 0.3937, 0.0781, 0.0793, 0.1006, 0.1167, 0.0800, 0.2538, 0.0868,\n",
      "        0.0954, 0.0869, 0.0785, 0.0793, 0.1468, 0.2246, 0.0780, 0.0910, 0.1482,\n",
      "        0.3032, 0.1668, 0.0886, 0.2910, 0.1035, 0.3042, 0.0785, 0.1085, 0.1987,\n",
      "        0.0890, 0.2320, 0.1380, 0.2244, 0.0838, 0.2342, 0.1342, 0.0814, 0.3489],\n",
      "       device='cuda:2', requires_grad=True)\n",
      "clf.ln1.bias Parameter containing:\n",
      "tensor([-1.7169e-03, -6.4024e-03, -4.0817e-03, -2.8322e-03,  3.0609e-03,\n",
      "         3.2516e-03, -1.0613e-03,  3.8960e-03,  2.5319e-03,  2.4460e-03,\n",
      "         6.5901e-03, -5.4361e-03,  2.6698e-03, -6.1033e-03, -1.8769e-03,\n",
      "         6.0750e-03,  3.7635e-03, -3.1617e-03, -3.6937e-03, -7.2553e-03,\n",
      "         4.7570e-03, -4.8233e-05, -6.2589e-03, -9.6564e-04,  1.5505e-03,\n",
      "        -2.5191e-03,  5.9887e-03, -2.7519e-03,  7.1440e-03, -3.0155e-03,\n",
      "         6.6367e-03, -6.6881e-05,  4.1761e-03, -1.8205e-03,  4.6875e-03,\n",
      "        -1.1505e-03,  4.3006e-03,  1.8032e-03,  2.5827e-03,  1.3467e-03,\n",
      "         6.5443e-03, -3.5968e-03,  5.2514e-03, -4.7872e-03,  4.7710e-03,\n",
      "         8.7511e-03, -3.8376e-03, -1.6151e-03,  2.5726e-03,  9.3679e-03,\n",
      "         4.2489e-03, -2.5855e-03, -1.4595e-03,  2.9561e-03, -1.0671e-03,\n",
      "         6.2592e-03,  1.6030e-03,  7.1540e-03,  1.1825e-03,  2.3114e-03,\n",
      "        -6.8138e-03, -1.2981e-03, -5.2673e-03,  5.1305e-04,  3.7510e-03,\n",
      "         5.6093e-03,  1.9092e-04, -6.9248e-04,  3.6319e-03,  3.5979e-03,\n",
      "        -2.5885e-03, -7.3299e-03,  3.2451e-03,  5.5232e-03,  7.3445e-03,\n",
      "         3.6901e-03,  2.4866e-03,  3.8887e-03,  3.3705e-03, -1.6095e-03,\n",
      "        -2.1066e-03,  6.6740e-03,  3.2104e-03, -7.2744e-04,  3.6151e-03,\n",
      "         3.0407e-03, -4.9732e-03, -5.1959e-03,  3.5279e-03,  2.6547e-03,\n",
      "        -4.0316e-03,  2.9055e-03,  1.8915e-03, -7.1677e-03, -8.4716e-05,\n",
      "         9.9647e-04, -2.2603e-03,  7.2356e-03, -2.5841e-03,  3.7609e-03,\n",
      "         1.2996e-03, -6.0478e-06,  7.6986e-03,  3.9677e-03,  8.7882e-03,\n",
      "        -5.2402e-03, -5.1996e-03,  9.0843e-04, -1.0661e-05, -1.2774e-03,\n",
      "         6.6963e-03, -1.1735e-03,  4.5238e-03, -5.4784e-03, -3.6891e-03,\n",
      "        -9.8194e-04, -4.9928e-04,  1.7495e-03,  7.2891e-03, -2.9155e-03,\n",
      "         2.5630e-03,  9.1983e-04,  3.2941e-03, -1.3372e-03,  1.6226e-03,\n",
      "         3.4274e-03,  5.3412e-04, -2.1217e-03, -1.0208e-03, -5.1439e-03,\n",
      "         2.1468e-03,  2.4617e-03, -4.6370e-03,  1.8262e-03, -1.2361e-03,\n",
      "        -5.6781e-04,  2.2630e-03, -4.1486e-03,  9.3931e-03,  3.4902e-03,\n",
      "        -4.2583e-03, -3.9366e-03, -6.9751e-03,  4.2000e-03,  4.2952e-03,\n",
      "         3.1982e-03,  2.1038e-03,  4.2516e-03,  4.2748e-03,  2.2211e-03,\n",
      "         2.5939e-03, -5.1954e-03,  2.6547e-03, -8.8790e-05, -9.6481e-04,\n",
      "        -6.7785e-03,  5.6276e-04, -4.6442e-03,  4.2841e-03,  3.3749e-04,\n",
      "        -4.4963e-03,  5.3245e-05, -2.6936e-04,  2.6724e-03,  4.5386e-03,\n",
      "         4.3848e-03,  8.9149e-03, -6.7733e-03,  1.3215e-03,  3.2201e-03,\n",
      "         3.4018e-03,  3.5517e-03,  2.5417e-03, -4.6048e-03,  3.2198e-03,\n",
      "        -8.8734e-04,  1.6324e-03, -6.0494e-03, -3.6494e-03,  2.8981e-03,\n",
      "         6.7651e-05, -3.2245e-03,  5.2721e-03, -1.7140e-03,  3.1385e-03,\n",
      "         4.8572e-03, -4.7048e-03,  4.1077e-03, -9.0880e-04, -4.4186e-03,\n",
      "        -1.5650e-03,  5.2427e-03,  4.9873e-03,  7.5581e-03, -4.4017e-03,\n",
      "         2.2968e-03, -4.5359e-03,  1.0032e-03, -4.9931e-03, -2.4281e-03,\n",
      "         2.7529e-03,  3.0492e-03,  3.0366e-03,  8.0719e-03,  3.5544e-03,\n",
      "         7.7903e-03, -3.7275e-03,  3.7900e-03,  2.5442e-03,  5.3166e-03,\n",
      "        -1.9836e-03,  4.8077e-03, -5.5137e-03,  3.6882e-03,  3.8475e-03,\n",
      "         1.6942e-03, -3.0567e-03, -2.7887e-03, -1.7471e-03,  4.2166e-03,\n",
      "        -4.0746e-03,  7.6173e-04,  5.3100e-05,  4.2503e-03, -8.4897e-04,\n",
      "         6.5718e-03,  2.5249e-03, -5.9585e-03, -1.0726e-03, -1.1319e-03,\n",
      "         9.4659e-04, -3.7647e-03,  6.9439e-03, -4.0191e-03,  6.1331e-03,\n",
      "         4.7025e-03,  2.0306e-03,  4.5050e-03,  4.4499e-03,  3.9740e-03,\n",
      "        -3.4693e-03,  5.5257e-03, -5.3991e-04, -5.7186e-03,  7.0201e-05,\n",
      "         4.2586e-03,  4.3470e-03, -1.2227e-03, -4.8707e-03,  6.4680e-03,\n",
      "         6.6786e-03,  2.4905e-03,  3.8261e-04, -5.3732e-03, -4.0731e-03,\n",
      "        -1.8837e-04,  3.2447e-05, -1.2291e-03,  4.1231e-03, -2.1154e-03,\n",
      "        -6.5319e-03,  3.5192e-03,  3.3003e-03,  1.6539e-03, -1.0008e-03,\n",
      "         2.0766e-03,  3.9354e-03,  1.9069e-03,  2.7128e-03,  2.8479e-03,\n",
      "        -1.2695e-03,  4.1526e-03,  1.0440e-03,  6.8113e-04, -3.1886e-03,\n",
      "        -5.3649e-04,  1.1478e-03,  3.9880e-05,  8.8157e-03, -4.4572e-03,\n",
      "        -6.9641e-04, -3.9386e-03,  5.4461e-03,  5.8342e-03,  1.0130e-03,\n",
      "         7.6428e-04,  4.4386e-03,  2.5995e-03,  1.2913e-03,  5.0453e-03,\n",
      "         4.9594e-03, -4.4249e-03,  3.9027e-03,  4.8459e-03,  5.4668e-04,\n",
      "         3.1372e-03,  7.1809e-03,  3.7587e-03, -5.8207e-04, -1.0494e-03,\n",
      "         6.5072e-03,  3.1304e-03,  5.1495e-03, -2.6987e-03,  4.6983e-03,\n",
      "        -5.7414e-03,  3.5624e-03, -1.8211e-03,  2.1574e-03,  5.0386e-03,\n",
      "         4.2335e-03,  6.3784e-03,  4.8858e-03, -8.8545e-04,  2.4251e-03,\n",
      "         5.6907e-03,  4.5334e-03, -3.8192e-03, -9.6119e-04, -3.0815e-03,\n",
      "        -2.6393e-03,  2.9800e-03,  2.5510e-03,  7.3937e-03,  2.3042e-03,\n",
      "         3.8165e-03, -1.4908e-03, -1.6872e-04,  2.9918e-03, -2.7639e-03,\n",
      "        -1.6390e-03,  7.7472e-03,  1.4382e-04,  4.4371e-04, -3.2413e-03,\n",
      "        -1.3548e-03,  3.5655e-03, -3.4944e-03, -6.6127e-04, -1.3404e-03,\n",
      "         4.1042e-03,  5.9579e-03, -4.2710e-03, -3.1435e-03,  3.0984e-03,\n",
      "         3.1176e-03,  3.3186e-03,  1.9791e-03, -4.8484e-04,  3.0187e-03,\n",
      "        -5.8445e-03, -3.8346e-03, -2.4358e-03, -1.5640e-03, -4.3997e-03,\n",
      "         1.8110e-03,  1.2584e-03, -3.4799e-03,  2.3630e-03, -3.4408e-03,\n",
      "         1.3044e-02,  3.4573e-03, -1.0708e-03,  5.2649e-03, -4.3005e-04,\n",
      "         2.7071e-03,  9.0830e-04,  1.0327e-03,  9.0725e-06,  1.0959e-03,\n",
      "         2.4268e-03, -1.5082e-03,  3.5193e-03, -7.7678e-04,  7.0963e-03,\n",
      "        -9.5247e-04, -2.7094e-03, -5.6585e-03,  2.4441e-03,  1.9781e-04,\n",
      "        -1.5814e-03, -7.4467e-04, -3.4565e-03,  1.2991e-02,  2.8941e-03,\n",
      "         4.8483e-04, -6.5297e-03,  2.9045e-03,  1.1490e-05, -1.5250e-03,\n",
      "         5.3740e-03, -4.5160e-03, -3.1390e-03,  2.5321e-03, -4.8296e-03,\n",
      "         3.1942e-03,  3.2589e-03, -4.5805e-03,  1.0853e-03, -6.0256e-04,\n",
      "         9.3623e-04, -5.7330e-04,  3.1469e-03,  1.1394e-03, -1.0681e-02,\n",
      "         5.1593e-03,  5.1299e-03,  4.1923e-04, -5.0436e-03, -1.2714e-03,\n",
      "        -4.5662e-03,  3.7948e-03,  5.1192e-03,  4.3405e-03,  1.5547e-03,\n",
      "         8.0495e-04, -5.4770e-03, -4.3682e-03,  1.2135e-04,  3.1228e-03,\n",
      "         4.3030e-03, -3.1620e-03,  2.2579e-03, -5.1392e-03,  7.3365e-03,\n",
      "         3.1072e-03,  1.6420e-03,  3.2801e-03, -3.6090e-03, -3.8053e-03,\n",
      "        -1.0366e-03,  3.6422e-03,  7.0699e-03,  6.3525e-03,  4.7306e-03,\n",
      "         6.0049e-03, -6.4592e-04,  4.0021e-03, -3.1342e-03, -2.0982e-03,\n",
      "         4.4884e-03, -1.2862e-03, -5.0746e-03,  7.1182e-03, -4.2060e-03,\n",
      "         3.5041e-03,  2.7183e-03,  4.4076e-03,  1.3522e-03,  2.4042e-03,\n",
      "        -3.8380e-03,  2.8803e-03, -3.7142e-04, -4.4956e-03, -5.9256e-04,\n",
      "        -1.0589e-03,  6.0239e-03,  5.6488e-04,  2.3930e-03,  4.0433e-03,\n",
      "         4.2737e-03,  1.3197e-03,  2.2189e-03,  6.3790e-03, -5.9200e-03,\n",
      "        -4.8676e-03, -6.4263e-03,  3.3313e-03,  6.9326e-04, -1.1633e-03,\n",
      "        -5.4137e-04, -8.2302e-03,  2.3488e-03,  1.7001e-04,  5.1193e-03,\n",
      "        -2.3022e-03, -7.6225e-05,  4.5208e-03,  9.5531e-04,  2.5367e-03,\n",
      "        -1.0990e-03, -4.8384e-03,  5.1225e-04,  3.1121e-03, -7.0463e-03,\n",
      "        -2.8990e-03, -3.7989e-03, -5.1725e-03,  3.5360e-03, -2.7052e-03,\n",
      "         4.9766e-03,  4.2025e-03,  4.0796e-03,  5.3245e-03,  2.5167e-03,\n",
      "         3.8535e-03,  7.1950e-03, -5.4333e-03,  2.0426e-03,  3.9677e-03,\n",
      "        -2.2897e-03, -4.9444e-03, -2.2634e-03,  6.0194e-03,  2.8302e-03,\n",
      "         6.3532e-04, -1.2417e-03,  4.9555e-04, -4.2937e-03, -5.2242e-03,\n",
      "        -1.8925e-03, -4.3446e-03,  3.9666e-04, -3.2465e-03,  1.5074e-03,\n",
      "         1.9837e-03, -2.7517e-03, -2.3451e-03, -4.3494e-03,  6.8762e-03,\n",
      "        -4.5205e-03,  6.0533e-03, -6.8286e-03,  8.1998e-03, -1.4872e-03,\n",
      "        -5.4506e-03,  3.6701e-03, -6.1596e-03,  1.5351e-03, -4.6115e-03,\n",
      "         2.8454e-03,  2.8644e-03,  9.9247e-04, -6.3254e-03,  1.4656e-03,\n",
      "        -1.6860e-03,  3.7626e-03, -4.1496e-04, -8.4843e-05, -3.2065e-03,\n",
      "         7.8444e-03, -3.4443e-04,  2.8216e-03,  3.0704e-04,  2.1233e-03,\n",
      "        -3.8907e-03, -3.5005e-03, -2.9644e-03,  2.5672e-03,  1.0615e-03,\n",
      "         3.7628e-03, -5.2772e-04, -1.9884e-03,  8.6778e-05,  4.4170e-03,\n",
      "        -1.9148e-03,  4.6962e-03, -3.5299e-03,  3.6498e-03, -2.0560e-03,\n",
      "         2.1988e-03, -1.7963e-03, -3.8985e-03,  6.3822e-03,  5.5111e-03,\n",
      "         8.5996e-04,  7.6556e-04,  3.1649e-03,  3.7718e-03, -5.1426e-03,\n",
      "        -2.0823e-03,  1.9926e-03,  3.6202e-03, -3.0058e-03, -2.9444e-03,\n",
      "         4.6843e-03], device='cuda:2', requires_grad=True)\n",
      "clf.ln2.weight Parameter containing:\n",
      "tensor([0.2796, 0.0954, 0.4265, 0.1609, 0.3313, 0.4390, 0.2942, 0.2715, 0.1213,\n",
      "        0.1891, 0.1178, 0.3941, 0.5056, 0.3772, 0.1263, 0.3743, 0.0734, 0.0717,\n",
      "        0.0783, 0.0749, 0.0964, 0.2928, 0.1136, 0.0768, 0.1447, 0.3933, 0.3830,\n",
      "        0.3155, 0.3445, 0.3710, 0.0796, 0.3368, 0.2142, 0.1085, 0.3591, 0.1859,\n",
      "        0.1571, 0.5300, 0.0776, 0.3970, 0.3845, 0.4430, 0.1633, 0.0978, 0.1248,\n",
      "        0.3566, 0.0932, 0.3729, 0.2815, 0.1733, 0.4386, 0.3913, 0.2762, 0.3380,\n",
      "        0.4702, 0.3958, 0.0947, 0.1041, 0.1019, 0.3219, 0.3020, 0.1112, 0.0717,\n",
      "        0.1189, 0.3092, 0.4183, 0.3527, 0.5100, 0.3606, 0.3081, 0.2157, 0.1388,\n",
      "        0.0749, 0.4343, 0.4745, 0.1865, 0.3904, 0.1216, 0.0828, 0.0791, 0.2571,\n",
      "        0.3819, 0.5330, 0.2543, 0.4532, 0.0803, 0.3566, 0.2743, 0.3738, 0.3410,\n",
      "        0.0810, 0.5131, 0.1525, 0.3166, 0.4280, 0.3783, 0.1495, 0.1600, 0.0946,\n",
      "        0.4698, 0.4513, 0.0926, 0.3524, 0.0723, 0.1333, 0.3168, 0.0842, 0.1972,\n",
      "        0.0896, 0.0841, 0.1233, 0.2914, 0.5222, 0.2194, 0.1807, 0.4916, 0.0872,\n",
      "        0.2586, 0.1082, 0.3350, 0.0832, 0.3584, 0.3740, 0.4248, 0.0868, 0.3411,\n",
      "        0.0967, 0.3199, 0.4523, 0.3608, 0.1061, 0.5352, 0.0742, 0.4125, 0.4094,\n",
      "        0.4598, 0.3994, 0.1154, 0.0752, 0.3332, 0.1283, 0.1459, 0.0968, 0.2157,\n",
      "        0.3495, 0.1055, 0.1095, 0.3881, 0.1229, 0.4325, 0.0712, 0.4474, 0.1243,\n",
      "        0.0710, 0.4843, 0.0800, 0.4143, 0.1442, 0.3430, 0.0994, 0.4646, 0.4237,\n",
      "        0.2148, 0.3885, 0.1482, 0.1796, 0.4566, 0.0874, 0.4727, 0.4663, 0.0759,\n",
      "        0.4996, 0.0917, 0.3147, 0.4274, 0.4060, 0.3569, 0.2084, 0.0804, 0.1217,\n",
      "        0.0790, 0.1466, 0.2699, 0.4170, 0.3039, 0.0777, 0.3582, 0.4439, 0.1322,\n",
      "        0.3979, 0.4504, 0.1163, 0.0853, 0.2506, 0.1390, 0.4727, 0.3209, 0.0801,\n",
      "        0.0748, 0.1290, 0.2817, 0.0735, 0.1027, 0.4545, 0.0828, 0.4346, 0.3820,\n",
      "        0.1778, 0.4665, 0.1596, 0.2425, 0.0981, 0.0735, 0.3606, 0.1045, 0.0822,\n",
      "        0.3573, 0.1226, 0.1432, 0.2540, 0.3489, 0.1628, 0.0968, 0.1333, 0.0766,\n",
      "        0.1218, 0.4390, 0.0801, 0.1938, 0.1913, 0.3241, 0.0703, 0.0825, 0.3507,\n",
      "        0.3407, 0.2676, 0.0745, 0.0838, 0.2710, 0.2050, 0.4977, 0.1206, 0.4114,\n",
      "        0.5316, 0.0782, 0.1519, 0.3468, 0.4257, 0.1165, 0.0803, 0.4601, 0.4003,\n",
      "        0.4361, 0.5037, 0.0938, 0.2549, 0.1053, 0.0733, 0.2155, 0.1454, 0.0845,\n",
      "        0.0813, 0.2084, 0.0767, 0.3273, 0.4133, 0.3741, 0.0809, 0.0724, 0.0776,\n",
      "        0.1290, 0.0898, 0.2151, 0.4919, 0.3290, 0.1039, 0.4182, 0.1239, 0.1496,\n",
      "        0.2020, 0.1776, 0.4464, 0.0955, 0.4811, 0.0907, 0.1728, 0.1848, 0.3779],\n",
      "       device='cuda:2', requires_grad=True)\n",
      "clf.ln2.bias Parameter containing:\n",
      "tensor([ 8.6169e-03,  2.9158e-04,  1.3521e-02,  6.4053e-03,  1.0579e-02,\n",
      "         5.8642e-03,  4.4049e-03,  6.8116e-03, -2.4615e-03,  2.7833e-03,\n",
      "         3.0098e-03,  1.4066e-02,  7.0938e-03,  7.3184e-03,  5.2483e-03,\n",
      "         2.3079e-02, -1.6376e-03, -1.3197e-04,  3.2767e-03, -4.6463e-04,\n",
      "         3.5050e-03,  7.5110e-03, -7.2622e-05,  2.4044e-03,  1.2425e-02,\n",
      "         2.9051e-03,  6.0351e-03,  8.7719e-03,  1.8175e-02,  1.4225e-02,\n",
      "        -1.1901e-03,  1.6341e-02,  3.8712e-03,  1.8694e-04,  4.7292e-03,\n",
      "         2.9701e-03,  2.2112e-03,  8.7568e-03, -1.1613e-04,  1.2618e-02,\n",
      "         1.0165e-02,  5.5450e-03, -8.9889e-04, -9.3869e-04,  1.1939e-03,\n",
      "         1.5697e-02, -1.7809e-03,  2.6342e-02,  8.7329e-03,  1.5549e-03,\n",
      "         1.6926e-02,  1.4787e-02,  1.8020e-02,  2.1513e-02,  1.9330e-02,\n",
      "         6.0448e-03,  3.8052e-04,  2.2868e-05,  3.0641e-03,  1.2943e-02,\n",
      "         8.8392e-03,  7.1072e-04,  7.1078e-04,  9.8074e-04,  8.2485e-03,\n",
      "         1.7356e-02,  7.7785e-03,  1.0131e-02,  2.8876e-02,  9.2524e-03,\n",
      "         3.3684e-04,  5.9845e-03,  2.2527e-03,  9.7945e-03,  7.9807e-03,\n",
      "         4.6242e-04,  1.2643e-02, -7.1027e-04,  1.5094e-03, -2.5191e-03,\n",
      "         6.5175e-03,  1.8476e-02,  9.0412e-03,  1.4186e-02,  7.4216e-03,\n",
      "        -2.7297e-03,  1.0920e-02,  1.0229e-02,  8.2256e-04,  1.7135e-02,\n",
      "         6.2551e-05,  6.9766e-03,  7.7284e-03,  1.8904e-02,  2.2586e-02,\n",
      "         2.7122e-02, -2.0892e-04, -2.3299e-03, -1.4796e-04,  1.0291e-02,\n",
      "         1.7326e-02,  1.2612e-03,  1.9452e-02, -7.7772e-04, -5.8827e-04,\n",
      "         1.4405e-02, -8.3631e-04,  1.2823e-02, -2.4092e-03, -6.1532e-03,\n",
      "         5.0125e-03,  8.3509e-03,  3.9538e-03,  7.5956e-03,  3.5327e-03,\n",
      "         9.2519e-03,  2.0568e-04,  1.1161e-02,  1.6302e-03,  1.0405e-02,\n",
      "         9.7594e-05,  1.0746e-02,  2.9203e-03,  7.6575e-03,  3.4200e-04,\n",
      "         1.1090e-02,  7.7512e-04,  3.5358e-03,  1.1309e-02,  1.4584e-02,\n",
      "        -8.5288e-04,  6.5821e-03, -5.9907e-04,  2.5361e-02,  1.7141e-02,\n",
      "         9.5432e-03,  1.4009e-02, -1.0661e-03,  3.0729e-03,  1.2107e-02,\n",
      "         4.0797e-03,  2.9660e-03, -2.0776e-04,  9.7255e-03,  1.0893e-02,\n",
      "        -3.6015e-03, -1.6041e-03,  1.7171e-03,  1.0903e-04,  1.1590e-02,\n",
      "        -1.1382e-03,  4.8394e-03,  1.3899e-03, -1.5115e-03,  1.2071e-02,\n",
      "        -2.4485e-03,  2.1414e-02,  3.6034e-03,  9.2700e-03,  1.0866e-03,\n",
      "         9.8533e-03,  1.4794e-02,  9.0513e-03,  1.1358e-02,  5.4163e-03,\n",
      "         8.0557e-04,  1.2088e-02,  3.3856e-04,  2.3726e-03,  4.6063e-03,\n",
      "         2.3272e-03,  8.8741e-03, -3.0413e-03,  1.2576e-02,  1.0176e-02,\n",
      "         8.5202e-03,  3.0240e-02,  7.2848e-03, -1.4583e-03,  5.6529e-04,\n",
      "         1.3586e-03,  6.2066e-03,  3.4451e-03,  6.4599e-03,  1.3158e-02,\n",
      "         3.8068e-03,  1.4265e-02,  2.0656e-02,  1.8939e-03,  1.9455e-02,\n",
      "         1.5566e-02, -5.0603e-04, -3.1872e-04,  1.1708e-02,  3.2021e-04,\n",
      "         1.4296e-02,  1.2144e-02,  2.5228e-04, -1.0568e-03,  3.8168e-03,\n",
      "         8.5489e-03, -1.6990e-03, -1.0559e-03,  1.3639e-02, -8.5688e-04,\n",
      "         9.2560e-03,  1.7919e-02,  6.0917e-03,  1.7188e-02, -2.4522e-03,\n",
      "         1.1667e-02, -5.1120e-03, -3.2488e-04,  9.0676e-03,  2.4016e-03,\n",
      "        -4.6788e-04,  1.2370e-02,  3.5706e-03,  2.3372e-03,  1.1298e-02,\n",
      "         1.3010e-02,  4.1725e-03, -1.0658e-03,  6.4803e-04,  2.0543e-03,\n",
      "         6.9118e-04,  5.4502e-03, -9.5041e-04, -5.7196e-03,  7.5998e-03,\n",
      "         1.0888e-02, -1.9110e-03,  2.8987e-03,  5.6721e-03,  8.4877e-03,\n",
      "         1.1040e-02,  1.7315e-04,  2.7616e-04,  1.1907e-02,  1.5554e-02,\n",
      "         3.8336e-03,  5.1500e-04,  1.3497e-02,  9.1580e-03,  3.4520e-03,\n",
      "         4.0590e-03,  1.6146e-02,  1.8940e-02, -1.0306e-03,  1.0368e-03,\n",
      "         1.6371e-02,  7.7471e-03,  1.2469e-02,  1.2390e-02,  3.4519e-03,\n",
      "         2.1271e-03,  1.8929e-03, -3.8818e-05,  8.1168e-03,  4.8030e-03,\n",
      "         1.6059e-03,  1.2390e-03,  9.9383e-03,  9.3789e-05,  2.1970e-02,\n",
      "         1.2487e-02,  1.8115e-02,  1.4796e-03, -1.0992e-03, -8.6139e-04,\n",
      "        -3.5361e-03,  1.1325e-03,  1.2623e-02,  9.9336e-03,  1.2327e-02,\n",
      "         3.6923e-03,  1.2138e-02,  4.3567e-03, -5.2354e-05,  5.5097e-03,\n",
      "         5.2499e-03,  7.1194e-03, -3.0409e-03,  1.6723e-02, -4.5372e-04,\n",
      "        -1.2470e-03,  1.0979e-02,  1.0922e-02], device='cuda:2',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i, j in model.named_parameters():\n",
    "    if j.requires_grad:\n",
    "        print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def listMLE(y_pred, y_true, eps=1e-8, padded_value_indicator=0):\n",
    "    \"\"\"\n",
    "    ListMLE loss introduced in \"Listwise Approach to Learning to Rank - Theory and Algorithm\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param eps: epsilon value, used for numerical stability\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    # shuffle for randomised tie resolution\n",
    "    random_indices = torch.randperm(y_pred.shape[-1])\n",
    "    y_pred_shuffled = y_pred[:, random_indices]\n",
    "    y_true_shuffled = y_true[:, random_indices]\n",
    "\n",
    "    y_true_sorted, indices = y_true_shuffled.sort(descending=True, dim=-1)\n",
    "\n",
    "    mask = y_true_sorted == padded_value_indicator\n",
    "\n",
    "    preds_sorted_by_true = torch.gather(y_pred_shuffled, dim=1, index=indices)\n",
    "    preds_sorted_by_true[mask] = float(\"-inf\")\n",
    "\n",
    "    max_pred_values, _ = preds_sorted_by_true.max(dim=1, keepdim=True)\n",
    "\n",
    "    preds_sorted_by_true_minus_max = preds_sorted_by_true - max_pred_values\n",
    "\n",
    "    cumsums = torch.cumsum(preds_sorted_by_true_minus_max.exp().flip(dims=[1]), dim=1).flip(dims=[1])\n",
    "\n",
    "    observation_loss = torch.log(cumsums + eps) - preds_sorted_by_true_minus_max\n",
    "\n",
    "    observation_loss[mask] = 0.0\n",
    "\n",
    "    return torch.mean(torch.sum(observation_loss, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.softmax(out)\n",
    "        return out\n",
    "model = NN(256, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Linearcls(256, p0=0.2, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3083)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1]).unsqueeze(0)\n",
    "b = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5]).unsqueeze(0)\n",
    "listMLE(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(85, 20, 256) \n",
    "b = torch.tensor(range(85)).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cri = models.ListMLE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(291.3251), tensor(295.7650))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([ 3.5803e-04,  2.1979e-03,  3.8311e-04,  5.2792e-04,  4.8840e-04,\n",
    "         1.0956e-03,  4.0375e-04,  2.1282e-03, -1.0719e-03,  2.1690e-03,\n",
    "         3.5637e-04,  7.8660e-04, -1.7349e-04,  1.1136e-03,  1.4135e-03,\n",
    "        -5.7234e-05, -1.9069e-04,  1.5373e-03, -1.5336e-04,  5.0368e-04,\n",
    "         1.0354e-03,  9.4463e-04,  1.5163e-03,  1.9483e-03,  6.8653e-04,\n",
    "         1.1449e-03,  1.4879e-04,  3.3806e-04,  1.5077e-03,  9.2047e-04,\n",
    "         2.3447e-03,  2.2386e-03,  1.4788e-03,  1.7258e-03, -2.6664e-04,\n",
    "        -1.5059e-04,  6.0289e-04, -6.1682e-04,  7.1805e-04,  1.0268e-03,\n",
    "         2.7502e-03, -7.3034e-04, -8.2444e-05, -7.1462e-04,  1.1572e-03,\n",
    "         7.2149e-04,  6.2846e-04,  1.6673e-03,  1.2135e-03,  1.4576e-03,\n",
    "         1.4565e-03,  1.9022e-03, -4.4323e-04,  8.4509e-04,  8.5283e-04,\n",
    "         3.7751e-04,  1.6441e-03,  7.9547e-04, -1.8599e-04,  2.2232e-04,\n",
    "         9.0810e-04,  1.7102e-03,  5.1223e-04,  7.4762e-05, -5.0921e-04,\n",
    "         2.4580e-03,  1.3834e-03,  1.8816e-04,  1.7089e-03,  1.1783e-03,\n",
    "         1.0902e-03,  9.6748e-06,  7.7385e-04,  2.1562e-03,  8.9369e-04,\n",
    "         1.0644e-03,  4.2991e-05,  1.5249e-03,  2.2387e-03, -1.2712e-03,\n",
    "         3.6258e-04, -3.7134e-04,  2.8287e-04,  2.0146e-03,  2.1470e-03])\n",
    "listMLE(t.unsqueeze(0), torch.tensor(range(len(t))).unsqueeze(0)), cri(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(298.6251, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cri(model(a).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295.49237060546875\n",
      "275.42333984375\n",
      "241.91757202148438\n",
      "204.74676513671875\n",
      "191.94525146484375\n",
      "182.60096740722656\n",
      "167.7770538330078\n",
      "175.0205841064453\n",
      "163.2277069091797\n",
      "177.66397094726562\n",
      "150.88702392578125\n",
      "172.1427764892578\n",
      "155.294677734375\n",
      "151.4325408935547\n",
      "163.6064453125\n",
      "142.91651916503906\n",
      "166.66835021972656\n",
      "137.78041076660156\n",
      "149.24563598632812\n",
      "132.16134643554688\n",
      "148.91616821289062\n",
      "149.13629150390625\n",
      "142.8118896484375\n",
      "131.6849822998047\n",
      "151.8173828125\n",
      "141.29368591308594\n",
      "131.7283172607422\n",
      "153.69264221191406\n",
      "140.82652282714844\n",
      "147.15045166015625\n",
      "128.57325744628906\n",
      "147.77639770507812\n",
      "139.59857177734375\n",
      "143.10546875\n",
      "133.0745849609375\n",
      "132.4609832763672\n",
      "145.72999572753906\n",
      "151.0432891845703\n",
      "142.7871551513672\n",
      "134.4236297607422\n",
      "161.66993713378906\n",
      "158.68896484375\n",
      "129.1192626953125\n",
      "142.7677459716797\n",
      "138.77366638183594\n",
      "150.19432067871094\n",
      "133.1096954345703\n",
      "148.2271728515625\n",
      "141.4473114013672\n",
      "142.9604034423828\n",
      "136.13873291015625\n",
      "139.6472625732422\n",
      "134.7795867919922\n",
      "144.79043579101562\n",
      "140.19461059570312\n",
      "130.98997497558594\n",
      "148.70086669921875\n",
      "126.11974334716797\n",
      "138.04574584960938\n",
      "129.45277404785156\n",
      "155.41293334960938\n",
      "155.3843994140625\n",
      "152.86058044433594\n",
      "144.29441833496094\n",
      "141.6836395263672\n",
      "137.73965454101562\n",
      "140.59402465820312\n",
      "148.1285400390625\n",
      "147.47164916992188\n",
      "143.66482543945312\n",
      "149.9176483154297\n",
      "133.9109649658203\n",
      "138.9539031982422\n",
      "140.3853302001953\n",
      "134.3394775390625\n",
      "133.19647216796875\n",
      "162.97421264648438\n",
      "127.69279479980469\n",
      "139.0380859375\n",
      "156.953125\n",
      "157.62698364257812\n",
      "135.82864379882812\n",
      "134.2296142578125\n",
      "152.6393585205078\n",
      "128.60675048828125\n",
      "133.19615173339844\n",
      "148.83828735351562\n",
      "144.60336303710938\n",
      "151.8235626220703\n",
      "146.94686889648438\n",
      "147.8402557373047\n",
      "146.79795837402344\n",
      "138.21009826660156\n",
      "132.25259399414062\n",
      "134.09786987304688\n",
      "139.45126342773438\n",
      "133.45138549804688\n",
      "136.47923278808594\n",
      "140.7250518798828\n",
      "131.56924438476562\n"
     ]
    }
   ],
   "source": [
    "q = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for i in range(10000):\n",
    "    q.zero_grad()\n",
    "    loss = cri(model(a).squeeze(1))\n",
    "    # loss = listMLE(model(a).T, b)\n",
    "    loss.backward()\n",
    "    q.step()\n",
    "    if i % 10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3798e+02],\n",
       "        [-2.4609e+02],\n",
       "        [-2.1981e+02],\n",
       "        [-1.6382e+02],\n",
       "        [-1.5919e+02],\n",
       "        [-1.6381e+02],\n",
       "        [-1.6579e+02],\n",
       "        [-1.3379e+02],\n",
       "        [-1.3725e+02],\n",
       "        [-1.2384e+02],\n",
       "        [-1.0684e+02],\n",
       "        [-1.1047e+02],\n",
       "        [-1.0213e+02],\n",
       "        [-1.0535e+02],\n",
       "        [-1.1044e+02],\n",
       "        [-1.0032e+02],\n",
       "        [-7.3577e+01],\n",
       "        [-8.4864e+01],\n",
       "        [-8.2507e+01],\n",
       "        [-6.1382e+01],\n",
       "        [-6.3275e+01],\n",
       "        [-7.0639e+01],\n",
       "        [-4.8080e+01],\n",
       "        [-8.1123e+01],\n",
       "        [-4.8971e+01],\n",
       "        [-5.3162e+01],\n",
       "        [-4.6056e+01],\n",
       "        [-3.8653e+01],\n",
       "        [-3.3820e+01],\n",
       "        [-3.5937e+00],\n",
       "        [-3.2592e+01],\n",
       "        [-2.8362e+01],\n",
       "        [-2.1326e+01],\n",
       "        [-2.1170e+01],\n",
       "        [-1.3926e+01],\n",
       "        [-1.6375e+01],\n",
       "        [-1.6533e+00],\n",
       "        [-3.9321e+00],\n",
       "        [-2.9408e-01],\n",
       "        [-1.5114e+01],\n",
       "        [ 2.0434e+00],\n",
       "        [ 3.7254e+00],\n",
       "        [ 3.3746e+01],\n",
       "        [ 1.3356e+01],\n",
       "        [ 1.4661e+01],\n",
       "        [ 1.9333e+01],\n",
       "        [ 2.6533e+01],\n",
       "        [ 2.8550e+01],\n",
       "        [ 2.3901e+01],\n",
       "        [ 3.5871e+01],\n",
       "        [ 6.1827e+01],\n",
       "        [ 3.0783e+01],\n",
       "        [ 4.1004e+01],\n",
       "        [ 5.0971e+01],\n",
       "        [ 4.9163e+01],\n",
       "        [ 6.2599e+01],\n",
       "        [ 3.9214e+01],\n",
       "        [ 6.5981e+01],\n",
       "        [ 5.3100e+01],\n",
       "        [ 7.6994e+01],\n",
       "        [ 9.3172e+01],\n",
       "        [ 6.6187e+01],\n",
       "        [ 8.6606e+01],\n",
       "        [ 9.9729e+01],\n",
       "        [ 7.3664e+01],\n",
       "        [ 9.3026e+01],\n",
       "        [ 1.0247e+02],\n",
       "        [ 1.2577e+02],\n",
       "        [ 9.4082e+01],\n",
       "        [ 1.3462e+02],\n",
       "        [ 1.3662e+02],\n",
       "        [ 1.1586e+02],\n",
       "        [ 1.3069e+02],\n",
       "        [ 1.2077e+02],\n",
       "        [ 1.9283e+02],\n",
       "        [ 1.3778e+02],\n",
       "        [ 1.7272e+02],\n",
       "        [ 1.5234e+02],\n",
       "        [ 2.2631e+02],\n",
       "        [ 2.0410e+02],\n",
       "        [ 2.4119e+02],\n",
       "        [ 2.1043e+02],\n",
       "        [ 2.9828e+02],\n",
       "        [ 4.1472e+02],\n",
       "        [ 4.0051e+02]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = models.ListMLE() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3083)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(b.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./temp/1204.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSheet1\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39mpd\u001b[38;5;241m.\u001b[39misnull(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"./temp/1204.xlsx\", sheet_name=\"Sheet1\", index_col=0)\n",
    "df = df[~pd.isnull(df[\"\"])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...</td>\n",
       "      <td>GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...</td>\n",
       "      <td>VIL1</td>\n",
       "      <td>VILH-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VIL2</td>\n",
       "      <td>VILH-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...</td>\n",
       "      <td>GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...</td>\n",
       "      <td>VIL3</td>\n",
       "      <td>VILH-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...</td>\n",
       "      <td>GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...</td>\n",
       "      <td>VIL4</td>\n",
       "      <td>VILH-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VIL5</td>\n",
       "      <td>VILH-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \\\n",
       "num                                                      \n",
       "0    MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...   \n",
       "1    MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "2    MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...   \n",
       "3    MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...   \n",
       "4    MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "\n",
       "                                                     name2  \n",
       "num                                                                   \n",
       "0    GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...  VIL1  VILH-1  \n",
       "1    GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...  VIL2  VILH-2  \n",
       "2    GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...  VIL3  VILH-3  \n",
       "3    GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...  VIL4  VILH-4  \n",
       "4    GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...  VIL5  VILH-5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"\", \"\", \"\"]]\n",
    "df[\"name2\"] = df[\"\"].apply(lambda x: x[:3]+\"H-\"+x[3:])\n",
    "df.to_csv(\"./temp/all885.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>.1</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VILH-301</td>\n",
       "      <td>GSMPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSR...</td>\n",
       "      <td>MPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSRSS...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VILH-197</td>\n",
       "      <td>GSMRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPC...</td>\n",
       "      <td>MRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPCYF...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VILH-19</td>\n",
       "      <td>GSMAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPC...</td>\n",
       "      <td>MAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPCSY...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VILH-363</td>\n",
       "      <td>GSMDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALE...</td>\n",
       "      <td>MDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALEFA...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VILH-426</td>\n",
       "      <td>GSMDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISAL...</td>\n",
       "      <td>MDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISALLL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      \\\n",
       "0  VILH-301  GSMPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSR...   \n",
       "1  VILH-197  GSMRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPC...   \n",
       "2   VILH-19  GSMAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPC...   \n",
       "3  VILH-363  GSMDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALE...   \n",
       "4  VILH-426  GSMDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISAL...   \n",
       "\n",
       "                                             .1  rank  \n",
       "0  MPPQRARGAPPRRRGSDPPDPGSLAGRLSPGGRSGGGSRRTLSRSS...   NaN  \n",
       "1  MRASKSDRFLMSSWVKLLFVAVIMYICSAVVPMAATYEGLGFPCYF...   NaN  \n",
       "2  MAPSKVDSVNSRIWGISVFLAFLTFANICGHTTMMNVPGVGYPCSY...   NaN  \n",
       "3  MDAVSALCVALASAAAMFVALQMWAVYENYDNIREFNSANAALEFA...   NaN  \n",
       "4  MDKTTLSVNACNLEYVREKAIVGVQAAKTSTLIFFVIILAISALLL...   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfexp = pd.read_excel(\"./temp/1210_.xlsx\", sheet_name=\"Sheet1\")\n",
    "# dfexp[\"new\"] = dfexp[\".1\"].map(df[\"new\"])\n",
    "dfexp2 = pd.read_excel(\"./temp/1210_.xlsx\", sheet_name=\"Sheet2\")\n",
    "dfexp2[\"rank\"] = range(len(dfexp2))\n",
    "dfexp2[\"rank\"] = -dfexp2[\"rank\"]\n",
    "dfexp[\"rank\"] = dfexp[\"\"].map(dfexp2.set_index(\"Name\")[\"rank\"])\n",
    "dfexp = dfexp.sort_values(\"rank\")\n",
    "dfexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MGCDVHDPSWQCQWGVPTIIVAWITCAALGIWCLAGSSADVSSGPGIAAVVGCSVFMIFLCAYLIRYREFFKDSVIDLLTCRWVRYCSCSCKCSCKCISGPCSRCCSACYKETMIYDMVQYGHRRRPGHGDDPDRVICEIVESPPVSAPTVFVPPPSEESHQPVIPPQPPTPTSEPKPKKGRAKDKPKGRPKNKPPCEPTVSSQPPSQPTAMPGGPPDASPPAMPQMPPGVAEAVQAAVQAAMAAALQQQQQHQTGT',\n",
       " 'MISQGNGGGCRPGEPCWRCALESTRCITLMGVLVALLAACMLSVPPAASTMLLGVASLMAMLRLPMPLVDRFIPACMGLQLVGAAVFAAGWALASRDAISAGVLLWAVCALISHMYNVVCVASGPDAHYRPACLVMGVAAACGAAGALVNVRTEARLGIALGLAVTCATNNVARSLRGTCTYVASRARFLAAPADLGRGYSVENADADPTAEPERRVYEATVPHTHAYAGSIALFALVFSAASSLQWMVSQMVGRGNQLVSPTTAAAAGAAGFLDAAAVSLFVRPSTRHLSVAVKGAHTLLILAAIVLTAAGEPMGVPISLAASTGLGAARGGPRPLRHTRAYRLAAAHVTRALLVQAYVTVAMCATSIKSVS')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/0/test.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f) \n",
    "data[-1][\"ori_seq\"], data[0][\"ori_seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MGGWSSKPRQGMGTNLSVPNPLGFFPDHQLDPAFGANSNNPDWDFNPNKDHWPEANQVGAGAFGPGFTPPHGGLLGWSPQAQGILTTVPAAPPPASTNRQSGRQPTPISPPLRDSHPQAIRWNSTTFHQALLDPRVRGLYFPAGGSSSGTVNPVPTTASPISSIFSRTGDPATNMENTTSGFLGPLLVLQAGFFSLTRILTIPQSLDSWWTSLNFLGGAPTCPGQNSQSPTSNHSPTSCPPICPGYRWMCLRRFIIFLFILLLCLIFLLVLLDYQGMLPVCPLLPGTSTTSTGPCKTCTIPAQGTSMFPSCCCTKPSDGNCTCIPIPSSWAFARFLWEWASVRFSWLSLLVPFVQWFVGLSPTVWLSVIWMMWYWGPSLYNILSPFLPLLPIFFYLWVYI',\n",
       " 'MNNATFNCTNINPITHIRGSIIITICVSLIVILIVFGCIAKIFINKNNCTNNVIRVHKRIKCPDCEPFCNKRDDISTPRAGVDIPSFILPGLNLSEGTPN')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/0/test_1.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f) \n",
    "data[-1][\"ori_seq\"], data[0][\"ori_seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>experiment_1</th>\n",
       "      <th>experiment_2</th>\n",
       "      <th>yeast_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>MNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVLIR...</td>\n",
       "      <td>GSMNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVL...</td>\n",
       "      <td>VIL100</td>\n",
       "      <td>-1</td>\n",
       "      <td>62</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>MALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRASHR...</td>\n",
       "      <td>GSMALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRAS...</td>\n",
       "      <td>VIL455</td>\n",
       "      <td>-1</td>\n",
       "      <td>61</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>MMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDSKI...</td>\n",
       "      <td>GSMMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDS...</td>\n",
       "      <td>VIL750</td>\n",
       "      <td>-1</td>\n",
       "      <td>60</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVICL...</td>\n",
       "      <td>GSMTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVI...</td>\n",
       "      <td>VIL10</td>\n",
       "      <td>-1</td>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>MPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLGNL...</td>\n",
       "      <td>GSMPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLG...</td>\n",
       "      <td>VIL835</td>\n",
       "      <td>-1</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>MVDLFFNDTAWYIGQILVLVLFCLISLIFVVAFLATIKLCMQLCGF...</td>\n",
       "      <td>GSMVDLFFNDTAWYIGQILVLVLFCLISLIFVVAFLATIKLCMQLC...</td>\n",
       "      <td>VIL847</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>MMNLLNKSLEENGSFLTALYIIVGFLALYLLGRALQAFVQAADACC...</td>\n",
       "      <td>GSMMNLLNKSLEENGSFLTALYIIVGFLALYLLGRALQAFVQAADA...</td>\n",
       "      <td>VIL821</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>MDLFMRIFTIGTVTLKQGEIKDATPSDFVRATATIPIQASLPFGWL...</td>\n",
       "      <td>GSMDLFMRIFTIGTVTLKQGEIKDATPSDFVRATATIPIQASLPFG...</td>\n",
       "      <td>VIL859</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>MALGLFTLQIESAVNQSLSKSKVSAVVSRQVIQDVRAAAVTFNLLA...</td>\n",
       "      <td>GSMALGLFTLQIESAVNQSLSKSKVSAVVSRQVIQDVRAAAVTFNL...</td>\n",
       "      <td>VIL843</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>MLPFVQERIGLFIVNFFIFTVVCAITLLVCMAFLTATRLCVQCMTG...</td>\n",
       "      <td>GSMLPFVQERIGLFIVNFFIFTVVCAITLLVCMAFLTATRLCVQCM...</td>\n",
       "      <td>VIL854</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \\\n",
       "num                                                      \n",
       "101  MNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVLIR...   \n",
       "459  MALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRASHR...   \n",
       "758  MMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDSKI...   \n",
       "9    MTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVICL...   \n",
       "844  MPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLGNL...   \n",
       "..                                                 ...   \n",
       "856  MVDLFFNDTAWYIGQILVLVLFCLISLIFVVAFLATIKLCMQLCGF...   \n",
       "830  MMNLLNKSLEENGSFLTALYIIVGFLALYLLGRALQAFVQAADACC...   \n",
       "870  MDLFMRIFTIGTVTLKQGEIKDATPSDFVRATATIPIQASLPFGWL...   \n",
       "852  MALGLFTLQIESAVNQSLSKSKVSAVVSRQVIQDVRAAAVTFNLLA...   \n",
       "865  MLPFVQERIGLFIVNFFIFTVVCAITLLVCMAFLTATRLCVQCMTG...   \n",
       "\n",
       "                                                      experiment_1  \\\n",
       "num                                                                            \n",
       "101  GSMNNSSCDLLQAFKIDDASRDVSVGFYSIAICVGLVANILILLVL...  VIL100            -1   \n",
       "459  GSMALTCRLRFPVPGFRGRMHRRRGMAGHGLTGGMRRAHHRRRRAS...  VIL455            -1   \n",
       "758  GSMMADSKLVSLNNNLSGKIKDQGKVIKNYYGTMDIKKINDGLLDS...  VIL750            -1   \n",
       "9    GSMTGEVCHVNDTMKAYGMTPDLTISLYSLGMILGIGGNMLILCVI...   VIL10            -1   \n",
       "844  GSMPLSYQHFRRLLLLDDEAGPLEEELPRLADEGLNRRVAEDLNLG...  VIL835            -1   \n",
       "..                                                 ...     ...           ...   \n",
       "856  GSMVDLFFNDTAWYIGQILVLVLFCLISLIFVVAFLATIKLCMQLC...  VIL847            -1   \n",
       "830  GSMMNLLNKSLEENGSFLTALYIIVGFLALYLLGRALQAFVQAADA...  VIL821            -1   \n",
       "870  GSMDLFMRIFTIGTVTLKQGEIKDATPSDFVRATATIPIQASLPFG...  VIL859            -1   \n",
       "852  GSMALGLFTLQIESAVNQSLSKSKVSAVVSRQVIQDVRAAAVTFNL...  VIL843            -1   \n",
       "865  GSMLPFVQERIGLFIVNFFIFTVVCAITLLVCMAFLTATRLCVQCM...  VIL854            -1   \n",
       "\n",
       "     experiment_2  yeast_1  \n",
       "num                         \n",
       "101            62       -1  \n",
       "459            61       -1  \n",
       "758            60       -1  \n",
       "9              59       -1  \n",
       "844            58       -1  \n",
       "..            ...      ...  \n",
       "856             5       -1  \n",
       "830             4        1  \n",
       "870             3       -1  \n",
       "852             2       -1  \n",
       "865             1       -1  \n",
       "\n",
       "[62 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv(\"./temp/expres.csv\", index_col=0)\n",
    "t = t.sort_values(\"experiment_2\", ascending=False)\n",
    "t = t[t[\"experiment_2\"] > 0]\n",
    "len(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 62)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [] \n",
    "for i in t[\"\"]:\n",
    "    for j in data:\n",
    "        if i == j[\"ori_seq\"]:\n",
    "            res.append(j)\n",
    "len(res), len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/exp2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(106))\n",
    "random.seed(1509)\n",
    "random.shuffle(a)\n",
    "splits = [0, 21, 42, 63, 84, 106]\n",
    "for i in range(5):\n",
    "    test_indices = a[splits[i]:splits[i+1]] \n",
    "    train = [] \n",
    "    test = [] \n",
    "    for j in range(106):\n",
    "        if j in test_indices:\n",
    "            test.append(res[j])\n",
    "        else:\n",
    "            train.append(res[j])\n",
    "    with open(f\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/{i}/train_2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train, f)\n",
    "    with open(f\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/{i}/test_2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31,\n",
       " 5,\n",
       " 51,\n",
       " 69,\n",
       " 20,\n",
       " 90,\n",
       " 49,\n",
       " 54,\n",
       " 77,\n",
       " 0,\n",
       " 93,\n",
       " 17,\n",
       " 100,\n",
       " 13,\n",
       " 83,\n",
       " 50,\n",
       " 28,\n",
       " 74,\n",
       " 24,\n",
       " 46,\n",
       " 15,\n",
       " 98,\n",
       " 8,\n",
       " 34,\n",
       " 42,\n",
       " 81,\n",
       " 27,\n",
       " 36,\n",
       " 40,\n",
       " 14,\n",
       " 95,\n",
       " 76,\n",
       " 84,\n",
       " 25,\n",
       " 96,\n",
       " 43,\n",
       " 64,\n",
       " 62,\n",
       " 21,\n",
       " 67,\n",
       " 38,\n",
       " 66,\n",
       " 48,\n",
       " 1,\n",
       " 44,\n",
       " 47,\n",
       " 80,\n",
       " 92,\n",
       " 58,\n",
       " 10,\n",
       " 68,\n",
       " 105,\n",
       " 87,\n",
       " 7,\n",
       " 39,\n",
       " 57,\n",
       " 55,\n",
       " 60,\n",
       " 85,\n",
       " 26,\n",
       " 91,\n",
       " 63,\n",
       " 41,\n",
       " 22,\n",
       " 65,\n",
       " 103,\n",
       " 16,\n",
       " 12,\n",
       " 72,\n",
       " 89,\n",
       " 30,\n",
       " 70,\n",
       " 59,\n",
       " 52,\n",
       " 99,\n",
       " 86,\n",
       " 56,\n",
       " 53,\n",
       " 6,\n",
       " 101,\n",
       " 88,\n",
       " 32,\n",
       " 102,\n",
       " 104,\n",
       " 4,\n",
       " 29,\n",
       " 45,\n",
       " 23,\n",
       " 73,\n",
       " 71,\n",
       " 11,\n",
       " 3,\n",
       " 82,\n",
       " 78,\n",
       " 75,\n",
       " 61,\n",
       " 37,\n",
       " 97,\n",
       " 2,\n",
       " 79,\n",
       " 19,\n",
       " 9,\n",
       " 33,\n",
       " 18,\n",
       " 94,\n",
       " 35]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import trainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/FromTestDomain/1\"\n",
    "with open(os.path.join(path, \"config.json\"), \"r\") as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix model for active learning\n"
     ]
    }
   ],
   "source": [
    "pretrain_model = trainUtils.loadPretrainModel(configs)\n",
    "model = trainUtils.buildModel(configs, pretrain_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = trainUtils.loadDataset(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = ds.train_dataloader()\n",
    "for i, j in enumerate(dl):\n",
    "    break\n",
    "print(j[0].shape, j[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.l1.weight\n",
      "clf.l1.bias\n",
      "clf.l2.weight\n",
      "clf.l2.bias\n",
      "clf.l3.weight\n",
      "clf.l3.bias\n",
      "clf.ln1.weight\n",
      "clf.ln1.bias\n",
      "clf.ln2.weight\n",
      "clf.ln2.bias\n"
     ]
    }
   ],
   "source": [
    "for i, j in model.named_parameters():\n",
    "    if j.requires_grad:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = trainUtils.loadDataset(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIL1</th>\n",
       "      <td>MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...</td>\n",
       "      <td>GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...</td>\n",
       "      <td>VILH-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL2</th>\n",
       "      <td>MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...</td>\n",
       "      <td>GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...</td>\n",
       "      <td>VILH-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL3</th>\n",
       "      <td>MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...</td>\n",
       "      <td>GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...</td>\n",
       "      <td>VILH-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL4</th>\n",
       "      <td>MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...</td>\n",
       "      <td>GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...</td>\n",
       "      <td>VILH-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIL5</th>\n",
       "      <td>MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...</td>\n",
       "      <td>GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...</td>\n",
       "      <td>VILH-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      \\\n",
       "                                                      \n",
       "VIL1   MKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHLRD...   \n",
       "VIL2   MLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQLNR...   \n",
       "VIL3   MGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAFCL...   \n",
       "VIL4   MEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLMWL...   \n",
       "VIL5   MDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGTLS...   \n",
       "\n",
       "                                                      name2  \n",
       "                                                             \n",
       "VIL1   GSMKPLCTPWVDGTILCSLLFLLAFSGVSSAWSNDTKLPQRRSGHL...  VILH-1  \n",
       "VIL2   GSMLRYPWLQLLATFLLFEVSLCCFFSKKGLTTSYNRRFHYRWIQL...  VILH-2  \n",
       "VIL3   GSMGRQDTSREGNEDYEDIMRWVRRFVWLTRVYTVLAVQMAVTLAF...  VILH-3  \n",
       "VIL4   GSMEVLRFQVRFTESIIWIQRFKILIQLYSYWLLQVTVTSTLSTLM...  VILH-4  \n",
       "VIL5   GSMDHRSYADAELAESWMHENLVQWIDRFRSVVAIYSNALFEVAGT...  VILH-5  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./temp/all885.csv\", index_col=0)\n",
    "df = df.set_index(\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexp = pd.read_excel(\"./temp/exp2.xlsx\", sheet_name=\"Sheet1\")\n",
    "dfexp = dfexp.set_index(\"Name\")\n",
    "dfexp[\"seq\"] = df[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 62)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [] \n",
    "for i in dfexp[\"seq\"]:\n",
    "    for j in data:\n",
    "        if i == j[\"ori_seq\"]:\n",
    "            res.append(j)\n",
    "len(res), len(dfexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/exp2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2-validate_acc=0.9944.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.62it/s]\n",
      "epoch=8-validate_acc=0.9823.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:43<00:00, 20.25it/s]\n",
      "load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 55, in run\n",
      "    model = trainUtils.buildModel(\n",
      "  File \"/home/tyfei/ionChannel/trainUtils.py\", line 470, in buildModel\n",
      "    t = torch.load(checkpoint, map_location=\"cpu\")\n",
      "  File \"/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/serialization.py\", line 1040, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/torch/serialization.py\", line 1262, in _legacy_load\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\n",
      "_pickle.UnpicklingError: invalid load key, '{'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4-validate_acc=0.9909.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 21.06it/s]\n",
      "epoch=11-validate_acc=0.9924.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:44<00:00, 20.10it/s]\n",
      "epoch=9-validate_acc=0.9894.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:43<00:00, 20.39it/s]\n",
      "epoch=13-validate_acc=0.9889.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.81it/s]\n",
      "epoch=5-validate_acc=0.9884.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.69it/s]\n",
      "epoch=14-validate_acc=0.9934.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.98it/s]\n",
      "epoch=15-validate_acc=0.9924.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:43<00:00, 20.48it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.80it/s]\n",
      "epoch=3-validate_acc=0.9863.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.93it/s]\n",
      "epoch=12-validate_acc=0.9924.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:43<00:00, 20.19it/s]\n",
      "epoch=1-validate_acc=0.9909.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:43<00:00, 20.57it/s]\n",
      "epoch=7-validate_acc=0.9919.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:43<00:00, 20.57it/s]\n",
      "epoch=0-validate_acc=0.9828.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.99it/s]\n",
      "epoch=6-validate_acc=0.9899.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.59it/s]\n",
      "epoch=10-validate_acc=0.9873.ckpt\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 885/885 [00:42<00:00, 20.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from scipy import stats\n",
    "target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest/\"\n",
    "output_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest/bayes10/\"\n",
    "dataset = \"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\"\n",
    "for i in os.listdir(target_dir):\n",
    "    if i.startswith(\"epoch\"):\n",
    "        print(i) \n",
    "        \n",
    "    ret = os.system(\"python test.py -p %s -c %s -i %s -d %d -b %d -o %s\"%(target_dir, i, dataset, 5, 10, output_dir)) \n",
    "    \n",
    "target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest2/\"\n",
    "output_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest2/bayes10/\"\n",
    "dataset = \"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\"\n",
    "for i in os.listdir(target_dir):\n",
    "    if i.startswith(\"epoch\"):\n",
    "        print(i) \n",
    "        \n",
    "    ret = os.system(\"python test.py -p %s -c %s -i %s -d %d -b %d -o %s\"%(target_dir, i, dataset, 5, 10, output_dir)) \n",
    "    \n",
    "# target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest3/\"\n",
    "# output_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/newtest3/bayes10/\"\n",
    "# dataset = \"/data/tyfei/datasets/ion_channel/Interprot/test885.pkl\"\n",
    "# for i in os.listdir(target_dir):\n",
    "#     if i.startswith(\"epoch\"):\n",
    "#         print(i) \n",
    "        \n",
    "#     ret = os.system(\"python test.py -p %s -c %s -i %s -d %d -b %d -o %s\"%(target_dir, i, dataset, 5, 10, output_dir)) \n",
    "    # if ret == 0:\n",
    "    #     a = np.loadtxt(\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/FromTestDomain/4_2/test_last.txt\")\n",
    "    #     stats.spearmanr(a, range(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 16.90it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 17.04it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 16.68it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 17.41it/s]\n",
      "load model\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 22/22 [00:01<00:00, 17.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in range(5):\n",
    "    # target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/%d_2/\"%i\n",
    "    target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMC/nodomain/\"\n",
    "    dataset = \"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/%d/test.pkl\"%i\n",
    "    outputdir = \"/data2/tyfei/trainresults/ionChannels/ESMC/nodomain/%d\"%i\n",
    "    os.system(\"python test.py -p %s -c %s -i %s -d %d -o %s\"%(target_dir, target_dir+\"epoch=8-validate_acc=0.9899.ckpt\", dataset, 5, outputdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "fix model for active learning\n",
      "load dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/tyfei/anaconda3/envs/esm3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|| 21/21 [00:01<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 92, in <module>\n",
      "    run()\n",
      "  File \"/home/tyfei/ionChannel/test.py\", line 36, in run\n",
      "    assert os.path.isdir(args.path)\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in range(5):\n",
    "    target_dir = \"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/%d_4/\"%i\n",
    "    dataset = \"/data/tyfei/datasets/ion_channel/Interprot/activeLearningTest/%d/test.pkl\"%i\n",
    "    os.system(\"python test.py -p %s -c %s -i %s -d %d\"%(target_dir, target_dir+\"last.ckpt\", dataset, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.247482962061986, pvalue=0.279429612882753)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import testUtils \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "a = np.loadtxt(\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/FromTestDomain/4_2/test_last.txt\")\n",
    "stats.spearmanr(a, range(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14285714285714285,\n",
       " -0.212987012987013,\n",
       " 0.14935064935064934,\n",
       " 0.22337662337662337,\n",
       " -0.005081874647092039]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0 = []\n",
    "for i in range(5):\n",
    "    a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMC/nodomain/{i}/test_epoch=8-validate_acc=0.9899.txt\")\n",
    "    d0.append(stats.spearmanr(a, range(len(a)))[0])\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32597402597402597,\n",
       " 0.15194805194805194,\n",
       " 0.6896103896103896,\n",
       " 0.5064935064935064,\n",
       " 0.11914172783738003]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = [] \n",
    "for i in range(5):\n",
    "    if os.path.exists(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}/test_last-v1.txt\"):\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}/test_last-v1.txt\")\n",
    "    else:\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}/test_last.txt\")\n",
    "    d1.append(stats.spearmanr(a, range(len(a)))[0])\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6168831168831169,\n",
       " -0.15584415584415584,\n",
       " 0.3922077922077922,\n",
       " 0.5649350649350648,\n",
       " 0.1981931112365895]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = [] \n",
    "for i in range(5):\n",
    "    if os.path.exists(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}_3/test_last-v1.txt\"):\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}_3/test_last-v1.txt\")\n",
    "    else:\n",
    "        # print(\"train\")\n",
    "        a = np.loadtxt(f\"/data2/tyfei/trainresults/ionChannels/ESMCActiveLearning/nodomain/{i}_3/test_last.txt\")\n",
    "    d2.append(stats.spearmanr(a, range(len(a)))[0])\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m [] \n\u001b[0;32m----> 2\u001b[0m d\u001b[38;5;241m.\u001b[39mextend(\u001b[43md0\u001b[49m)\n\u001b[1;32m      3\u001b[0m d\u001b[38;5;241m.\u001b[39mextend(d1)\n\u001b[1;32m      4\u001b[0m d\u001b[38;5;241m.\u001b[39mextend(d2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd0' is not defined"
     ]
    }
   ],
   "source": [
    "d = [] \n",
    "d.extend(d0)\n",
    "d.extend(d1)\n",
    "d.extend(d2)\n",
    "f = [] \n",
    "f.extend([\"0\"]*5)\n",
    "f.extend([\"1\"]*5)\n",
    "f.extend([\"2\"]*5)\n",
    "df = pd.DataFrame({\"d\": d, \"f\": f}) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='f', ylabel='d'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=\"f\", y=\"d\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
